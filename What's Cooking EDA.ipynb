{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What's Cooking Kaggle Challenge\n",
    "\n",
    "Link: https://www.kaggle.com/c/whats-cooking/overview\n",
    "\n",
    "In this notebook I'll be looking at the dataset from the Kaggle What's Cooking Challenge which is a supervised problem of predicting cuisines from ingredients list. The dataset is a .json file with only 2 important columns being the cuisine as well as the ingredients. In this notebook we will be doing exploratory data analysis followed by using some Natural Language Processing techniques to solve the problem. I will be using PyTorch to implement the deep learning model to learn the vector representations of the words.\n",
    "\n",
    "## Things to do: \n",
    "Add in a content page using Markdown\n",
    "Segment the Notebook into different components\n",
    "- Data Preprocessing\n",
    "- Preliminary Model\n",
    "- Data Visualisation - 1st Iteration\n",
    "- CBOW Model\n",
    "- NN Architecture\n",
    "- Evaluation\n",
    "- Visualisation - 2nd Iteration\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch.nn\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39774\n",
      "9944\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cuisine</th>\n",
       "      <th>ingredients</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10259</td>\n",
       "      <td>greek</td>\n",
       "      <td>[romaine lettuce, black olives, grape tomatoes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25693</td>\n",
       "      <td>southern_us</td>\n",
       "      <td>[plain flour, ground pepper, salt, tomatoes, g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20130</td>\n",
       "      <td>filipino</td>\n",
       "      <td>[eggs, pepper, salt, mayonaise, cooking oil, g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22213</td>\n",
       "      <td>indian</td>\n",
       "      <td>[water, vegetable oil, wheat, salt]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13162</td>\n",
       "      <td>indian</td>\n",
       "      <td>[black pepper, shallots, cornflour, cayenne pe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id      cuisine                                        ingredients\n",
       "0  10259        greek  [romaine lettuce, black olives, grape tomatoes...\n",
       "1  25693  southern_us  [plain flour, ground pepper, salt, tomatoes, g...\n",
       "2  20130     filipino  [eggs, pepper, salt, mayonaise, cooking oil, g...\n",
       "3  22213       indian                [water, vegetable oil, wheat, salt]\n",
       "4  13162       indian  [black pepper, shallots, cornflour, cayenne pe..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_json(\"datasets/train.json\")\n",
    "df_test = pd.read_json(\"datasets/test.json\")\n",
    "print(len(df))\n",
    "print(len(df_test))\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "italian         7838\n",
       "mexican         6438\n",
       "southern_us     4320\n",
       "indian          3003\n",
       "chinese         2673\n",
       "french          2646\n",
       "cajun_creole    1546\n",
       "thai            1539\n",
       "japanese        1423\n",
       "greek           1175\n",
       "spanish          989\n",
       "korean           830\n",
       "vietnamese       825\n",
       "moroccan         821\n",
       "british          804\n",
       "filipino         755\n",
       "irish            667\n",
       "jamaican         526\n",
       "russian          489\n",
       "brazilian        467\n",
       "Name: cuisine, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Different Cuisines present and their counts\n",
    "df[\"cuisine\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingredients = df[\"ingredients\"].tolist()\n",
    "test_ingredients = df_test[\"ingredients\"].tolist()\n",
    "ingredients = ingredients + test_ingredients\n",
    "\n",
    "# The Vector space has to include all ingredients from both Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>salt</th>\n",
       "      <td>22534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>onions</th>\n",
       "      <td>10008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>olive oil</th>\n",
       "      <td>9889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>water</th>\n",
       "      <td>9293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>garlic</th>\n",
       "      <td>9171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>seville orange juice</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dried hibiscus blossoms</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pancake batter</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dairy free coconut ice cream</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shark fillets</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7137 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Counts\n",
       "salt                           22534\n",
       "onions                         10008\n",
       "olive oil                       9889\n",
       "water                           9293\n",
       "garlic                          9171\n",
       "...                              ...\n",
       "seville orange juice               1\n",
       "dried hibiscus blossoms            1\n",
       "pancake batter                     1\n",
       "dairy free coconut ice cream       1\n",
       "shark fillets                      1\n",
       "\n",
       "[7137 rows x 1 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ingredients_dict = {}\n",
    "for recipe in ingredients:\n",
    "    for ingredient in recipe:\n",
    "        ingredients_dict[ingredient] = ingredients_dict.get(ingredient,0)+ 1\n",
    "\n",
    "ing_df = pd.DataFrame(data = ingredients_dict.values(),index = ingredients_dict.keys(),columns = [\"Counts\"])\n",
    "ing_df.sort_values([\"Counts\"],ascending = False, inplace = True)\n",
    "ing_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen from the dataframe, there are currently 7137 differnt types of ingredients present in the dataset. However many of them are repeated but have a slightly different name in the recipe. (Eg. Garlic vs Chopped Garlic). Below you can see a list of stopwords which are redundant. This results in a reduction of the number of ingredients by around 200 which is relatively sizeable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords= [\"fresh\",\"chopped\",\"large\",\"all-purpose\",\"grated\",\"freshly\",\"crushed\",\"minced\",\"skinless\"\n",
    "           \"sodium\",\"low\",\"diced\",\"unsalted\",\"coarse\",\"low-fat\",\"medium\",\"powdered\",\"finely\",\"fine\",\n",
    "           \"pitted\",\"plain\",\"low-fat\",\"full-fat\",\"nonfat\",\"fat-free\"]\n",
    "def find_occurence(word,recipe_list): \n",
    "    #Utility function to check if an ingredient is present in the list of recipes\n",
    "    result = {}\n",
    "    for recipe in recipe_list:\n",
    "        for ingredient in recipe:\n",
    "            if word in ingredient:\n",
    "                result[ingredient] = result.get(ingredient,0) + 1\n",
    "    return list(result.keys())\n",
    "\n",
    "ingredients2 = []\n",
    "for index,i in enumerate(ingredients):\n",
    "    recipe = []\n",
    "    for j in i:\n",
    "        ing_word = j.split(\" \")\n",
    "        ing_word = [i for i in ing_word if i not in stopwords]\n",
    "        recipe.append(\" \".join(ing_word))\n",
    "    ingredients2.append(recipe)\n",
    "ingredients = ingredients2[:]\n",
    "\n",
    "ingredients_dict2 = {}\n",
    "for recipe in ingredients:\n",
    "    for ingredient in recipe:\n",
    "        ingredients_dict2[ingredient] = ingredients_dict2.get(ingredient,0)+ 1\n",
    "ing_df = pd.DataFrame(data = ingredients_dict2.values(),index = ingredients_dict2.keys(),columns = [\"Counts\"])\n",
    "ing_df.sort_values([\"Counts\"],ascending = False, inplace = True)\n",
    "df[\"ingredients\"]= ingredients[:len(df)] #Append the \"cleaned\" list of ingredients to the dataframe\n",
    "ingredients_map = {k:v for k,v in zip(ing_df.index,range(len(ing_df)))}\n",
    "\n",
    "def convert_recipe(recipe):\n",
    "    '''\n",
    "    Convert Recipe from a List of String Ingredients to a Vector\n",
    "    recipe: List of Ingredients\n",
    "    output: 7137x1 Vector\n",
    "    '''\n",
    "    output = np.zeros(7137)\n",
    "    for ingredient in recipe:\n",
    "        output[ingredients_map[ingredient]] = 1\n",
    "    return output\n",
    "    \n",
    "df[\"Vector\"] = df[\"ingredients\"].apply(convert_recipe) # Convert each recipe to a OHE Sparse Vector Form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that most of the preprocessing is done, traditional ML methods are used as a baseline effectiveness for the classification task so that we can compare the performance using Deep Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "df[\"Target\"] = le.fit_transform(df[\"cuisine\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things to do:\n",
    "- Reduce dimensionality of the data by removing redundant words\n",
    "- Turning everything into a one hot encoded vector\n",
    "- K-Means Clustering to treat the problem like an unsupervised one\n",
    "- KNN, SVM vs Neural Network\n",
    "- Use additional feature engineering to enhance model performance - Protein, spices others etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store all the vectors as a Matrix of M x 7137\n",
    "mat = list(df[\"Vector\"])\n",
    "mat = np.array(mat)\n",
    "mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of the curse of dimensionality, training a model on this vector space will take way too long. For this we first use a linear dimensionality reduction tool PCA and later in the Natural Language Processing Section we can see how this actually compares with non-linear methods such as using an autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca_128 = PCA(128)\n",
    "mat_pca_128 = pca_128.fit_transform(mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(mat_pca_128,df['Target'],\n",
    "                                                    test_size=0.30)\n",
    "sv_linear = SVC(kernel = \"linear\")\n",
    "sv_linear.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.20      0.27       144\n",
      "           1       0.30      0.17      0.22       236\n",
      "           2       0.73      0.66      0.69       451\n",
      "           3       0.72      0.80      0.76       769\n",
      "           4       0.63      0.53      0.57       212\n",
      "           5       0.47      0.52      0.49       748\n",
      "           6       0.67      0.60      0.63       346\n",
      "           7       0.85      0.86      0.85       886\n",
      "           8       0.40      0.17      0.24       218\n",
      "           9       0.72      0.84      0.77      2353\n",
      "          10       0.81      0.57      0.67       162\n",
      "          11       0.70      0.55      0.61       433\n",
      "          12       0.72      0.57      0.64       242\n",
      "          13       0.87      0.88      0.88      1961\n",
      "          14       0.76      0.64      0.69       248\n",
      "          15       0.41      0.30      0.35       139\n",
      "          16       0.58      0.73      0.65      1334\n",
      "          17       0.61      0.28      0.38       327\n",
      "          18       0.72      0.68      0.70       461\n",
      "          19       0.73      0.49      0.58       263\n",
      "\n",
      "    accuracy                           0.70     11933\n",
      "   macro avg       0.64      0.55      0.58     11933\n",
      "weighted avg       0.70      0.70      0.69     11933\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "predictions = sv_linear.predict(X_test)\n",
    "print(classification_report(y_test,predictions))\n",
    "cr = classification_report(y_test,predictions,output_dict= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(x_pca[:,0],x_pca[:,1],c= df[\"Target\"])\n",
    "\n",
    "plt.xlabel('First principal component')\n",
    "plt.ylabel('Second Principal Component')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_pca,df['Target'],\n",
    "                                                    test_size=0.30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC()\n",
    "svm.fit(X_train,y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = svm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = sum(y_test == predictions)/len(y_test)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_df = df[df[\"Target\"] < 6]\n",
    "subset = list(subset_df[\"Vector\"])\n",
    "subset = np.array(subset)\n",
    "x_pca = pca.fit_transform(subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x = x_pca[:,0], y = x_pca[:,1],hue = subset_df[\"Target\"])\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le.inverse_transform(range(0,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Processing\n",
    "\n",
    "Now that we have seen how something like PCA can make sense of the data, we come to the interesting part which is to try to find a better vector representation of the words in a recipe.\n",
    "\n",
    "Here we will use the Common Bag Of Words (CBOW) model in order to learn the representation of the words. The idea behind CBOW is to use the context of a word to learn what the word actually means. For example a sentence such as \"I like to eat pasta\", in order to learn the representation of the word \"eat\", we look at \"pasta\" and \"like\" as context words. In this case because the recipes are inherently unordered, the context words will be obtained randomly from the sample.\n",
    "\n",
    "The implementation of this model is to use an autoencoder architecture where the One-Hot encoded word vectors are used in the model as inputs and encoded into the learnt representations and then the decoder will try to recreate the target word. \n",
    "\n",
    "Link: https://towardsdatascience.com/introduction-to-word-embedding-and-word2vec-652d0c2060fa\n",
    "\n",
    "## To-Do: Insert picture of the autoencoder\n",
    "    \n",
    "\n",
    "Recipe Sampling, Randomly choose 5 other ingredients that are in the same recipe and try to use the words to learn the representation of that word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\reone\\Anaconda3\\envs\\pytorch37\\lib\\site-packages\\ipykernel_launcher.py:1: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df = pd.concat([df,df_test]) # This is to train the Language Model on the entire set of ingredients\n",
    "\n",
    "stopwords= [\"fresh\",\"chopped\",\"large\",\"all-purpose\",\"grated\",\"freshly\",\"crushed\",\"minced\",\"skinless\"\n",
    "           \"sodium\",\"low\",\"diced\",\"unsalted\",\"coarse\",\"low-fat\",\"medium\",\"powdered\",\"finely\",\"fine\",\n",
    "           \"pitted\",\"plain\",\"low-fat\",\"full-fat\",\"nonfat\",\"fat-free\"]\n",
    "def find_occurence(word,recipe_list):\n",
    "    result = {}\n",
    "    for recipe in recipe_list:\n",
    "        for ingredient in recipe:\n",
    "            if word in ingredient:\n",
    "                result[ingredient] = result.get(ingredient,0) + 1\n",
    "    return list(result.keys())\n",
    "\n",
    "ingredients2 = []\n",
    "for index,i in enumerate(ingredients):\n",
    "    recipe = []\n",
    "    for j in i:\n",
    "        ing_word = j.split(\" \")\n",
    "        ing_word = [i for i in ing_word if i not in stopwords]\n",
    "        recipe.append(\" \".join(ing_word))\n",
    "    ingredients2.append(recipe)\n",
    "ingredients = ingredients2[:]\n",
    "\n",
    "ingredients_dict2 = {}\n",
    "for recipe in ingredients2:\n",
    "    for ingredient in recipe:\n",
    "        ingredients_dict2[ingredient] = ingredients_dict2.get(ingredient,0)+ 1\n",
    "ing_df = pd.DataFrame(data = ingredients_dict2.values(),index = ingredients_dict2.keys(),columns = [\"Counts\"])\n",
    "ing_df.sort_values([\"Counts\"],ascending = False, inplace = True)\n",
    "df[\"ingredients\"]= ingredients[:len(df)] #Append the \"cleaned\" list of ingredients to the dataframe\n",
    "ingredients_map = {k:v for k,v in zip(ing_df.index,range(len(ing_df)))}\n",
    "\n",
    "def convert_recipe(recipe):\n",
    "    '''\n",
    "    Convert Recipe from a List of String Ingredients to a Vector\n",
    "    recipe: List of Ingredients\n",
    "    output: 7137x1 Vector\n",
    "    '''\n",
    "    output = np.zeros(7137)\n",
    "    for ingredient in recipe:\n",
    "        output[ingredients_map[ingredient]] = 1\n",
    "    return output\n",
    "    \n",
    "df[\"Vector\"] = df[\"ingredients\"].apply(convert_recipe) # Convert each recipe to a OHE Sparse Vector Form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper Functions\n",
    "CONTEXT_SIZE = 4\n",
    "#The sampling function to get context words from a recipe\n",
    "def sample(recipe,ingredient,samples):\n",
    "    recipe = recipe[:] #Copy the recipe to prevent alteration\n",
    "    recipe.remove(ingredient)\n",
    "    if len(recipe) < CONTEXT_SIZE+1:\n",
    "        context = random.choices(recipe, k=samples) #With Replacement when they are insufficient ingredients in the recipe\n",
    "    else: \n",
    "        context = random.sample(recipe, k=samples) #Without Replacement \n",
    "    return context\n",
    "def createVector(context):\n",
    "    idxs = [ingredients_map[w] for w in context]\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "class RecipeDataset(Dataset):\n",
    "    def __init__(self, samples):\n",
    "        self.samples = samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''samples = []\n",
    "for recipe in df[\"ingredients\"]:\n",
    "    for ingredient in recipe[::2]:\n",
    "        if len(recipe) > 2:\n",
    "            samples.append([sample(recipe,ingredient,CONTEXT_SIZE),ingredient])\n",
    "samples_idx = []\n",
    "for context,target in samples:\n",
    "    target_idx = (torch.tensor([ingredients_map[target]], dtype=torch.long))\n",
    "    samples_idx.append([createVector(context),target_idx])'''\n",
    "def loadData(dataCol,batch_size):\n",
    "    samples = []\n",
    "    for recipe in dataCol:\n",
    "        for ingredient in recipe:\n",
    "            if len(recipe) > 2:\n",
    "                samples.append([sample(recipe,ingredient,CONTEXT_SIZE),ingredient])\n",
    "    samples_idx = []\n",
    "    for context,target in samples:\n",
    "        target_idx = (torch.tensor([ingredients_map[target]], dtype=torch.long))\n",
    "        samples_idx.append([createVector(context),target_idx])\n",
    "    \n",
    "    sample_DS = RecipeDataset(samples_idx)\n",
    "    train_loader = DataLoader(dataset=sample_DS, batch_size=batch_size, shuffle=True)\n",
    "    return train_loader\n",
    "#train_loader= loadData(df[\"ingredients\"],64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "280032"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sample_DS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(ingredients_dict2)\n",
    "EMBED_DIM = 20\n",
    "\n",
    "class CBOWModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        self.context_size = context_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        super(CBOWModel, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim) #Transform to Lower Dimension Embeddings\n",
    "        self.linear1 = nn.Linear(context_size*embedding_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs).view((-1,self.context_size*self.embedding_dim))\n",
    "        #print(embeds.size())\n",
    "        out = F.relu(self.linear1(embeds))\n",
    "        out = self.linear2(out)\n",
    "        return out\n",
    "    def getEmbeds(self,wordVec):\n",
    "        return self.embeddings(wordVec)\n",
    "        \n",
    "cbow = CBOWModel(VOCAB_SIZE,EMBED_DIM,CONTEXT_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 0.11427265455376527 %\n",
      "Progress: 0.22854530910753054 %\n",
      "Progress: 0.34281796366129585 %\n",
      "Progress: 0.4570906182150611 %\n",
      "Progress: 0.5713632727688265 %\n",
      "Progress: 0.6856359273225917 %\n",
      "Progress: 0.799908581876357 %\n",
      "Progress: 0.9141812364301222 %\n",
      "Progress: 1.0284538909838876 %\n",
      "Progress: 1.142726545537653 %\n",
      "Progress: 1.256999200091418 %\n",
      "Progress: 1.3712718546451834 %\n",
      "Progress: 1.4855445091989488 %\n",
      "Progress: 1.599817163752714 %\n",
      "Progress: 1.7140898183064794 %\n",
      "Progress: 1.8283624728602443 %\n",
      "Progress: 1.9426351274140097 %\n",
      "Progress: 2.0569077819677752 %\n",
      "Progress: 2.1711804365215404 %\n",
      "Progress: 2.285453091075306 %\n",
      "Progress: 2.399725745629071 %\n",
      "Progress: 2.513998400182836 %\n",
      "Progress: 2.6282710547366013 %\n",
      "Progress: 2.742543709290367 %\n",
      "Progress: 2.856816363844132 %\n",
      "Progress: 2.9710890183978975 %\n",
      "Progress: 3.0853616729516626 %\n",
      "Progress: 3.199634327505428 %\n",
      "Progress: 3.3139069820591933 %\n",
      "Progress: 3.428179636612959 %\n",
      "Progress: 3.5424522911667236 %\n",
      "Progress: 3.6567249457204887 %\n",
      "Progress: 3.7709976002742542 %\n",
      "Progress: 3.8852702548280194 %\n",
      "Progress: 3.999542909381785 %\n",
      "Progress: 4.1138155639355505 %\n",
      "Progress: 4.228088218489315 %\n",
      "Progress: 4.342360873043081 %\n",
      "Progress: 4.456633527596846 %\n",
      "Progress: 4.570906182150612 %\n",
      "Progress: 4.6851788367043765 %\n",
      "Progress: 4.799451491258142 %\n",
      "Progress: 4.913724145811908 %\n",
      "Progress: 5.027996800365672 %\n",
      "Progress: 5.142269454919438 %\n",
      "Progress: 5.2565421094732026 %\n",
      "Progress: 5.370814764026968 %\n",
      "Progress: 5.485087418580734 %\n",
      "Progress: 5.599360073134499 %\n",
      "Progress: 5.713632727688264 %\n",
      "Progress: 5.8279053822420295 %\n",
      "Progress: 5.942178036795795 %\n",
      "Progress: 6.056450691349561 %\n",
      "Progress: 6.170723345903325 %\n",
      "Progress: 6.284996000457091 %\n",
      "Progress: 6.399268655010856 %\n",
      "Progress: 6.513541309564622 %\n",
      "Progress: 6.627813964118387 %\n",
      "Progress: 6.742086618672152 %\n",
      "Progress: 6.856359273225918 %\n",
      "Progress: 6.970631927779683 %\n",
      "Progress: 7.084904582333447 %\n",
      "Progress: 7.199177236887213 %\n",
      "Progress: 7.313449891440977 %\n",
      "Progress: 7.427722545994743 %\n",
      "Progress: 7.5419952005485085 %\n",
      "Progress: 7.656267855102274 %\n",
      "Progress: 7.770540509656039 %\n",
      "Progress: 7.884813164209804 %\n",
      "Progress: 7.99908581876357 %\n",
      "Progress: 8.113358473317335 %\n",
      "Progress: 8.227631127871101 %\n",
      "Progress: 8.341903782424867 %\n",
      "Progress: 8.45617643697863 %\n",
      "Progress: 8.570449091532396 %\n",
      "Progress: 8.684721746086161 %\n",
      "Progress: 8.798994400639927 %\n",
      "Progress: 8.913267055193693 %\n",
      "Progress: 9.027539709747458 %\n",
      "Progress: 9.141812364301224 %\n",
      "Progress: 9.25608501885499 %\n",
      "Progress: 9.370357673408753 %\n",
      "Progress: 9.484630327962519 %\n",
      "Progress: 9.598902982516284 %\n",
      "Progress: 9.71317563707005 %\n",
      "Progress: 9.827448291623815 %\n",
      "Progress: 9.941720946177579 %\n",
      "Progress: 10.055993600731345 %\n",
      "Progress: 10.17026625528511 %\n",
      "Progress: 10.284538909838876 %\n",
      "Progress: 10.398811564392641 %\n",
      "Progress: 10.513084218946405 %\n",
      "Progress: 10.62735687350017 %\n",
      "Progress: 10.741629528053936 %\n",
      "Progress: 10.855902182607702 %\n",
      "Progress: 10.970174837161467 %\n",
      "Progress: 11.084447491715233 %\n",
      "Progress: 11.198720146268998 %\n",
      "Progress: 11.312992800822762 %\n",
      "Progress: 11.427265455376528 %\n",
      "Progress: 11.541538109930293 %\n",
      "Progress: 11.655810764484059 %\n",
      "Progress: 11.770083419037825 %\n",
      "Progress: 11.88435607359159 %\n",
      "Progress: 11.998628728145356 %\n",
      "Progress: 12.112901382699121 %\n",
      "Progress: 12.227174037252887 %\n",
      "Progress: 12.34144669180665 %\n",
      "Progress: 12.455719346360416 %\n",
      "Progress: 12.569992000914182 %\n",
      "Progress: 12.684264655467947 %\n",
      "Progress: 12.798537310021713 %\n",
      "Progress: 12.912809964575478 %\n",
      "Progress: 13.027082619129244 %\n",
      "Progress: 13.141355273683008 %\n",
      "Progress: 13.255627928236773 %\n",
      "Progress: 13.369900582790539 %\n",
      "Progress: 13.484173237344304 %\n",
      "Progress: 13.59844589189807 %\n",
      "Progress: 13.712718546451836 %\n",
      "Progress: 13.826991201005601 %\n",
      "Progress: 13.941263855559367 %\n",
      "Progress: 14.055536510113129 %\n",
      "Progress: 14.169809164666894 %\n",
      "Progress: 14.28408181922066 %\n",
      "Progress: 14.398354473774425 %\n",
      "Progress: 14.51262712832819 %\n",
      "Progress: 14.626899782881955 %\n",
      "Progress: 14.74117243743572 %\n",
      "Progress: 14.855445091989486 %\n",
      "Progress: 14.969717746543251 %\n",
      "Progress: 15.083990401097017 %\n",
      "Progress: 15.198263055650783 %\n",
      "Progress: 15.312535710204548 %\n",
      "Progress: 15.426808364758312 %\n",
      "Progress: 15.541081019312077 %\n",
      "Progress: 15.655353673865843 %\n",
      "Progress: 15.769626328419609 %\n",
      "Progress: 15.883898982973374 %\n",
      "Progress: 15.99817163752714 %\n",
      "Progress: 16.112444292080905 %\n",
      "Progress: 16.22671694663467 %\n",
      "Progress: 16.340989601188436 %\n",
      "Progress: 16.455262255742202 %\n",
      "Progress: 16.569534910295967 %\n",
      "Progress: 16.683807564849733 %\n",
      "Progress: 16.798080219403495 %\n",
      "Progress: 16.91235287395726 %\n",
      "Progress: 17.026625528511026 %\n",
      "Progress: 17.14089818306479 %\n",
      "Progress: 17.255170837618557 %\n",
      "Progress: 17.369443492172323 %\n",
      "Progress: 17.48371614672609 %\n",
      "Progress: 17.597988801279854 %\n",
      "Progress: 17.71226145583362 %\n",
      "Progress: 17.826534110387385 %\n",
      "Progress: 17.94080676494115 %\n",
      "Progress: 18.055079419494916 %\n",
      "Progress: 18.169352074048682 %\n",
      "Progress: 18.283624728602447 %\n",
      "Progress: 18.397897383156213 %\n",
      "Progress: 18.51217003770998 %\n",
      "Progress: 18.62644269226374 %\n",
      "Progress: 18.740715346817506 %\n",
      "Progress: 18.85498800137127 %\n",
      "Progress: 18.969260655925037 %\n",
      "Progress: 19.083533310478803 %\n",
      "Progress: 19.19780596503257 %\n",
      "Progress: 19.312078619586334 %\n",
      "Progress: 19.4263512741401 %\n",
      "Progress: 19.540623928693865 %\n",
      "Progress: 19.65489658324763 %\n",
      "Progress: 19.769169237801393 %\n",
      "Progress: 19.883441892355158 %\n",
      "Progress: 19.997714546908924 %\n",
      "Progress: 20.11198720146269 %\n",
      "Progress: 20.226259856016455 %\n",
      "Progress: 20.34053251057022 %\n",
      "Progress: 20.454805165123986 %\n",
      "Progress: 20.56907781967775 %\n",
      "Progress: 20.683350474231517 %\n",
      "Progress: 20.797623128785283 %\n",
      "Progress: 20.911895783339045 %\n",
      "Progress: 21.02616843789281 %\n",
      "Progress: 21.140441092446576 %\n",
      "Progress: 21.25471374700034 %\n",
      "Progress: 21.368986401554107 %\n",
      "Progress: 21.483259056107872 %\n",
      "Progress: 21.597531710661638 %\n",
      "Progress: 21.711804365215404 %\n",
      "Progress: 21.82607701976917 %\n",
      "Progress: 21.940349674322935 %\n",
      "Progress: 22.0546223288767 %\n",
      "Progress: 22.168894983430466 %\n",
      "Progress: 22.28316763798423 %\n",
      "Progress: 22.397440292537997 %\n",
      "Progress: 22.511712947091763 %\n",
      "Progress: 22.625985601645525 %\n",
      "Progress: 22.74025825619929 %\n",
      "Progress: 22.854530910753056 %\n",
      "Progress: 22.96880356530682 %\n",
      "Progress: 23.083076219860587 %\n",
      "Progress: 23.197348874414352 %\n",
      "Progress: 23.311621528968118 %\n",
      "Progress: 23.425894183521883 %\n",
      "Progress: 23.54016683807565 %\n",
      "Progress: 23.654439492629415 %\n",
      "Progress: 23.76871214718318 %\n",
      "Progress: 23.882984801736946 %\n",
      "Progress: 23.99725745629071 %\n",
      "Progress: 24.111530110844477 %\n",
      "Progress: 24.225802765398242 %\n",
      "Progress: 24.340075419952008 %\n",
      "Progress: 24.454348074505774 %\n",
      "Progress: 24.568620729059536 %\n",
      "Progress: 24.6828933836133 %\n",
      "Progress: 24.797166038167067 %\n",
      "Progress: 24.911438692720832 %\n",
      "Progress: 25.025711347274598 %\n",
      "Progress: 25.139984001828363 %\n",
      "Progress: 25.25425665638213 %\n",
      "Progress: 25.368529310935894 %\n",
      "Progress: 25.48280196548966 %\n",
      "Progress: 25.597074620043426 %\n",
      "Progress: 25.71134727459719 %\n",
      "Progress: 25.825619929150957 %\n",
      "Progress: 25.939892583704722 %\n",
      "Progress: 26.054165238258488 %\n",
      "Progress: 26.168437892812253 %\n",
      "Progress: 26.282710547366015 %\n",
      "Progress: 26.39698320191978 %\n",
      "Progress: 26.511255856473547 %\n",
      "Progress: 26.625528511027312 %\n",
      "Progress: 26.739801165581078 %\n",
      "Progress: 26.854073820134843 %\n",
      "Progress: 26.96834647468861 %\n",
      "Progress: 27.082619129242374 %\n",
      "Progress: 27.19689178379614 %\n",
      "Progress: 27.311164438349905 %\n",
      "Progress: 27.42543709290367 %\n",
      "Progress: 27.539709747457437 %\n",
      "Progress: 27.653982402011202 %\n",
      "Progress: 27.768255056564968 %\n",
      "Progress: 27.882527711118733 %\n",
      "Progress: 27.9968003656725 %\n",
      "Progress: 28.111073020226257 %\n",
      "Progress: 28.225345674780023 %\n",
      "Progress: 28.33961832933379 %\n",
      "Progress: 28.453890983887554 %\n",
      "Progress: 28.56816363844132 %\n",
      "Progress: 28.682436292995085 %\n",
      "Progress: 28.79670894754885 %\n",
      "Progress: 28.910981602102616 %\n",
      "Progress: 29.02525425665638 %\n",
      "Progress: 29.139526911210144 %\n",
      "Progress: 29.25379956576391 %\n",
      "Progress: 29.368072220317675 %\n",
      "Progress: 29.48234487487144 %\n",
      "Progress: 29.596617529425206 %\n",
      "Progress: 29.71089018397897 %\n",
      "Progress: 29.825162838532737 %\n",
      "Progress: 29.939435493086503 %\n",
      "Progress: 30.05370814764027 %\n",
      "Progress: 30.167980802194034 %\n",
      "Progress: 30.2822534567478 %\n",
      "Progress: 30.396526111301565 %\n",
      "Progress: 30.51079876585533 %\n",
      "Progress: 30.625071420409096 %\n",
      "Progress: 30.739344074962858 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 30.853616729516624 %\n",
      "Progress: 30.96788938407039 %\n",
      "Progress: 31.082162038624155 %\n",
      "Progress: 31.19643469317792 %\n",
      "Progress: 31.310707347731686 %\n",
      "Progress: 31.42498000228545 %\n",
      "Progress: 31.539252656839217 %\n",
      "Progress: 31.653525311392983 %\n",
      "Progress: 31.767797965946748 %\n",
      "Progress: 31.882070620500514 %\n",
      "Progress: 31.99634327505428 %\n",
      "Progress: 32.110615929608045 %\n",
      "Progress: 32.22488858416181 %\n",
      "Progress: 32.339161238715576 %\n",
      "Progress: 32.45343389326934 %\n",
      "Progress: 32.56770654782311 %\n",
      "Progress: 32.68197920237687 %\n",
      "Progress: 32.79625185693064 %\n",
      "Progress: 32.910524511484404 %\n",
      "Progress: 33.02479716603817 %\n",
      "Progress: 33.139069820591935 %\n",
      "Progress: 33.2533424751457 %\n",
      "Progress: 33.367615129699466 %\n",
      "Progress: 33.48188778425323 %\n",
      "Progress: 33.59616043880699 %\n",
      "Progress: 33.710433093360756 %\n",
      "Progress: 33.82470574791452 %\n",
      "Progress: 33.93897840246829 %\n",
      "Progress: 34.05325105702205 %\n",
      "Progress: 34.16752371157582 %\n",
      "Progress: 34.28179636612958 %\n",
      "Progress: 34.39606902068335 %\n",
      "Progress: 34.510341675237115 %\n",
      "Progress: 34.62461432979088 %\n",
      "Progress: 34.738886984344646 %\n",
      "Progress: 34.85315963889841 %\n",
      "Progress: 34.96743229345218 %\n",
      "Progress: 35.08170494800594 %\n",
      "Progress: 35.19597760255971 %\n",
      "Progress: 35.31025025711347 %\n",
      "Progress: 35.42452291166724 %\n",
      "Progress: 35.538795566221005 %\n",
      "Progress: 35.65306822077477 %\n",
      "Progress: 35.767340875328536 %\n",
      "Progress: 35.8816135298823 %\n",
      "Progress: 35.99588618443607 %\n",
      "Progress: 36.11015883898983 %\n",
      "Progress: 36.2244314935436 %\n",
      "Progress: 36.338704148097364 %\n",
      "Progress: 36.45297680265113 %\n",
      "Progress: 36.567249457204895 %\n",
      "Progress: 36.68152211175866 %\n",
      "Progress: 36.795794766312426 %\n",
      "Progress: 36.91006742086619 %\n",
      "Progress: 37.02434007541996 %\n",
      "Progress: 37.138612729973715 %\n",
      "Progress: 37.25288538452748 %\n",
      "Progress: 37.36715803908125 %\n",
      "Progress: 37.48143069363501 %\n",
      "Progress: 37.59570334818878 %\n",
      "Progress: 37.70997600274254 %\n",
      "Progress: 37.82424865729631 %\n",
      "Progress: 37.938521311850074 %\n",
      "Progress: 38.05279396640384 %\n",
      "Progress: 38.167066620957605 %\n",
      "Progress: 38.28133927551137 %\n",
      "Progress: 38.39561193006514 %\n",
      "Progress: 38.5098845846189 %\n",
      "Progress: 38.62415723917267 %\n",
      "Progress: 38.73842989372643 %\n",
      "Progress: 38.8527025482802 %\n",
      "Progress: 38.966975202833964 %\n",
      "Progress: 39.08124785738773 %\n",
      "Progress: 39.195520511941496 %\n",
      "Progress: 39.30979316649526 %\n",
      "Progress: 39.42406582104902 %\n",
      "Progress: 39.538338475602785 %\n",
      "Progress: 39.65261113015655 %\n",
      "Progress: 39.766883784710316 %\n",
      "Progress: 39.88115643926408 %\n",
      "Progress: 39.99542909381785 %\n",
      "Progress: 40.10970174837161 %\n",
      "Progress: 40.22397440292538 %\n",
      "Progress: 40.338247057479144 %\n",
      "Progress: 40.45251971203291 %\n",
      "Progress: 40.566792366586675 %\n",
      "Progress: 40.68106502114044 %\n",
      "Progress: 40.795337675694206 %\n",
      "Progress: 40.90961033024797 %\n",
      "Progress: 41.02388298480174 %\n",
      "Progress: 41.1381556393555 %\n",
      "Progress: 41.25242829390927 %\n",
      "Progress: 41.366700948463034 %\n",
      "Progress: 41.4809736030168 %\n",
      "Progress: 41.595246257570565 %\n",
      "Progress: 41.709518912124324 %\n",
      "Progress: 41.82379156667809 %\n",
      "Progress: 41.938064221231855 %\n",
      "Progress: 42.05233687578562 %\n",
      "Progress: 42.166609530339386 %\n",
      "Progress: 42.28088218489315 %\n",
      "Progress: 42.39515483944692 %\n",
      "Progress: 42.50942749400068 %\n",
      "Progress: 42.62370014855445 %\n",
      "Progress: 42.737972803108214 %\n",
      "Progress: 42.85224545766198 %\n",
      "Progress: 42.966518112215745 %\n",
      "Progress: 43.08079076676951 %\n",
      "Progress: 43.195063421323276 %\n",
      "Progress: 43.30933607587704 %\n",
      "Progress: 43.42360873043081 %\n",
      "Progress: 43.53788138498457 %\n",
      "Progress: 43.65215403953834 %\n",
      "Progress: 43.766426694092104 %\n",
      "Progress: 43.88069934864587 %\n",
      "Progress: 43.994972003199635 %\n",
      "Progress: 44.1092446577534 %\n",
      "Progress: 44.223517312307166 %\n",
      "Progress: 44.33778996686093 %\n",
      "Progress: 44.4520626214147 %\n",
      "Progress: 44.56633527596846 %\n",
      "Progress: 44.68060793052223 %\n",
      "Progress: 44.794880585075994 %\n",
      "Progress: 44.90915323962976 %\n",
      "Progress: 45.023425894183525 %\n",
      "Progress: 45.13769854873729 %\n",
      "Progress: 45.25197120329105 %\n",
      "Progress: 45.366243857844815 %\n",
      "Progress: 45.48051651239858 %\n",
      "Progress: 45.594789166952346 %\n",
      "Progress: 45.70906182150611 %\n",
      "Progress: 45.82333447605988 %\n",
      "Progress: 45.93760713061364 %\n",
      "Progress: 46.05187978516741 %\n",
      "Progress: 46.16615243972117 %\n",
      "Progress: 46.28042509427494 %\n",
      "Progress: 46.394697748828705 %\n",
      "Progress: 46.50897040338247 %\n",
      "Progress: 46.623243057936236 %\n",
      "Progress: 46.73751571249 %\n",
      "Progress: 46.85178836704377 %\n",
      "Progress: 46.96606102159753 %\n",
      "Progress: 47.0803336761513 %\n",
      "Progress: 47.194606330705064 %\n",
      "Progress: 47.30887898525883 %\n",
      "Progress: 47.423151639812595 %\n",
      "Progress: 47.53742429436636 %\n",
      "Progress: 47.651696948920126 %\n",
      "Progress: 47.76596960347389 %\n",
      "Progress: 47.88024225802766 %\n",
      "Progress: 47.99451491258142 %\n",
      "Progress: 48.10878756713519 %\n",
      "Progress: 48.223060221688954 %\n",
      "Progress: 48.33733287624272 %\n",
      "Progress: 48.451605530796485 %\n",
      "Progress: 48.56587818535025 %\n",
      "Progress: 48.680150839904016 %\n",
      "Progress: 48.79442349445778 %\n",
      "Progress: 48.90869614901155 %\n",
      "Progress: 49.022968803565306 %\n",
      "Progress: 49.13724145811907 %\n",
      "Progress: 49.25151411267284 %\n",
      "Progress: 49.3657867672266 %\n",
      "Progress: 49.48005942178037 %\n",
      "Progress: 49.59433207633413 %\n",
      "Progress: 49.7086047308879 %\n",
      "Progress: 49.822877385441664 %\n",
      "Progress: 49.93715003999543 %\n",
      "Progress: 50.051422694549196 %\n",
      "Progress: 50.165695349102954 %\n",
      "Progress: 50.27996800365673 %\n",
      "Progress: 50.394240658210485 %\n",
      "Progress: 50.50851331276426 %\n",
      "Progress: 50.622785967318016 %\n",
      "Progress: 50.73705862187179 %\n",
      "Progress: 50.85133127642555 %\n",
      "Progress: 50.96560393097932 %\n",
      "Progress: 51.07987658553308 %\n",
      "Progress: 51.19414924008685 %\n",
      "Progress: 51.30842189464061 %\n",
      "Progress: 51.42269454919438 %\n",
      "Progress: 51.53696720374814 %\n",
      "Progress: 51.65123985830191 %\n",
      "Progress: 51.76551251285567 %\n",
      "Progress: 51.879785167409445 %\n",
      "Progress: 51.9940578219632 %\n",
      "Progress: 52.108330476516976 %\n",
      "Progress: 52.222603131070734 %\n",
      "Progress: 52.33687578562451 %\n",
      "Progress: 52.451148440178265 %\n",
      "Progress: 52.56542109473203 %\n",
      "Progress: 52.679693749285796 %\n",
      "Progress: 52.79396640383956 %\n",
      "Progress: 52.90823905839333 %\n",
      "Progress: 53.02251171294709 %\n",
      "Progress: 53.13678436750086 %\n",
      "Progress: 53.251057022054624 %\n",
      "Progress: 53.36532967660838 %\n",
      "Progress: 53.479602331162155 %\n",
      "Progress: 53.593874985715914 %\n",
      "Progress: 53.70814764026969 %\n",
      "Progress: 53.822420294823445 %\n",
      "Progress: 53.93669294937722 %\n",
      "Progress: 54.050965603930976 %\n",
      "Progress: 54.16523825848475 %\n",
      "Progress: 54.27951091303851 %\n",
      "Progress: 54.39378356759228 %\n",
      "Progress: 54.50805622214604 %\n",
      "Progress: 54.62232887669981 %\n",
      "Progress: 54.73660153125357 %\n",
      "Progress: 54.85087418580734 %\n",
      "Progress: 54.9651468403611 %\n",
      "Progress: 55.07941949491487 %\n",
      "Progress: 55.19369214946863 %\n",
      "Progress: 55.307964804022404 %\n",
      "Progress: 55.42223745857616 %\n",
      "Progress: 55.536510113129935 %\n",
      "Progress: 55.650782767683694 %\n",
      "Progress: 55.76505542223747 %\n",
      "Progress: 55.879328076791225 %\n",
      "Progress: 55.993600731345 %\n",
      "Progress: 56.107873385898756 %\n",
      "Progress: 56.222146040452515 %\n",
      "Progress: 56.33641869500629 %\n",
      "Progress: 56.450691349560046 %\n",
      "Progress: 56.56496400411382 %\n",
      "Progress: 56.67923665866758 %\n",
      "Progress: 56.79350931322135 %\n",
      "Progress: 56.90778196777511 %\n",
      "Progress: 57.02205462232888 %\n",
      "Progress: 57.13632727688264 %\n",
      "Progress: 57.250599931436405 %\n",
      "Progress: 57.36487258599017 %\n",
      "Progress: 57.479145240543936 %\n",
      "Progress: 57.5934178950977 %\n",
      "Progress: 57.70769054965147 %\n",
      "Progress: 57.82196320420523 %\n",
      "Progress: 57.936235858759 %\n",
      "Progress: 58.05050851331276 %\n",
      "Progress: 58.16478116786653 %\n",
      "Progress: 58.27905382242029 %\n",
      "Progress: 58.39332647697406 %\n",
      "Progress: 58.50759913152782 %\n",
      "Progress: 58.62187178608159 %\n",
      "Progress: 58.73614444063535 %\n",
      "Progress: 58.85041709518912 %\n",
      "Progress: 58.96468974974288 %\n",
      "Progress: 59.078962404296654 %\n",
      "Progress: 59.19323505885041 %\n",
      "Progress: 59.307507713404185 %\n",
      "Progress: 59.42178036795794 %\n",
      "Progress: 59.536053022511716 %\n",
      "Progress: 59.650325677065474 %\n",
      "Progress: 59.76459833161925 %\n",
      "Progress: 59.878870986173006 %\n",
      "Progress: 59.99314364072678 %\n",
      "Progress: 60.10741629528054 %\n",
      "Progress: 60.22168894983431 %\n",
      "Progress: 60.33596160438807 %\n",
      "Progress: 60.45023425894184 %\n",
      "Progress: 60.5645069134956 %\n",
      "Progress: 60.678779568049364 %\n",
      "Progress: 60.79305222260313 %\n",
      "Progress: 60.907324877156896 %\n",
      "Progress: 61.02159753171066 %\n",
      "Progress: 61.13587018626443 %\n",
      "Progress: 61.25014284081819 %\n",
      "Progress: 61.36441549537196 %\n",
      "Progress: 61.478688149925716 %\n",
      "Progress: 61.59296080447949 %\n",
      "Progress: 61.70723345903325 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 61.82150611358702 %\n",
      "Progress: 61.93577876814078 %\n",
      "Progress: 62.05005142269455 %\n",
      "Progress: 62.16432407724831 %\n",
      "Progress: 62.27859673180208 %\n",
      "Progress: 62.39286938635584 %\n",
      "Progress: 62.50714204090961 %\n",
      "Progress: 62.62141469546337 %\n",
      "Progress: 62.735687350017145 %\n",
      "Progress: 62.8499600045709 %\n",
      "Progress: 62.964232659124676 %\n",
      "Progress: 63.078505313678434 %\n",
      "Progress: 63.19277796823221 %\n",
      "Progress: 63.307050622785965 %\n",
      "Progress: 63.42132327733974 %\n",
      "Progress: 63.535595931893496 %\n",
      "Progress: 63.64986858644727 %\n",
      "Progress: 63.76414124100103 %\n",
      "Progress: 63.8784138955548 %\n",
      "Progress: 63.99268655010856 %\n",
      "Progress: 64.10695920466233 %\n",
      "Progress: 64.22123185921609 %\n",
      "Progress: 64.33550451376986 %\n",
      "Progress: 64.44977716832362 %\n",
      "Progress: 64.5640498228774 %\n",
      "Progress: 64.67832247743115 %\n",
      "Progress: 64.79259513198492 %\n",
      "Progress: 64.90686778653868 %\n",
      "Progress: 65.02114044109246 %\n",
      "Progress: 65.13541309564621 %\n",
      "Progress: 65.24968575019999 %\n",
      "Progress: 65.36395840475375 %\n",
      "Progress: 65.47823105930752 %\n",
      "Progress: 65.59250371386128 %\n",
      "Progress: 65.70677636841505 %\n",
      "Progress: 65.82104902296881 %\n",
      "Progress: 65.93532167752258 %\n",
      "Progress: 66.04959433207634 %\n",
      "Progress: 66.1638669866301 %\n",
      "Progress: 66.27813964118387 %\n",
      "Progress: 66.39241229573763 %\n",
      "Progress: 66.5066849502914 %\n",
      "Progress: 66.62095760484516 %\n",
      "Progress: 66.73523025939893 %\n",
      "Progress: 66.84950291395269 %\n",
      "Progress: 66.96377556850646 %\n",
      "Progress: 67.07804822306022 %\n",
      "Progress: 67.19232087761398 %\n",
      "Progress: 67.30659353216775 %\n",
      "Progress: 67.42086618672151 %\n",
      "Progress: 67.53513884127528 %\n",
      "Progress: 67.64941149582904 %\n",
      "Progress: 67.7636841503828 %\n",
      "Progress: 67.87795680493657 %\n",
      "Progress: 67.99222945949033 %\n",
      "Progress: 68.1065021140441 %\n",
      "Progress: 68.22077476859786 %\n",
      "Progress: 68.33504742315164 %\n",
      "Progress: 68.4493200777054 %\n",
      "Progress: 68.56359273225917 %\n",
      "Progress: 68.67786538681293 %\n",
      "Progress: 68.7921380413667 %\n",
      "Progress: 68.90641069592046 %\n",
      "Progress: 69.02068335047423 %\n",
      "Progress: 69.13495600502799 %\n",
      "Progress: 69.24922865958176 %\n",
      "Progress: 69.36350131413552 %\n",
      "Progress: 69.47777396868929 %\n",
      "Progress: 69.59204662324305 %\n",
      "Progress: 69.70631927779682 %\n",
      "Progress: 69.82059193235058 %\n",
      "Progress: 69.93486458690435 %\n",
      "Progress: 70.04913724145811 %\n",
      "Progress: 70.16340989601188 %\n",
      "Progress: 70.27768255056564 %\n",
      "Progress: 70.39195520511942 %\n",
      "Progress: 70.50622785967317 %\n",
      "Progress: 70.62050051422695 %\n",
      "Progress: 70.7347731687807 %\n",
      "Progress: 70.84904582333448 %\n",
      "Progress: 70.96331847788824 %\n",
      "Progress: 71.07759113244201 %\n",
      "Progress: 71.19186378699577 %\n",
      "Progress: 71.30613644154954 %\n",
      "Progress: 71.4204090961033 %\n",
      "Progress: 71.53468175065707 %\n",
      "Progress: 71.64895440521083 %\n",
      "Progress: 71.7632270597646 %\n",
      "Progress: 71.87749971431836 %\n",
      "Progress: 71.99177236887213 %\n",
      "Progress: 72.10604502342589 %\n",
      "Progress: 72.22031767797966 %\n",
      "Progress: 72.33459033253342 %\n",
      "Progress: 72.4488629870872 %\n",
      "Progress: 72.56313564164095 %\n",
      "Progress: 72.67740829619473 %\n",
      "Progress: 72.79168095074849 %\n",
      "Progress: 72.90595360530226 %\n",
      "Progress: 73.02022625985602 %\n",
      "Progress: 73.13449891440979 %\n",
      "Progress: 73.24877156896355 %\n",
      "Progress: 73.36304422351732 %\n",
      "Progress: 73.47731687807108 %\n",
      "Progress: 73.59158953262485 %\n",
      "Progress: 73.70586218717861 %\n",
      "Progress: 73.82013484173238 %\n",
      "Progress: 73.93440749628614 %\n",
      "Progress: 74.04868015083991 %\n",
      "Progress: 74.16295280539367 %\n",
      "Progress: 74.27722545994743 %\n",
      "Progress: 74.3914981145012 %\n",
      "Progress: 74.50577076905496 %\n",
      "Progress: 74.62004342360873 %\n",
      "Progress: 74.7343160781625 %\n",
      "Progress: 74.84858873271627 %\n",
      "Progress: 74.96286138727002 %\n",
      "Progress: 75.0771340418238 %\n",
      "Progress: 75.19140669637756 %\n",
      "Progress: 75.30567935093131 %\n",
      "Progress: 75.41995200548509 %\n",
      "Progress: 75.53422466003884 %\n",
      "Progress: 75.64849731459262 %\n",
      "Progress: 75.76276996914638 %\n",
      "Progress: 75.87704262370015 %\n",
      "Progress: 75.99131527825391 %\n",
      "Progress: 76.10558793280768 %\n",
      "Progress: 76.21986058736144 %\n",
      "Progress: 76.33413324191521 %\n",
      "Progress: 76.44840589646897 %\n",
      "Progress: 76.56267855102274 %\n",
      "Progress: 76.6769512055765 %\n",
      "Progress: 76.79122386013027 %\n",
      "Progress: 76.90549651468403 %\n",
      "Progress: 77.0197691692378 %\n",
      "Progress: 77.13404182379156 %\n",
      "Progress: 77.24831447834534 %\n",
      "Progress: 77.3625871328991 %\n",
      "Progress: 77.47685978745287 %\n",
      "Progress: 77.59113244200663 %\n",
      "Progress: 77.7054050965604 %\n",
      "Progress: 77.81967775111416 %\n",
      "Progress: 77.93395040566793 %\n",
      "Progress: 78.04822306022169 %\n",
      "Progress: 78.16249571477546 %\n",
      "Progress: 78.27676836932922 %\n",
      "Progress: 78.39104102388299 %\n",
      "Progress: 78.50531367843675 %\n",
      "Progress: 78.61958633299052 %\n",
      "Progress: 78.73385898754428 %\n",
      "Progress: 78.84813164209804 %\n",
      "Progress: 78.96240429665181 %\n",
      "Progress: 79.07667695120557 %\n",
      "Progress: 79.19094960575934 %\n",
      "Progress: 79.3052222603131 %\n",
      "Progress: 79.41949491486687 %\n",
      "Progress: 79.53376756942063 %\n",
      "Progress: 79.6480402239744 %\n",
      "Progress: 79.76231287852816 %\n",
      "Progress: 79.87658553308194 %\n",
      "Progress: 79.9908581876357 %\n",
      "Progress: 80.10513084218947 %\n",
      "Progress: 80.21940349674323 %\n",
      "Progress: 80.333676151297 %\n",
      "Progress: 80.44794880585076 %\n",
      "Progress: 80.56222146040453 %\n",
      "Progress: 80.67649411495829 %\n",
      "Progress: 80.79076676951206 %\n",
      "Progress: 80.90503942406582 %\n",
      "Progress: 81.01931207861959 %\n",
      "Progress: 81.13358473317335 %\n",
      "Progress: 81.24785738772712 %\n",
      "Progress: 81.36213004228088 %\n",
      "Progress: 81.47640269683465 %\n",
      "Progress: 81.59067535138841 %\n",
      "Progress: 81.70494800594219 %\n",
      "Progress: 81.81922066049594 %\n",
      "Progress: 81.93349331504972 %\n",
      "Progress: 82.04776596960347 %\n",
      "Progress: 82.16203862415725 %\n",
      "Progress: 82.276311278711 %\n",
      "Progress: 82.39058393326476 %\n",
      "Progress: 82.50485658781854 %\n",
      "Progress: 82.6191292423723 %\n",
      "Progress: 82.73340189692607 %\n",
      "Progress: 82.84767455147983 %\n",
      "Progress: 82.9619472060336 %\n",
      "Progress: 83.07621986058736 %\n",
      "Progress: 83.19049251514113 %\n",
      "Progress: 83.30476516969489 %\n",
      "Progress: 83.41903782424865 %\n",
      "Progress: 83.53331047880242 %\n",
      "Progress: 83.64758313335618 %\n",
      "Progress: 83.76185578790995 %\n",
      "Progress: 83.87612844246371 %\n",
      "Progress: 83.99040109701748 %\n",
      "Progress: 84.10467375157124 %\n",
      "Progress: 84.21894640612501 %\n",
      "Progress: 84.33321906067877 %\n",
      "Progress: 84.44749171523254 %\n",
      "Progress: 84.5617643697863 %\n",
      "Progress: 84.67603702434008 %\n",
      "Progress: 84.79030967889383 %\n",
      "Progress: 84.9045823334476 %\n",
      "Progress: 85.01885498800137 %\n",
      "Progress: 85.13312764255514 %\n",
      "Progress: 85.2474002971089 %\n",
      "Progress: 85.36167295166267 %\n",
      "Progress: 85.47594560621643 %\n",
      "Progress: 85.5902182607702 %\n",
      "Progress: 85.70449091532396 %\n",
      "Progress: 85.81876356987773 %\n",
      "Progress: 85.93303622443149 %\n",
      "Progress: 86.04730887898526 %\n",
      "Progress: 86.16158153353902 %\n",
      "Progress: 86.2758541880928 %\n",
      "Progress: 86.39012684264655 %\n",
      "Progress: 86.50439949720032 %\n",
      "Progress: 86.61867215175408 %\n",
      "Progress: 86.73294480630786 %\n",
      "Progress: 86.84721746086161 %\n",
      "Progress: 86.96149011541539 %\n",
      "Progress: 87.07576276996915 %\n",
      "Progress: 87.19003542452292 %\n",
      "Progress: 87.30430807907668 %\n",
      "Progress: 87.41858073363045 %\n",
      "Progress: 87.53285338818421 %\n",
      "Progress: 87.64712604273798 %\n",
      "Progress: 87.76139869729174 %\n",
      "Progress: 87.87567135184551 %\n",
      "Progress: 87.98994400639927 %\n",
      "Progress: 88.10421666095304 %\n",
      "Progress: 88.2184893155068 %\n",
      "Progress: 88.33276197006057 %\n",
      "Progress: 88.44703462461433 %\n",
      "Progress: 88.5613072791681 %\n",
      "Progress: 88.67557993372186 %\n",
      "Progress: 88.78985258827564 %\n",
      "Progress: 88.9041252428294 %\n",
      "Progress: 89.01839789738317 %\n",
      "Progress: 89.13267055193693 %\n",
      "Progress: 89.2469432064907 %\n",
      "Progress: 89.36121586104446 %\n",
      "Progress: 89.47548851559823 %\n",
      "Progress: 89.58976117015199 %\n",
      "Progress: 89.70403382470576 %\n",
      "Progress: 89.81830647925952 %\n",
      "Progress: 89.93257913381328 %\n",
      "Progress: 90.04685178836705 %\n",
      "Progress: 90.16112444292081 %\n",
      "Progress: 90.27539709747458 %\n",
      "Progress: 90.38966975202834 %\n",
      "Progress: 90.5039424065821 %\n",
      "Progress: 90.61821506113587 %\n",
      "Progress: 90.73248771568963 %\n",
      "Progress: 90.8467603702434 %\n",
      "Progress: 90.96103302479716 %\n",
      "Progress: 91.07530567935093 %\n",
      "Progress: 91.18957833390469 %\n",
      "Progress: 91.30385098845846 %\n",
      "Progress: 91.41812364301222 %\n",
      "Progress: 91.53239629756598 %\n",
      "Progress: 91.64666895211975 %\n",
      "Progress: 91.76094160667351 %\n",
      "Progress: 91.87521426122728 %\n",
      "Progress: 91.98948691578104 %\n",
      "Progress: 92.10375957033482 %\n",
      "Progress: 92.21803222488857 %\n",
      "Progress: 92.33230487944235 %\n",
      "Progress: 92.4465775339961 %\n",
      "Progress: 92.56085018854988 %\n",
      "Progress: 92.67512284310364 %\n",
      "Progress: 92.78939549765741 %\n",
      "Progress: 92.90366815221117 %\n",
      "Progress: 93.01794080676494 %\n",
      "Progress: 93.1322134613187 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 93.24648611587247 %\n",
      "Progress: 93.36075877042623 %\n",
      "Progress: 93.47503142498 %\n",
      "Progress: 93.58930407953376 %\n",
      "Progress: 93.70357673408753 %\n",
      "Progress: 93.81784938864129 %\n",
      "Progress: 93.93212204319506 %\n",
      "Progress: 94.04639469774882 %\n",
      "Progress: 94.1606673523026 %\n",
      "Progress: 94.27494000685635 %\n",
      "Progress: 94.38921266141013 %\n",
      "Progress: 94.50348531596389 %\n",
      "Progress: 94.61775797051766 %\n",
      "Progress: 94.73203062507142 %\n",
      "Progress: 94.84630327962519 %\n",
      "Progress: 94.96057593417895 %\n",
      "Progress: 95.07484858873272 %\n",
      "Progress: 95.18912124328648 %\n",
      "Progress: 95.30339389784025 %\n",
      "Progress: 95.41766655239401 %\n",
      "Progress: 95.53193920694778 %\n",
      "Progress: 95.64621186150154 %\n",
      "Progress: 95.76048451605531 %\n",
      "Progress: 95.87475717060907 %\n",
      "Progress: 95.98902982516285 %\n",
      "Progress: 96.1033024797166 %\n",
      "Progress: 96.21757513427038 %\n",
      "Progress: 96.33184778882413 %\n",
      "Progress: 96.44612044337791 %\n",
      "Progress: 96.56039309793167 %\n",
      "Progress: 96.67466575248544 %\n",
      "Progress: 96.7889384070392 %\n",
      "Progress: 96.90321106159297 %\n",
      "Progress: 97.01748371614673 %\n",
      "Progress: 97.1317563707005 %\n",
      "Progress: 97.24602902525426 %\n",
      "Progress: 97.36030167980803 %\n",
      "Progress: 97.47457433436179 %\n",
      "Progress: 97.58884698891556 %\n",
      "Progress: 97.70311964346932 %\n",
      "Progress: 97.8173922980231 %\n",
      "Progress: 97.93166495257685 %\n",
      "Progress: 98.04593760713061 %\n",
      "Progress: 98.16021026168438 %\n",
      "Progress: 98.27448291623814 %\n",
      "Progress: 98.38875557079191 %\n",
      "Progress: 98.50302822534567 %\n",
      "Progress: 98.61730087989943 %\n",
      "Progress: 98.7315735344532 %\n",
      "Progress: 98.84584618900696 %\n",
      "Progress: 98.96011884356074 %\n",
      "Progress: 99.0743914981145 %\n",
      "Progress: 99.18866415266827 %\n",
      "Progress: 99.30293680722203 %\n",
      "Progress: 99.4172094617758 %\n",
      "Progress: 99.53148211632956 %\n",
      "Progress: 99.64575477088333 %\n",
      "Progress: 99.76002742543709 %\n",
      "Progress: 99.87430007999086 %\n",
      "Progress: 99.98857273454462 %\n",
      "Progress: 100.10284538909839 %\n",
      "Progress: 100.21711804365216 %\n",
      "Progress: 100.33139069820591 %\n",
      "Progress: 100.44566335275968 %\n",
      "Progress: 100.55993600731345 %\n",
      "Progress: 100.67420866186723 %\n",
      "Progress: 100.78848131642097 %\n",
      "Progress: 100.90275397097474 %\n",
      "Progress: 101.01702662552852 %\n",
      "Progress: 101.13129928008226 %\n",
      "Progress: 101.24557193463603 %\n",
      "Progress: 101.3598445891898 %\n",
      "Progress: 101.47411724374358 %\n",
      "Progress: 101.58838989829732 %\n",
      "Progress: 101.7026625528511 %\n",
      "Progress: 101.81693520740487 %\n",
      "Progress: 101.93120786195864 %\n",
      "Progress: 102.04548051651238 %\n",
      "Progress: 102.15975317106616 %\n",
      "Progress: 102.27402582561993 %\n",
      "Progress: 102.3882984801737 %\n",
      "Progress: 102.50257113472745 %\n",
      "Progress: 102.61684378928122 %\n",
      "Progress: 102.73111644383499 %\n",
      "Progress: 102.84538909838876 %\n",
      "Progress: 102.95966175294251 %\n",
      "Progress: 103.07393440749628 %\n",
      "Progress: 103.18820706205005 %\n",
      "Progress: 103.30247971660383 %\n",
      "Progress: 103.41675237115757 %\n",
      "Progress: 103.53102502571134 %\n",
      "Progress: 103.64529768026512 %\n",
      "Progress: 103.75957033481889 %\n",
      "Progress: 103.87384298937263 %\n",
      "Progress: 103.9881156439264 %\n",
      "Progress: 104.10238829848018 %\n",
      "Progress: 104.21666095303395 %\n",
      "Progress: 104.3309336075877 %\n",
      "Progress: 104.44520626214147 %\n",
      "Progress: 104.55947891669524 %\n",
      "Progress: 104.67375157124901 %\n",
      "Progress: 104.78802422580276 %\n",
      "Progress: 104.90229688035653 %\n",
      "Progress: 105.0165695349103 %\n",
      "Progress: 105.13084218946406 %\n",
      "Progress: 105.24511484401782 %\n",
      "Progress: 105.35938749857159 %\n",
      "Progress: 105.47366015312537 %\n",
      "Progress: 105.58793280767912 %\n",
      "Progress: 105.70220546223288 %\n",
      "Progress: 105.81647811678666 %\n",
      "Progress: 105.93075077134043 %\n",
      "Progress: 106.04502342589419 %\n",
      "Progress: 106.15929608044794 %\n",
      "Progress: 106.27356873500172 %\n",
      "Progress: 106.38784138955548 %\n",
      "Progress: 106.50211404410925 %\n",
      "Progress: 106.616386698663 %\n",
      "Progress: 106.73065935321677 %\n",
      "Progress: 106.84493200777054 %\n",
      "Progress: 106.95920466232431 %\n",
      "Progress: 107.07347731687807 %\n",
      "Progress: 107.18774997143183 %\n",
      "Progress: 107.3020226259856 %\n",
      "Progress: 107.41629528053937 %\n",
      "Progress: 107.53056793509313 %\n",
      "Progress: 107.64484058964689 %\n",
      "Progress: 107.75911324420066 %\n",
      "Progress: 107.87338589875444 %\n",
      "Progress: 107.98765855330818 %\n",
      "Progress: 108.10193120786195 %\n",
      "Progress: 108.21620386241572 %\n",
      "Progress: 108.3304765169695 %\n",
      "Progress: 108.44474917152324 %\n",
      "Progress: 108.55902182607701 %\n",
      "Progress: 108.67329448063079 %\n",
      "Progress: 108.78756713518456 %\n",
      "Progress: 108.9018397897383 %\n",
      "Progress: 109.01611244429208 %\n",
      "Progress: 109.13038509884585 %\n",
      "Progress: 109.24465775339962 %\n",
      "Progress: 109.35893040795337 %\n",
      "Progress: 109.47320306250714 %\n",
      "Progress: 109.58747571706091 %\n",
      "Progress: 109.70174837161468 %\n",
      "Progress: 109.81602102616843 %\n",
      "Progress: 109.9302936807222 %\n",
      "Progress: 110.04456633527597 %\n",
      "Progress: 110.15883898982975 %\n",
      "Progress: 110.27311164438349 %\n",
      "Progress: 110.38738429893726 %\n",
      "Progress: 110.50165695349104 %\n",
      "Progress: 110.61592960804481 %\n",
      "Progress: 110.73020226259855 %\n",
      "Progress: 110.84447491715233 %\n",
      "Progress: 110.9587475717061 %\n",
      "Progress: 111.07302022625987 %\n",
      "Progress: 111.18729288081362 %\n",
      "Progress: 111.30156553536739 %\n",
      "Progress: 111.41583818992116 %\n",
      "Progress: 111.53011084447493 %\n",
      "Progress: 111.64438349902868 %\n",
      "Progress: 111.75865615358245 %\n",
      "Progress: 111.87292880813622 %\n",
      "Progress: 111.98720146269 %\n",
      "Progress: 112.10147411724374 %\n",
      "Progress: 112.21574677179751 %\n",
      "Progress: 112.33001942635129 %\n",
      "Progress: 112.44429208090503 %\n",
      "Progress: 112.5585647354588 %\n",
      "Progress: 112.67283739001257 %\n",
      "Progress: 112.78711004456635 %\n",
      "Progress: 112.90138269912009 %\n",
      "Progress: 113.01565535367386 %\n",
      "Progress: 113.12992800822764 %\n",
      "Progress: 113.2442006627814 %\n",
      "Progress: 113.35847331733515 %\n",
      "Progress: 113.47274597188893 %\n",
      "Progress: 113.5870186264427 %\n",
      "Progress: 113.70129128099646 %\n",
      "Progress: 113.81556393555022 %\n",
      "Progress: 113.92983659010399 %\n",
      "Progress: 114.04410924465776 %\n",
      "Progress: 114.15838189921152 %\n",
      "Progress: 114.27265455376528 %\n",
      "Progress: 114.38692720831905 %\n",
      "Progress: 114.50119986287281 %\n",
      "Progress: 114.61547251742658 %\n",
      "Progress: 114.72974517198034 %\n",
      "Progress: 114.8440178265341 %\n",
      "Progress: 114.95829048108787 %\n",
      "Progress: 115.07256313564164 %\n",
      "Progress: 115.1868357901954 %\n",
      "Progress: 115.30110844474916 %\n",
      "Progress: 115.41538109930293 %\n",
      "Progress: 115.5296537538567 %\n",
      "Progress: 115.64392640841047 %\n",
      "Progress: 115.75819906296422 %\n",
      "Progress: 115.872471717518 %\n",
      "Progress: 115.98674437207177 %\n",
      "Progress: 116.10101702662551 %\n",
      "Progress: 116.21528968117929 %\n",
      "Progress: 116.32956233573306 %\n",
      "Progress: 116.44383499028683 %\n",
      "Progress: 116.55810764484058 %\n",
      "Progress: 116.67238029939435 %\n",
      "Progress: 116.78665295394812 %\n",
      "Progress: 116.9009256085019 %\n",
      "Progress: 117.01519826305564 %\n",
      "Progress: 117.12947091760941 %\n",
      "Progress: 117.24374357216318 %\n",
      "Progress: 117.35801622671696 %\n",
      "Progress: 117.4722888812707 %\n",
      "Progress: 117.58656153582447 %\n",
      "Progress: 117.70083419037825 %\n",
      "Progress: 117.81510684493202 %\n",
      "Progress: 117.92937949948576 %\n",
      "Progress: 118.04365215403953 %\n",
      "Progress: 118.15792480859331 %\n",
      "Progress: 118.27219746314708 %\n",
      "Progress: 118.38647011770082 %\n",
      "Progress: 118.5007427722546 %\n",
      "Progress: 118.61501542680837 %\n",
      "Progress: 118.72928808136214 %\n",
      "Progress: 118.84356073591589 %\n",
      "Progress: 118.95783339046966 %\n",
      "Progress: 119.07210604502343 %\n",
      "Progress: 119.1863786995772 %\n",
      "Progress: 119.30065135413095 %\n",
      "Progress: 119.41492400868472 %\n",
      "Progress: 119.5291966632385 %\n",
      "Progress: 119.64346931779227 %\n",
      "Progress: 119.75774197234601 %\n",
      "Progress: 119.87201462689978 %\n",
      "Progress: 119.98628728145356 %\n",
      "Progress: 120.10055993600733 %\n",
      "Progress: 120.21483259056107 %\n",
      "Progress: 120.32910524511485 %\n",
      "Progress: 120.44337789966862 %\n",
      "Progress: 120.55765055422239 %\n",
      "Progress: 120.67192320877614 %\n",
      "Progress: 120.78619586332991 %\n",
      "Progress: 120.90046851788368 %\n",
      "Progress: 121.01474117243744 %\n",
      "Progress: 121.1290138269912 %\n",
      "Progress: 121.24328648154497 %\n",
      "Progress: 121.35755913609873 %\n",
      "Progress: 121.4718317906525 %\n",
      "Progress: 121.58610444520626 %\n",
      "Progress: 121.70037709976003 %\n",
      "Progress: 121.81464975431379 %\n",
      "Progress: 121.92892240886756 %\n",
      "Progress: 122.04319506342132 %\n",
      "Progress: 122.1574677179751 %\n",
      "Progress: 122.27174037252885 %\n",
      "Progress: 122.38601302708263 %\n",
      "Progress: 122.50028568163638 %\n",
      "Progress: 122.61455833619014 %\n",
      "Progress: 122.72883099074392 %\n",
      "Progress: 122.84310364529769 %\n",
      "Progress: 122.95737629985143 %\n",
      "Progress: 123.0716489544052 %\n",
      "Progress: 123.18592160895898 %\n",
      "Progress: 123.30019426351275 %\n",
      "Progress: 123.4144669180665 %\n",
      "Progress: 123.52873957262027 %\n",
      "Progress: 123.64301222717404 %\n",
      "Progress: 123.7572848817278 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 123.87155753628156 %\n",
      "Progress: 123.98583019083533 %\n",
      "Progress: 124.1001028453891 %\n",
      "Progress: 124.21437549994285 %\n",
      "Progress: 124.32864815449662 %\n",
      "Progress: 124.44292080905039 %\n",
      "Progress: 124.55719346360416 %\n",
      "Progress: 124.67146611815791 %\n",
      "Progress: 124.78573877271168 %\n",
      "Progress: 124.90001142726545 %\n",
      "Progress: 125.01428408181923 %\n"
     ]
    }
   ],
   "source": [
    "l_rs = [0.1,0.25,0.5,1,2]\n",
    "#Code to find ideal learning Rate for the model\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.set_device(0)\n",
    "\n",
    "loss_dict = {}\n",
    "num_epochs = 25\n",
    "iter_ = 0\n",
    "BATCH_SIZE = 128\n",
    "for l_r in l_rs:\n",
    "    losses = []\n",
    "    cbow = CBOWModel(VOCAB_SIZE,EMBED_DIM,CONTEXT_SIZE)\n",
    "    cbow.to(device)\n",
    "    optimizer = torch.optim.SGD(cbow.parameters(), lr=l_r) \n",
    "    print(\"Learning Rate:\",l_r)\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for i,(context,target) in enumerate(train_loader):\n",
    "            context = context.to(device)\n",
    "            target =target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = cbow(context)\n",
    "            target = target.view(-1)\n",
    "            loss = criterion(outputs,target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            iter_  += 1\n",
    "            total_loss +=loss.item()\n",
    "            if iter_%500 ==0:\n",
    "                print(\"Progress:\",iter_*BATCH_SIZE/(num_epochs*len(sample_DS))*100,\"%\")\n",
    "    \n",
    "        losses.append(total_loss)\n",
    "    loss_dict[l_r] = losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 0.597943075819182 %\n",
      "Progress: 1.195886151638364 %\n",
      "Progress: 1.793829227457546 %\n",
      "Progress: 2.391772303276728 %\n",
      "Progress: 2.98971537909591 %\n",
      "Progress: 3.587658454915092 %\n",
      "Progress: 4.1856015307342735 %\n",
      "Progress: 4.783544606553456 %\n",
      "Progress: 5.381487682372638 %\n",
      "Progress: 5.97943075819182 %\n",
      "Progress: 6.577373834011002 %\n",
      "Progress: 7.175316909830184 %\n",
      "Progress: 7.773259985649367 %\n",
      "Progress: 8.371203061468547 %\n",
      "Progress: 8.96914613728773 %\n",
      "Progress: 9.567089213106913 %\n",
      "Progress: 10.165032288926094 %\n",
      "Progress: 10.762975364745277 %\n",
      "Progress: 11.36091844056446 %\n",
      "Progress: 11.95886151638364 %\n",
      "Progress: 12.556804592202822 %\n",
      "Progress: 13.154747668022004 %\n",
      "Progress: 13.752690743841187 %\n",
      "Progress: 14.350633819660368 %\n",
      "Progress: 14.948576895479551 %\n",
      "Progress: 15.546519971298734 %\n",
      "Progress: 16.144463047117917 %\n",
      "Progress: 16.742406122937094 %\n",
      "Progress: 17.34034919875628 %\n",
      "Progress: 17.93829227457546 %\n",
      "Progress: 18.53623535039464 %\n",
      "Progress: 19.134178426213825 %\n",
      "Progress: 19.732121502033007 %\n",
      "Progress: 20.330064577852188 %\n",
      "Progress: 20.928007653671372 %\n",
      "Progress: 21.525950729490553 %\n",
      "Progress: 22.123893805309734 %\n",
      "Progress: 22.72183688112892 %\n",
      "Progress: 23.319779956948096 %\n",
      "Progress: 23.91772303276728 %\n",
      "Progress: 24.515666108586462 %\n",
      "Progress: 25.113609184405643 %\n",
      "Progress: 25.711552260224828 %\n",
      "Progress: 26.30949533604401 %\n",
      "Progress: 26.90743841186319 %\n",
      "Progress: 27.505381487682374 %\n",
      "Progress: 28.103324563501552 %\n",
      "Progress: 28.701267639320736 %\n",
      "Progress: 29.299210715139917 %\n",
      "Progress: 29.897153790959102 %\n",
      "Progress: 30.495096866778283 %\n",
      "Progress: 31.093039942597468 %\n",
      "Progress: 31.690983018416645 %\n",
      "Progress: 32.28892609423583 %\n",
      "Progress: 32.88686917005501 %\n",
      "Progress: 33.48481224587419 %\n",
      "Progress: 34.08275532169338 %\n",
      "Progress: 34.68069839751256 %\n",
      "Progress: 35.27864147333174 %\n",
      "Progress: 35.87658454915092 %\n",
      "Progress: 36.47452762497011 %\n",
      "Progress: 37.07247070078928 %\n",
      "Progress: 37.67041377660847 %\n",
      "Progress: 38.26835685242765 %\n",
      "Progress: 38.86629992824683 %\n",
      "Progress: 39.46424300406601 %\n",
      "Progress: 40.062186079885194 %\n",
      "Progress: 40.660129155704375 %\n",
      "Progress: 41.258072231523556 %\n",
      "Progress: 41.856015307342744 %\n",
      "Progress: 42.45395838316192 %\n",
      "Progress: 43.051901458981106 %\n",
      "Progress: 43.64984453480029 %\n",
      "Progress: 44.24778761061947 %\n",
      "Progress: 44.84573068643865 %\n",
      "Progress: 45.44367376225784 %\n",
      "Progress: 46.04161683807702 %\n",
      "Progress: 46.63955991389619 %\n",
      "Progress: 47.23750298971538 %\n",
      "Progress: 47.83544606553456 %\n",
      "Progress: 48.43338914135374 %\n",
      "Progress: 49.031332217172924 %\n",
      "Progress: 49.62927529299211 %\n",
      "Progress: 50.227218368811286 %\n",
      "Progress: 50.825161444630474 %\n",
      "Progress: 51.423104520449655 %\n",
      "Progress: 52.02104759626883 %\n",
      "Progress: 52.61899067208802 %\n",
      "Progress: 53.216933747907206 %\n",
      "Progress: 53.81487682372638 %\n",
      "Progress: 54.41281989954556 %\n",
      "Progress: 55.01076297536475 %\n",
      "Progress: 55.60870605118393 %\n",
      "Progress: 56.206649127003104 %\n",
      "Progress: 56.80459220282229 %\n",
      "Progress: 57.40253527864147 %\n",
      "Progress: 58.00047835446066 %\n",
      "Progress: 58.598421430279835 %\n",
      "Progress: 59.196364506099016 %\n",
      "Progress: 59.794307581918204 %\n",
      "Progress: 60.39225065773738 %\n",
      "Progress: 60.990193733556566 %\n",
      "Progress: 61.58813680937575 %\n",
      "Progress: 62.186079885194935 %\n",
      "Progress: 62.78402296101411 %\n",
      "Progress: 63.38196603683329 %\n",
      "Progress: 63.97990911265248 %\n",
      "Progress: 64.57785218847167 %\n",
      "Progress: 65.17579526429084 %\n",
      "Progress: 65.77373834011001 %\n",
      "Progress: 66.3716814159292 %\n",
      "Progress: 66.96962449174838 %\n",
      "Progress: 67.56756756756756 %\n",
      "Progress: 68.16551064338675 %\n",
      "Progress: 68.76345371920594 %\n",
      "Progress: 69.36139679502512 %\n",
      "Progress: 69.95933987084429 %\n",
      "Progress: 70.55728294666348 %\n",
      "Progress: 71.15522602248267 %\n",
      "Progress: 71.75316909830184 %\n",
      "Progress: 72.35111217412103 %\n",
      "Progress: 72.94905524994022 %\n",
      "Progress: 73.54699832575939 %\n",
      "Progress: 74.14494140157856 %\n",
      "Progress: 74.74288447739775 %\n",
      "Progress: 75.34082755321694 %\n",
      "Progress: 75.93877062903611 %\n",
      "Progress: 76.5367137048553 %\n",
      "Progress: 77.13465678067448 %\n",
      "Progress: 77.73259985649366 %\n",
      "Progress: 78.33054293231284 %\n",
      "Progress: 78.92848600813203 %\n",
      "Progress: 79.52642908395121 %\n",
      "Progress: 80.12437215977039 %\n",
      "Progress: 80.72231523558958 %\n",
      "Progress: 81.32025831140875 %\n",
      "Progress: 81.91820138722794 %\n",
      "Progress: 82.51614446304711 %\n",
      "Progress: 83.1140875388663 %\n",
      "Progress: 83.71203061468549 %\n",
      "Progress: 84.30997369050466 %\n",
      "Progress: 84.90791676632384 %\n",
      "Progress: 85.50585984214302 %\n",
      "Progress: 86.10380291796221 %\n",
      "Progress: 86.70174599378139 %\n",
      "Progress: 87.29968906960057 %\n",
      "Progress: 87.89763214541976 %\n",
      "Progress: 88.49557522123894 %\n",
      "Progress: 89.09351829705811 %\n",
      "Progress: 89.6914613728773 %\n",
      "Progress: 90.28940444869649 %\n",
      "Progress: 90.88734752451568 %\n",
      "Progress: 91.48529060033485 %\n",
      "Progress: 92.08323367615404 %\n",
      "Progress: 92.68117675197321 %\n",
      "Progress: 93.27911982779239 %\n",
      "Progress: 93.87706290361157 %\n",
      "Progress: 94.47500597943076 %\n",
      "Progress: 95.07294905524995 %\n",
      "Progress: 95.67089213106912 %\n",
      "Progress: 96.2688352068883 %\n",
      "Progress: 96.86677828270749 %\n",
      "Progress: 97.46472135852667 %\n",
      "Progress: 98.06266443434585 %\n",
      "Progress: 98.66060751016504 %\n",
      "Progress: 99.25855058598422 %\n",
      "Progress: 99.8564936618034 %\n",
      "[26242.655499458313, 25146.57937479019, 24647.42048215866, 24312.810389995575, 24394.054433345795, 24078.349966049194, 23865.17521238327, 23682.625658988953, 23925.678809165955, 23681.727396011353, 23519.179578781128, 23381.494799613953, 23644.875141620636, 23433.239610671997, 23298.507628440857, 23174.652583122253, 23450.11530637741, 23256.032232284546, 23126.492175102234, 23020.037919044495]\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "cbow = CBOWModel(VOCAB_SIZE,EMBED_DIM,CONTEXT_SIZE)\n",
    "torch.cuda.set_device(0)\n",
    "cbow.to(device)\n",
    "losses = []\n",
    "num_epochs = 20\n",
    "iter_ = 0\n",
    "BATCH_SIZE = 128\n",
    "learning_rate = 0.8\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(cbow.parameters(), lr=learning_rate)  \n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    if epoch%4 == 0:\n",
    "        train_loader = loadData(df[\"ingredients\"],BATCH_SIZE)\n",
    "    for i,(context,target) in enumerate(train_loader):\n",
    "        context = context.to(device)\n",
    "        target =target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = cbow(context)\n",
    "        target = target.view(-1)\n",
    "        loss = criterion(outputs,target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        iter_  += 1\n",
    "        total_loss +=loss.item()\n",
    "        if iter_%500 ==0:\n",
    "            print(\"Progress:\",iter_/(num_epochs*len(train_loader))*100,\"%\")\n",
    "    \n",
    "    losses.append(total_loss)\n",
    "\n",
    "print(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x22acdef5f88>]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = cbow.forward(hi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "hi = torch.tensor([1,0,0,0])\n",
    "hi = hi.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3.4385,  4.8261,  8.4366,  ..., -1.6990, -1.4244, -1.3252]],\n",
       "       device='cuda:0', grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\reone\\Anaconda3\\envs\\pytorch37\\lib\\site-packages\\ipykernel_launcher.py:1: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(1., device='cuda:0', grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(out).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1b0f2341e48>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for l_r,losses in loss_dict.items():\n",
    "    plt.plot(losses,label = l_r)\n",
    "    \n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[tensor([319, 262, 191, 456]), tensor([256])],\n",
       " [tensor([470,  32, 256, 191]), tensor([262])],\n",
       " [tensor([ 14, 456, 191, 470]), tensor([319])],\n",
       " [tensor([470,  32, 191, 256]), tensor([1])],\n",
       " [tensor([319, 191, 470,  32]), tensor([14])],\n",
       " [tensor([ 14, 470,   1, 319]), tensor([32])],\n",
       " [tensor([256, 319, 191,  14]), tensor([470])],\n",
       " [tensor([262, 191, 470,   1]), tensor([456])],\n",
       " [tensor([  1, 256, 319, 456]), tensor([191])],\n",
       " [tensor([13, 74, 11, 26]), tensor([9])],\n",
       " [tensor([ 0, 10, 26, 74]), tensor([51])],\n",
       " [tensor([13, 26, 10,  8]), tensor([0])],\n",
       " [tensor([ 13,  51, 592,   0]), tensor([10])],\n",
       " [tensor([ 13,  74,  51, 211]), tensor([11])],\n",
       " [tensor([ 51,   9, 211,   0]), tensor([74])],\n",
       " [tensor([ 9,  0, 51, 11]), tensor([8])],\n",
       " [tensor([11, 26,  9,  8]), tensor([592])],\n",
       " [tensor([ 74, 592,  26,  11]), tensor([211])],\n",
       " [tensor([211,  74,  11, 592]), tensor([26])],\n",
       " [tensor([51,  9, 26, 74]), tensor([13])],\n",
       " [tensor([  4,  63, 785,   0]), tensor([8])],\n",
       " [tensor([  4,  87, 103, 785]), tensor([14])],\n",
       " [tensor([149,  16,  63,   4]), tensor([0])],\n",
       " [tensor([ 16,   4,  48, 785]), tensor([103])],\n",
       " [tensor([  4,  14, 785,  16]), tensor([149])],\n",
       " [tensor([  0,  16, 149,  63]), tensor([87])],\n",
       " [tensor([  0, 149,  48,  14]), tensor([3036])],\n",
       " [tensor([785,  14,  16,  87]), tensor([48])],\n",
       " [tensor([   0,   14, 3036,   87]), tensor([63])],\n",
       " [tensor([ 87,   4, 103,  14]), tensor([16])],\n",
       " [tensor([  87,  785, 3036,    8]), tensor([4])],\n",
       " [tensor([3036,    0,    4,    8]), tensor([785])],\n",
       " [tensor([1298, 1298, 1298, 1298]), tensor([5])],\n",
       " [tensor([   5,    5, 1298,    5]), tensor([13])],\n",
       " [tensor([0, 5, 5, 0]), tensor([1298])],\n",
       " [tensor([13, 13,  5,  5]), tensor([0])],\n",
       " [tensor([ 22,   5,  95, 580]), tensor([24])],\n",
       " [tensor([  28, 1346,    2,   24]), tensor([46])],\n",
       " [tensor([  26, 1409,   28,    2]), tensor([580])],\n",
       " [tensor([1409,  250,  200,   28]), tensor([42])],\n",
       " [tensor([ 28,  24, 967,  85]), tensor([2])],\n",
       " [tensor([  29, 1346,  967,   26]), tensor([250])],\n",
       " [tensor([250,  95, 200, 967]), tensor([26])],\n",
       " [tensor([1409, 1346,   42,    0]), tensor([4])],\n",
       " [tensor([200,   4,  85,   2]), tensor([0])],\n",
       " [tensor([  5, 250,  26,  29]), tensor([18])],\n",
       " [tensor([ 967, 1409,   85,   29]), tensor([5])],\n",
       " [tensor([1409,   18,   24,   42]), tensor([28])],\n",
       " [tensor([ 24,  85, 200, 250]), tensor([1409])],\n",
       " [tensor([46, 22,  0, 95]), tensor([29])],\n",
       " [tensor([ 26,   0, 580,  18]), tensor([22])],\n",
       " [tensor([  26,   42,   24, 1346]), tensor([200])],\n",
       " [tensor([   2,    0, 1346,    5]), tensor([85])],\n",
       " [tensor([22, 24,  0,  4]), tensor([967])],\n",
       " [tensor([  4,   0, 250,  26]), tensor([1346])],\n",
       " [tensor([   5,   42, 1346,   46]), tensor([95])],\n",
       " [tensor([117,  38,  26,   7]), tensor([9])],\n",
       " [tensor([  9,   4, 117,   0]), tensor([7])],\n",
       " [tensor([ 38,   0,  53, 117]), tensor([4])],\n",
       " [tensor([117,   9,  53, 159]), tensor([8])],\n",
       " [tensor([ 7,  0, 38, 26]), tensor([159])],\n",
       " [tensor([53,  9,  8, 38]), tensor([0])],\n",
       " [tensor([9, 7, 8, 7]), tensor([60])],\n",
       " [tensor([  7,   9, 159,  38]), tensor([26])],\n",
       " [tensor([ 26,   8,   7, 117]), tensor([53])],\n",
       " [tensor([53,  7, 38,  9]), tensor([117])],\n",
       " [tensor([159,  60,  53,  26]), tensor([7])],\n",
       " [tensor([ 60,   7, 117,   9]), tensor([38])],\n",
       " [tensor([37,  0, 35, 55]), tensor([3])],\n",
       " [tensor([729,  14,  37,  35]), tensor([0])],\n",
       " [tensor([ 3, 12, 73, 95]), tensor([35])],\n",
       " [tensor([ 73,  12,  37, 137]), tensor([14])],\n",
       " [tensor([ 35,  14, 137,   0]), tensor([1])],\n",
       " [tensor([  1, 729,  73,  55]), tensor([12])],\n",
       " [tensor([ 0, 55, 14, 35]), tensor([37])],\n",
       " [tensor([ 55,  95, 729,   3]), tensor([73])],\n",
       " [tensor([  1, 589,  12,   0]), tensor([729])],\n",
       " [tensor([ 14,  12, 729,  35]), tensor([137])],\n",
       " [tensor([14, 73, 37,  1]), tensor([55])],\n",
       " [tensor([ 73,  37, 137,  55]), tensor([95])],\n",
       " [tensor([37, 12, 55,  0]), tensor([589])],\n",
       " [tensor([  9, 394,  53,   8]), tensor([7])],\n",
       " [tensor([ 53,   8,   7, 907]), tensor([1347])],\n",
       " [tensor([ 3, 53, 38,  9]), tensor([5581])],\n",
       " [tensor([ 394,    7,  907, 1347]), tensor([9])],\n",
       " [tensor([   3, 5581,    9,   38]), tensor([53])],\n",
       " [tensor([   8,   38, 1347,    7]), tensor([3])],\n",
       " [tensor([   3,   53,   38, 1347]), tensor([394])],\n",
       " [tensor([  38, 5581,   53,  907]), tensor([8])],\n",
       " [tensor([5581,    7,    9,    3]), tensor([38])],\n",
       " [tensor([1347, 5581,    3,    7]), tensor([907])],\n",
       " [tensor([ 37,  12, 324, 399]), tensor([3])],\n",
       " [tensor([ 37, 223,   3,   0]), tensor([32])],\n",
       " [tensor([223,  43, 164, 399]), tensor([324])],\n",
       " [tensor([ 37,   0, 399,  43]), tensor([223])],\n",
       " [tensor([11,  0, 32, 12]), tensor([522])],\n",
       " [tensor([ 37, 324, 164, 223]), tensor([79])],\n",
       " [tensor([324,  37, 399, 522]), tensor([164])],\n",
       " [tensor([ 32,  43, 522, 164]), tensor([11])],\n",
       " [tensor([522,   3, 223,  37]), tensor([0])],\n",
       " [tensor([  3,  43, 223,  11]), tensor([399])],\n",
       " [tensor([223,  79, 399,  12]), tensor([43])],\n",
       " [tensor([ 43,  12, 522,   0]), tensor([37])],\n",
       " [tensor([324,   0,  37,  32]), tensor([12])],\n",
       " [tensor([ 1, 17, 23, 73]), tensor([10])],\n",
       " [tensor([23, 73, 10,  1]), tensor([49])],\n",
       " [tensor([23, 17, 10, 49]), tensor([1])],\n",
       " [tensor([10, 17, 73,  1]), tensor([23])],\n",
       " [tensor([10, 23, 73,  1]), tensor([17])],\n",
       " [tensor([49, 23,  1, 17]), tensor([73])],\n",
       " [tensor([362,   3, 811,  59]), tensor([458])],\n",
       " [tensor([429,  14,   3,  39]), tensor([811])],\n",
       " [tensor([ 93, 362, 423, 458]), tensor([39])],\n",
       " [tensor([ 93,   1, 362, 429]), tensor([3])],\n",
       " [tensor([14, 93, 59,  3]), tensor([1])],\n",
       " [tensor([262, 362,  93, 811]), tensor([423])],\n",
       " [tensor([423, 429, 458, 811]), tensor([14])],\n",
       " [tensor([458, 362,   3,  39]), tensor([850])],\n",
       " [tensor([362, 811,   3, 262]), tensor([429])],\n",
       " [tensor([  3, 362, 429,  14]), tensor([59])],\n",
       " [tensor([850, 262,  14, 811]), tensor([93])],\n",
       " [tensor([ 39, 423, 811, 850]), tensor([262])],\n",
       " [tensor([ 39, 262,   3, 458]), tensor([362])],\n",
       " [tensor([ 15,  31, 427,  36]), tensor([171])],\n",
       " [tensor([ 36,  31, 217, 288]), tensor([15])],\n",
       " [tensor([ 65,  36, 419,  15]), tensor([427])],\n",
       " [tensor([  1,  34,  15, 427]), tensor([217])],\n",
       " [tensor([217,  65, 171,   7]), tensor([169])],\n",
       " [tensor([419,  59, 171,  34]), tensor([36])],\n",
       " [tensor([427,  34,  36, 288]), tensor([31])],\n",
       " [tensor([34, 15,  5, 31]), tensor([59])],\n",
       " [tensor([ 31, 419,  36,   5]), tensor([7])],\n",
       " [tensor([419,   5,  34,  31]), tensor([288])],\n",
       " [tensor([171,  15, 427,  65]), tensor([1])],\n",
       " [tensor([169, 171,  34,  36]), tensor([419])],\n",
       " [tensor([  7, 427,  31, 169]), tensor([5])],\n",
       " [tensor([171,   7,   1, 419]), tensor([65])],\n",
       " [tensor([ 65, 419, 217,  15]), tensor([34])],\n",
       " [tensor([  6,  55, 287, 414]), tensor([504])],\n",
       " [tensor([1747,  417,   18,   55]), tensor([287])],\n",
       " [tensor([ 18, 504,   6, 417]), tensor([414])],\n",
       " [tensor([1747,   18,  287,    6]), tensor([23])],\n",
       " [tensor([504, 414, 287,  55]), tensor([18])],\n",
       " [tensor([417, 414, 287,  23]), tensor([1747])],\n",
       " [tensor([ 73, 287,  23, 414]), tensor([6])],\n",
       " [tensor([ 287,  414,   73, 1747]), tensor([417])],\n",
       " [tensor([   6,  504, 1747,   73]), tensor([55])],\n",
       " [tensor([ 287,   55, 1747,  417]), tensor([73])],\n",
       " [tensor([102,   3,  28,  24]), tensor([60])],\n",
       " [tensor([24, 65,  1, 11]), tensor([12])],\n",
       " [tensor([22,  0, 99, 11]), tensor([28])],\n",
       " [tensor([22, 99, 60, 28]), tensor([102])],\n",
       " [tensor([ 22,  99,   1, 102]), tensor([17])],\n",
       " [tensor([ 1,  3, 22, 17]), tensor([11])],\n",
       " [tensor([ 3, 12, 99, 11]), tensor([1])],\n",
       " [tensor([11, 65,  1, 28]), tensor([99])],\n",
       " [tensor([12, 24,  3, 60]), tensor([62])],\n",
       " [tensor([60, 12,  2, 17]), tensor([25])],\n",
       " [tensor([ 1, 65,  0,  2]), tensor([286])],\n",
       " [tensor([ 1, 17,  2, 25]), tensor([0])],\n",
       " [tensor([65, 28, 24, 11]), tensor([22])],\n",
       " [tensor([65, 12,  1,  2]), tensor([24])],\n",
       " [tensor([ 12,  25, 102, 286]), tensor([3])],\n",
       " [tensor([  2,  17,  12, 102]), tensor([65])],\n",
       " [tensor([ 0, 24, 25, 22]), tensor([2])],\n",
       " [tensor([ 11, 138,  44, 482]), tensor([21])],\n",
       " [tensor([  0, 315, 155, 482]), tensor([4])],\n",
       " [tensor([  4, 155, 482,  44]), tensor([9])],\n",
       " [tensor([1053,    9,    0,    4]), tensor([155])],\n",
       " [tensor([  4,  11, 482,   0]), tensor([138])],\n",
       " [tensor([21,  0, 44,  9]), tensor([555])],\n",
       " [tensor([1053,  155,    0,   44]), tensor([11])],\n",
       " [tensor([   0,  482, 1053,   21]), tensor([315])],\n",
       " [tensor([482,   9, 555, 138]), tensor([1053])],\n",
       " [tensor([11,  9,  0, 21]), tensor([482])],\n",
       " [tensor([   0,  482,  155, 1053]), tensor([44])],\n",
       " [tensor([482, 138,   9, 315]), tensor([0])],\n",
       " [tensor([ 10, 561,  85, 118]), tensor([108])],\n",
       " [tensor([163,  10, 118,   2]), tensor([367])],\n",
       " [tensor([118, 561, 367,   2]), tensor([10])],\n",
       " [tensor([ 10, 561, 118,   2]), tensor([85])],\n",
       " [tensor([163, 130, 367, 118]), tensor([1085])],\n",
       " [tensor([1085,  118,  163,   85]), tensor([561])],\n",
       " [tensor([1085,  367,  108,    2]), tensor([118])],\n",
       " [tensor([130,  10, 108,  85]), tensor([2])],\n",
       " [tensor([  2,  10, 367, 108]), tensor([130])],\n",
       " [tensor([ 118, 1085,  108,  130]), tensor([163])],\n",
       " [tensor([ 181, 2884,  404,  181]), tensor([248])],\n",
       " [tensor([248, 404, 404, 248]), tensor([2884])],\n",
       " [tensor([ 248, 2884, 2884, 2884]), tensor([181])],\n",
       " [tensor([248, 248, 181, 181]), tensor([404])],\n",
       " [tensor([ 103,  103, 1788, 2562]), tensor([186])],\n",
       " [tensor([ 103,  103,  186, 1788]), tensor([2562])],\n",
       " [tensor([ 186, 1788, 2562, 2562]), tensor([103])],\n",
       " [tensor([ 186,  103, 2562, 2562]), tensor([1788])],\n",
       " [tensor([  25, 1622, 1622,   25]), tensor([7])],\n",
       " [tensor([ 7, 25,  7, 25]), tensor([1622])],\n",
       " [tensor([   7,   25, 1622, 1622]), tensor([410])],\n",
       " [tensor([ 410,    7, 1622, 1622]), tensor([25])],\n",
       " [tensor([ 0, 34, 25, 13]), tensor([16])],\n",
       " [tensor([180, 101,  16,  34]), tensor([13])],\n",
       " [tensor([ 34, 101,  41,  25]), tensor([30])],\n",
       " [tensor([  0,  13, 180, 151]), tensor([41])],\n",
       " [tensor([  41, 1472,   34,  505]), tensor([526])],\n",
       " [tensor([ 25, 101,  30,   0]), tensor([505])],\n",
       " [tensor([ 13,   0, 505,  41]), tensor([151])],\n",
       " [tensor([ 34, 151,  30,  25]), tensor([1472])],\n",
       " [tensor([ 25, 101,  13, 526]), tensor([0])],\n",
       " [tensor([ 34, 526, 505,   0]), tensor([25])],\n",
       " [tensor([ 505, 1472,  526,   30]), tensor([180])],\n",
       " [tensor([ 34, 180,  25,  13]), tensor([101])],\n",
       " [tensor([505,   0,  41,  25]), tensor([34])],\n",
       " [tensor([142,  88, 158,  12]), tensor([726])],\n",
       " [tensor([173, 547,  12, 726]), tensor([381])],\n",
       " [tensor([274,  88, 381,  35]), tensor([12])],\n",
       " [tensor([379,  12, 381, 158]), tensor([173])],\n",
       " [tensor([274,  88,   5, 158]), tensor([547])],\n",
       " [tensor([381, 379, 726, 158]), tensor([88])],\n",
       " [tensor([ 88,  12, 726, 547]), tensor([379])],\n",
       " [tensor([  5, 158, 726,  88]), tensor([142])],\n",
       " [tensor([726, 158, 379, 142]), tensor([35])],\n",
       " [tensor([ 35,  12, 173, 379]), tensor([5])],\n",
       " [tensor([142,  88,  35, 158]), tensor([274])],\n",
       " [tensor([142, 381,  88, 173]), tensor([158])],\n",
       " [tensor([32, 43, 37, 12]), tensor([322])],\n",
       " [tensor([ 43, 322,  12,  32]), tensor([17])],\n",
       " [tensor([17, 37, 12, 43]), tensor([32])],\n",
       " [tensor([ 12,  32,  17, 322]), tensor([37])],\n",
       " [tensor([ 37,  32,  17, 322]), tensor([43])],\n",
       " [tensor([322,  32,  17,  43]), tensor([12])],\n",
       " [tensor([1456,  350,    0,    8]), tensor([962])],\n",
       " [tensor([   8,    0, 1456,  962]), tensor([14])],\n",
       " [tensor([ 962,    8,   14, 1456]), tensor([0])],\n",
       " [tensor([   8, 1456,  962,    0]), tensor([350])],\n",
       " [tensor([ 350,    0, 1456,   14]), tensor([8])],\n",
       " [tensor([  8, 962,  14, 350]), tensor([1456])],\n",
       " [tensor([ 336,  506, 1823,  452]), tensor([114])],\n",
       " [tensor([  5, 506,  16,  15]), tensor([56])],\n",
       " [tensor([  5,  36, 506, 336]), tensor([928])],\n",
       " [tensor([ 928, 1823,   56,  452]), tensor([5])],\n",
       " [tensor([ 15,  56,  16, 114]), tensor([452])],\n",
       " [tensor([ 114, 1823,   36,   56]), tensor([336])],\n",
       " [tensor([1823,   36,  336,   56]), tensor([16])],\n",
       " [tensor([ 16,  36, 452, 336]), tensor([634])],\n",
       " [tensor([ 36, 452, 928,  56]), tensor([1823])],\n",
       " [tensor([634, 506, 336, 114]), tensor([15])],\n",
       " [tensor([634,   5,  56, 452]), tensor([36])],\n",
       " [tensor([1823,  928,  452,  114]), tensor([506])],\n",
       " [tensor([139, 293,   1,   3]), tensor([312])],\n",
       " [tensor([312, 345,  65, 125]), tensor([73])],\n",
       " [tensor([  1, 345, 312, 139]), tensor([3])],\n",
       " [tensor([139, 345,   1,   3]), tensor([293])],\n",
       " [tensor([345, 312, 125,  65]), tensor([139])],\n",
       " [tensor([  3, 125, 293, 139]), tensor([65])],\n",
       " [tensor([ 73,   1, 139, 293]), tensor([345])],\n",
       " [tensor([  1,  73,  65, 293]), tensor([125])],\n",
       " [tensor([125, 312,  65,  73]), tensor([1])],\n",
       " [tensor([ 177,  242, 2025,  167]), tensor([7])],\n",
       " [tensor([  7, 286, 209,  36]), tensor([1383])],\n",
       " [tensor([242, 286, 167, 158]), tensor([0])],\n",
       " [tensor([ 138,   14,  176, 1383]), tensor([41])],\n",
       " [tensor([ 14, 176,  36,  41]), tensor([209])],\n",
       " [tensor([ 209, 2025,  167,   41]), tensor([286])],\n",
       " [tensor([  36,  177, 1383,  167]), tensor([158])],\n",
       " [tensor([ 158,  138,  177, 1383]), tensor([2025])],\n",
       " [tensor([  41,    0, 2025,   36]), tensor([14])],\n",
       " [tensor([ 41, 167, 242,   0]), tensor([138])],\n",
       " [tensor([ 286, 2025,   14,  138]), tensor([167])],\n",
       " [tensor([ 209,  138,  177, 2025]), tensor([242])],\n",
       " [tensor([ 176, 1383,   41,    0]), tensor([177])],\n",
       " [tensor([209, 176, 138,  14]), tensor([36])],\n",
       " [tensor([1383,    7,  138,  209]), tensor([176])],\n",
       " [tensor([ 54, 266,  18,  29]), tensor([440])],\n",
       " [tensor([220,   2, 440, 118]), tensor([18])],\n",
       " [tensor([220,  18, 266, 118]), tensor([10])],\n",
       " [tensor([ 29,  18,  10, 266]), tensor([54])],\n",
       " [tensor([ 18, 118,  54, 606]), tensor([266])],\n",
       " [tensor([440,   2, 220,  29]), tensor([606])],\n",
       " [tensor([440,  10, 606,  29]), tensor([220])],\n",
       " [tensor([ 29, 220,  54,  10]), tensor([2])],\n",
       " [tensor([440,  18,  54,  29]), tensor([118])],\n",
       " [tensor([ 18,   2, 440,  54]), tensor([29])],\n",
       " [tensor([243,   9,   0,  41]), tensor([11])],\n",
       " [tensor([ 21,  41,   0, 212]), tensor([4])],\n",
       " [tensor([672,  66,   0,  11]), tensor([301])],\n",
       " [tensor([  9, 301, 243,  66]), tensor([672])],\n",
       " [tensor([301,  41, 212,  11]), tensor([0])],\n",
       " [tensor([ 11,   4, 672,   0]), tensor([21])],\n",
       " [tensor([  0, 301, 212,  21]), tensor([66])],\n",
       " [tensor([ 41,   0,   4, 243]), tensor([212])],\n",
       " [tensor([212, 672,  11,  21]), tensor([41])],\n",
       " [tensor([21,  4,  9, 41]), tensor([243])],\n",
       " [tensor([ 66, 212,  21,  41]), tensor([9])],\n",
       " [tensor([552,  16,   2, 875]), tensor([47])],\n",
       " [tensor([  13,   20,    2, 1905]), tensor([875])],\n",
       " [tensor([552, 159,  47,  34]), tensor([1905])],\n",
       " [tensor([  1,  16, 594, 552]), tensor([34])],\n",
       " [tensor([ 34,  29, 552,  13]), tensor([594])],\n",
       " [tensor([ 2, 34, 47,  1]), tensor([13])],\n",
       " [tensor([   2,   34,   13, 1905]), tensor([29])],\n",
       " [tensor([  29, 1905,  875,   20]), tensor([16])],\n",
       " [tensor([  34,   47,   13, 1905]), tensor([552])],\n",
       " [tensor([  20,   34,  157, 1905]), tensor([1])],\n",
       " [tensor([594, 157,  16,   1]), tensor([2])],\n",
       " [tensor([   1,   29,  157, 1905]), tensor([159])],\n",
       " [tensor([ 29,   1, 552, 875]), tensor([157])],\n",
       " [tensor([875,  13, 157,   2]), tensor([20])],\n",
       " [tensor([1255, 1255, 1255,    8]), tensor([91])],\n",
       " [tensor([  91,   91, 1255,    8]), tensor([160])],\n",
       " [tensor([ 91, 160, 160,  91]), tensor([1255])],\n",
       " [tensor([ 91,  91, 160, 160]), tensor([8])],\n",
       " [tensor([205,   4,  55,   4]), tensor([211])],\n",
       " [tensor([211,  55,   4,   4]), tensor([205])],\n",
       " [tensor([205,  21, 211, 211]), tensor([4])],\n",
       " [tensor([ 55, 211, 211,  55]), tensor([21])],\n",
       " [tensor([ 21,  21, 211, 211]), tensor([55])],\n",
       " [tensor([124, 116,  27, 194]), tensor([41])],\n",
       " [tensor([ 41, 194,  30, 134]), tensor([101])],\n",
       " [tensor([ 50,   6,   2, 134]), tensor([116])],\n",
       " [tensor([124, 134, 194,  50]), tensor([30])],\n",
       " [tensor([124,   2,  50,  35]), tensor([76])],\n",
       " [tensor([ 30, 194,  35,   6]), tensor([120])],\n",
       " [tensor([134,  27, 120,  41]), tensor([6])],\n",
       " [tensor([ 30, 124,  41,   6]), tensor([27])],\n",
       " [tensor([ 27, 101, 194,  35]), tensor([283])],\n",
       " [tensor([ 27, 116, 120,   2]), tensor([194])],\n",
       " [tensor([134,  27,   2, 120]), tensor([124])],\n",
       " [tensor([  9, 134, 101, 116]), tensor([50])],\n",
       " [tensor([  2,  76, 283,  30]), tensor([134])],\n",
       " [tensor([ 76, 194,   6, 124]), tensor([9])],\n",
       " [tensor([116,   6,  50, 194]), tensor([35])],\n",
       " [tensor([283, 116,  35,  50]), tensor([2])],\n",
       " [tensor([54, 39, 22, 55]), tensor([28])],\n",
       " [tensor([55, 28, 11, 22]), tensor([65])],\n",
       " [tensor([ 65,  54,  39, 141]), tensor([48])],\n",
       " [tensor([ 11, 141,  54,  65]), tensor([55])],\n",
       " [tensor([ 39, 141,  55,  65]), tensor([22])],\n",
       " [tensor([22, 48, 28, 11]), tensor([141])],\n",
       " [tensor([22, 65, 54, 48]), tensor([39])],\n",
       " [tensor([ 22,  48, 141,  55]), tensor([11])],\n",
       " [tensor([141,  55,  11,  65]), tensor([54])],\n",
       " [tensor([  13,  327, 1291,    6]), tensor([8])],\n",
       " [tensor([ 158,  327, 2962,  148]), tensor([46])],\n",
       " [tensor([   6,  497,  381, 1208]), tensor([327])],\n",
       " [tensor([546,   6,   8, 497]), tensor([158])],\n",
       " [tensor([   6,    8, 1208,  538]), tensor([546])],\n",
       " [tensor([158, 546, 381, 658]), tensor([497])],\n",
       " [tensor([1291,    6,   46,  327]), tensor([13])],\n",
       " [tensor([2962,   58,  148,  538]), tensor([6])],\n",
       " [tensor([ 58,  11,  46, 381]), tensor([2962])],\n",
       " [tensor([2962, 1291,  381, 1208]), tensor([58])],\n",
       " [tensor([ 327, 1208,   13,  546]), tensor([148])],\n",
       " [tensor([1291,  546,   58,  658]), tensor([381])],\n",
       " [tensor([ 13, 658, 538,  58]), tensor([1291])],\n",
       " [tensor([ 497,   46, 1208,  148]), tensor([538])],\n",
       " [tensor([ 546,  658, 1208,    6]), tensor([11])],\n",
       " [tensor([2962,   58,  546,  148]), tensor([658])],\n",
       " [tensor([381, 546,  13,  58]), tensor([1208])],\n",
       " [tensor([ 70, 132,  19,  10]), tensor([3])],\n",
       " [tensor([ 39, 132,  10,  19]), tensor([2])],\n",
       " [tensor([19,  3,  2, 39]), tensor([1])],\n",
       " [tensor([ 10,   2,   1, 132]), tensor([39])],\n",
       " [tensor([70, 10,  3, 39]), tensor([19])],\n",
       " [tensor([ 2, 10,  1,  3]), tensor([70])],\n",
       " [tensor([ 3, 70, 10, 19]), tensor([132])],\n",
       " [tensor([ 70,  19, 132,  39]), tensor([10])],\n",
       " [tensor([ 35, 115, 234,  10]), tensor([3])],\n",
       " [tensor([4990,   35,  115,   27]), tensor([10])],\n",
       " [tensor([115, 234, 263,  27]), tensor([4990])],\n",
       " [tensor([ 115,  263,   35, 4990]), tensor([27])],\n",
       " [tensor([3846,  115,  263, 4990]), tensor([35])],\n",
       " [tensor([263, 115,  10, 234]), tensor([3846])],\n",
       " [tensor([3846,   27,  263,   10]), tensor([115])],\n",
       " [tensor([  3,  10, 115,  35]), tensor([234])],\n",
       " [tensor([  27, 4990,  234,  115]), tensor([263])],\n",
       " [tensor([ 14, 931,   0,  35]), tensor([3])],\n",
       " [tensor([931, 581,  35,   6]), tensor([918])],\n",
       " [tensor([918, 581,   6,   0]), tensor([931])],\n",
       " [tensor([581,  14,   3, 918]), tensor([6])],\n",
       " [tensor([  0,   6, 931, 918]), tensor([14])],\n",
       " [tensor([ 3,  6, 14,  0]), tensor([35])],\n",
       " [tensor([931,  14,   6,  35]), tensor([581])],\n",
       " [tensor([ 14,   3, 931, 918]), tensor([0])],\n",
       " [tensor([  0,   1, 149,  11]), tensor([41])],\n",
       " [tensor([ 41, 222,  11,   1]), tensor([149])],\n",
       " [tensor([  7,  41,   0, 222]), tensor([296])],\n",
       " [tensor([222, 280,   7, 296]), tensor([11])],\n",
       " [tensor([222, 296,   1, 280]), tensor([0])],\n",
       " [tensor([11,  1,  0, 41]), tensor([7])],\n",
       " [tensor([  7,   0,  41, 149]), tensor([1])],\n",
       " [tensor([296, 280,  16, 149]), tensor([222])],\n",
       " [tensor([222, 149, 280,  41]), tensor([16])],\n",
       " [tensor([ 16,   1,  41, 222]), tensor([280])],\n",
       " [tensor([239, 128,  28, 106]), tensor([19])],\n",
       " [tensor([358, 106,  42,   6]), tensor([128])],\n",
       " [tensor([ 19, 239,   6,  42]), tensor([106])],\n",
       " [tensor([ 28, 358, 106, 128]), tensor([42])],\n",
       " [tensor([106, 239,  42, 358]), tensor([28])],\n",
       " [tensor([ 28, 239,  19, 128]), tensor([6])],\n",
       " [tensor([128, 106,  42,   6]), tensor([358])],\n",
       " [tensor([358,   6,  42,  19]), tensor([239])],\n",
       " [tensor([ 72,  42, 538, 174]), tensor([426])],\n",
       " [tensor([  0,  51,  75, 538]), tensor([23])],\n",
       " [tensor([ 42,  51, 538,  72]), tensor([985])],\n",
       " [tensor([ 72,  51,   2, 426]), tensor([75])],\n",
       " [tensor([538,   6,  72, 844]), tensor([1050])],\n",
       " [tensor([844,  72,  42,   6]), tensor([51])],\n",
       " [tensor([  2, 174,  51,  72]), tensor([538])],\n",
       " [tensor([  42,  538, 1050,  174]), tensor([72])],\n",
       " [tensor([ 426, 1050,   23,  985]), tensor([0])],\n",
       " [tensor([72,  2,  6, 51]), tensor([844])],\n",
       " [tensor([ 75,  42, 844,   2]), tensor([174])],\n",
       " [tensor([ 72, 174,   0,  23]), tensor([6])],\n",
       " [tensor([ 51, 985,  72, 844]), tensor([42])],\n",
       " [tensor([426,   0, 174,  75]), tensor([2])],\n",
       " [tensor([2232, 1689,  154,    4]), tensor([4013])],\n",
       " [tensor([ 154, 2232,    4,  880]), tensor([1689])],\n",
       " [tensor([ 154,    4, 2232,  186]), tensor([877])],\n",
       " [tensor([186, 880,   4, 154]), tensor([2232])],\n",
       " [tensor([ 880, 1689, 2232,    4]), tensor([154])],\n",
       " [tensor([2232,  186,  154, 4013]), tensor([880])],\n",
       " [tensor([ 877, 4013,  154, 1689]), tensor([4])],\n",
       " [tensor([   4,  880,  877, 2232]), tensor([186])],\n",
       " [tensor([895,  43, 895, 895]), tensor([498])],\n",
       " [tensor([1669,   43,  895,  895]), tensor([957])],\n",
       " [tensor([895,  43, 895,  43]), tensor([1669])],\n",
       " [tensor([ 957,  498,  957, 1669]), tensor([43])],\n",
       " [tensor([1669,  957,   43, 1669]), tensor([895])],\n",
       " [tensor([684,  40, 106, 868]), tensor([243])],\n",
       " [tensor([151,  79,   6, 243]), tensor([238])],\n",
       " [tensor([868,  40, 243, 393]), tensor([151])],\n",
       " [tensor([243, 684,   6,  40]), tensor([868])],\n",
       " [tensor([393,  40, 243,   6]), tensor([189])],\n",
       " [tensor([189, 393, 868, 151]), tensor([79])],\n",
       " [tensor([ 79,   6, 106,  59]), tensor([684])],\n",
       " [tensor([238,  59,  40, 189]), tensor([393])],\n",
       " [tensor([  6,  79, 189, 151]), tensor([40])],\n",
       " [tensor([393,  59, 684, 151]), tensor([106])],\n",
       " [tensor([189, 151,  79, 868]), tensor([6])],\n",
       " [tensor([  6, 243,  40, 684]), tensor([59])],\n",
       " [tensor([ 12, 112,  82, 241]), tensor([432])],\n",
       " [tensor([432,  12,   0,  13]), tensor([46])],\n",
       " [tensor([  0,  46,  15, 241]), tensor([82])],\n",
       " [tensor([162,  82, 432, 241]), tensor([12])],\n",
       " [tensor([ 15, 241,  13, 162]), tensor([0])],\n",
       " [tensor([432,  46,   0,  82]), tensor([112])],\n",
       " [tensor([ 46,  12, 432, 112]), tensor([13])],\n",
       " [tensor([112, 162,  13,  46]), tensor([241])],\n",
       " [tensor([ 12, 112,  82,  13]), tensor([15])],\n",
       " [tensor([241,  12,  15,  82]), tensor([162])],\n",
       " [tensor([  0,  37,  62, 368]), tensor([10])],\n",
       " [tensor([ 12, 368,  62,   0]), tensor([446])],\n",
       " [tensor([446,  24, 137,  13]), tensor([12])],\n",
       " [tensor([10, 37, 13, 24]), tensor([137])],\n",
       " [tensor([ 24,  33, 368,   0]), tensor([37])],\n",
       " [tensor([ 12,  24, 446, 368]), tensor([0])],\n",
       " [tensor([368,  10,  37,  12]), tensor([24])],\n",
       " [tensor([137, 446,  62,  12]), tensor([13])],\n",
       " [tensor([ 37,  12,   0, 368]), tensor([62])],\n",
       " [tensor([446,  33,  10,  37]), tensor([368])],\n",
       " [tensor([  0,  12,  13, 446]), tensor([33])],\n",
       " [tensor([   5, 1571, 1252, 1693]), tensor([8])],\n",
       " [tensor([  70, 1693,    5,  832]), tensor([1252])],\n",
       " [tensor([1252,   13,   70, 1571]), tensor([5])],\n",
       " [tensor([1252,    5, 1693,   13]), tensor([832])],\n",
       " [tensor([  70, 1571,  832,    5]), tensor([1693])],\n",
       " [tensor([  13,  832, 1693,   70]), tensor([1571])],\n",
       " [tensor([  8, 832,   5,  70]), tensor([13])],\n",
       " [tensor([1693,  832,    5,   13]), tensor([70])],\n",
       " [tensor([   5,  340, 1809,    0]), tensor([7])],\n",
       " [tensor([ 340, 1809,    7, 5606]), tensor([0])],\n",
       " [tensor([1809, 5606,    5,    0]), tensor([340])],\n",
       " [tensor([   7,  340, 5606,    0]), tensor([5])],\n",
       " [tensor([  0,   5, 340,   7]), tensor([5606])],\n",
       " [tensor([  5,   0,   7, 340]), tensor([1809])],\n",
       " [tensor([ 80,   9,  13, 810]), tensor([7])],\n",
       " [tensor([  2, 670,   7, 810]), tensor([9])],\n",
       " [tensor([ 2,  7,  9, 80]), tensor([13])],\n",
       " [tensor([810,   9,   7,   2]), tensor([670])],\n",
       " [tensor([ 7, 13, 80,  9]), tensor([38])],\n",
       " [tensor([ 13,   7, 810,  80]), tensor([2])],\n",
       " [tensor([670,   7,   2,   9]), tensor([810])],\n",
       " [tensor([ 38,   9, 670,   2]), tensor([80])],\n",
       " [tensor([  0, 128, 582,   1]), tensor([4])],\n",
       " [tensor([533,   0, 128,   4]), tensor([1751])],\n",
       " [tensor([ 582,  533, 1751,    0]), tensor([1])],\n",
       " [tensor([  0, 533, 128,   1]), tensor([582])],\n",
       " [tensor([  4, 533,   1,   0]), tensor([128])],\n",
       " [tensor([1751,    0,  128,  582]), tensor([533])],\n",
       " [tensor([ 533,  128,    1, 1751]), tensor([0])],\n",
       " [tensor([11,  0, 70, 18]), tensor([103])],\n",
       " [tensor([ 18,   0,  11, 103]), tensor([70])],\n",
       " [tensor([103,  18,  70, 137]), tensor([11])],\n",
       " [tensor([ 70, 137,  11, 103]), tensor([0])],\n",
       " [tensor([ 11,  70, 103,  18]), tensor([137])],\n",
       " [tensor([137,  11, 103,   0]), tensor([18])],\n",
       " [tensor([518, 171, 214,   7]), tensor([1309])],\n",
       " [tensor([328,   7, 214, 518]), tensor([146])],\n",
       " [tensor([171, 146,   7, 518]), tensor([63])],\n",
       " [tensor([ 63, 231,   5, 214]), tensor([171])],\n",
       " [tensor([   7, 1003,  231,   63]), tensor([5])],\n",
       " [tensor([   7,   19, 1003,  171]), tensor([518])],\n",
       " [tensor([ 19,   5, 214, 146]), tensor([7])],\n",
       " [tensor([  63, 1003, 1309,  214]), tensor([19])],\n",
       " [tensor([328, 231, 171,   5]), tensor([1003])],\n",
       " [tensor([  7,  63,  19, 518]), tensor([214])],\n",
       " [tensor([1003,  146,   19,  171]), tensor([231])],\n",
       " [tensor([1003,   19,  231,    7]), tensor([328])],\n",
       " [tensor([ 11,  41, 628, 116]), tensor([8])],\n",
       " [tensor([116, 628, 115, 134]), tensor([121])],\n",
       " [tensor([900, 299, 115, 134]), tensor([41])],\n",
       " [tensor([628, 900, 121, 116]), tensor([134])],\n",
       " [tensor([ 41, 134, 628, 116]), tensor([900])],\n",
       " [tensor([299, 121,  41, 134]), tensor([628])],\n",
       " [tensor([628, 134, 115,   8]), tensor([116])],\n",
       " [tensor([628, 115,   8, 121]), tensor([11])],\n",
       " [tensor([  8, 121, 134,  41]), tensor([115])],\n",
       " [tensor([ 11, 121, 134, 628]), tensor([299])],\n",
       " [tensor([ 56,   6, 374,  73]), tensor([49])],\n",
       " [tensor([ 64,  49, 293,  73]), tensor([72])],\n",
       " [tensor([  0,  72,  49, 374]), tensor([56])],\n",
       " [tensor([ 0, 72, 24, 49]), tensor([374])],\n",
       " [tensor([10, 24, 72, 56]), tensor([3])],\n",
       " [tensor([374,   6,   3,  64]), tensor([49])],\n",
       " [tensor([ 64, 374,  49,  49]), tensor([6])],\n",
       " [tensor([374,  49, 293,  73]), tensor([24])],\n",
       " [tensor([  6,  24,  10, 374]), tensor([64])],\n",
       " [tensor([73, 64, 49, 56]), tensor([0])],\n",
       " [tensor([ 72,   6, 374, 293]), tensor([10])],\n",
       " [tensor([64,  3,  0, 10]), tensor([33])],\n",
       " [tensor([73, 72, 56, 49]), tensor([293])],\n",
       " [tensor([ 10,  56, 293,  72]), tensor([73])],\n",
       " [tensor([ 52,   4,  86, 211]), tensor([7])],\n",
       " [tensor([ 86,   0, 211,  80]), tensor([8])],\n",
       " [tensor([211,   4,  80,   0]), tensor([9])],\n",
       " [tensor([  8,  80, 518,  52]), tensor([86])],\n",
       " [tensor([ 86, 211,   7, 518]), tensor([80])],\n",
       " [tensor([  8,   0, 211,   9]), tensor([52])],\n",
       " [tensor([  8, 211,  80,   0]), tensor([518])],\n",
       " [tensor([ 4, 52,  7,  8]), tensor([211])],\n",
       " [tensor([  7,  52, 211, 518]), tensor([4])],\n",
       " [tensor([  4, 518,  52,   7]), tensor([0])],\n",
       " [tensor([432, 517, 517, 193]), tensor([57])],\n",
       " [tensor([193, 432, 193,  57]), tensor([517])],\n",
       " [tensor([517, 517,  57,  57]), tensor([432])],\n",
       " [tensor([ 57,  57, 432, 517]), tensor([193])],\n",
       " [tensor([ 42,  98, 708,   8]), tensor([235])],\n",
       " [tensor([235, 508, 144,   8]), tensor([4])],\n",
       " [tensor([ 98,  23, 137,  74]), tensor([141])],\n",
       " [tensor([ 72,   9, 144,  17]), tensor([708])],\n",
       " [tensor([ 23, 141,  98, 235]), tensor([1460])],\n",
       " [tensor([ 8,  4, 14, 98]), tensor([3533])],\n",
       " [tensor([ 141,  115, 1460,    4]), tensor([112])],\n",
       " [tensor([137, 508, 112,   9]), tensor([72])],\n",
       " [tensor([  4, 144,  74,  14]), tensor([1])],\n",
       " [tensor([ 72, 708, 508,   4]), tensor([42])],\n",
       " [tensor([  23, 1460,   63,   72]), tensor([144])],\n",
       " [tensor([1460,    9,  235,  708]), tensor([14])],\n",
       " [tensor([  1,  17, 708,   9]), tensor([8])],\n",
       " [tensor([  98,  115,  144, 1460]), tensor([23])],\n",
       " [tensor([ 9, 17, 74,  1]), tensor([63])],\n",
       " [tensor([ 42,  14,   9, 115]), tensor([508])],\n",
       " [tensor([  63,   23, 1460,  508]), tensor([137])],\n",
       " [tensor([ 63,   8,   9, 115]), tensor([17])],\n",
       " [tensor([  8,  63,  98, 235]), tensor([74])],\n",
       " [tensor([137,   4,  17,   9]), tensor([115])],\n",
       " [tensor([137,  98, 508,  74]), tensor([9])],\n",
       " [tensor([ 23, 144, 508,  74]), tensor([98])],\n",
       " [tensor([2063,   70,   38,   26]), tensor([264])],\n",
       " [tensor([ 0, 38,  9, 26]), tensor([2063])],\n",
       " [tensor([264,  26,  38,   9]), tensor([70])],\n",
       " [tensor([   9,    8,    0, 2063]), tensor([26])],\n",
       " [tensor([  70,   38,    0, 2063]), tensor([9])],\n",
       " [tensor([ 264,   70,    0, 2063]), tensor([8])],\n",
       " [tensor([  26,    9, 2063,   70]), tensor([0])],\n",
       " [tensor([  0,  26,  70, 264]), tensor([38])],\n",
       " [tensor([2063,    0,    9,   38]), tensor([287])],\n",
       " [tensor([ 769,  291, 1075,   76]), tensor([0])],\n",
       " [tensor([  0,  76,  13, 291]), tensor([1075])],\n",
       " [tensor([  13, 1075,  291,    0]), tensor([769])],\n",
       " [tensor([ 291,   13,    0, 1075]), tensor([76])],\n",
       " [tensor([  0,  13,  76, 769]), tensor([291])],\n",
       " [tensor([  76,    0, 1075,  769]), tensor([13])],\n",
       " [tensor([ 99, 325,  99,  99]), tensor([984])],\n",
       " [tensor([325, 984, 325, 325]), tensor([99])],\n",
       " [tensor([984,  99, 325, 984]), tensor([517])],\n",
       " [tensor([517, 517, 984,  99]), tensor([325])],\n",
       " [tensor([ 24, 370,   0,  40]), tensor([44])],\n",
       " [tensor([ 24, 335,  44, 370]), tensor([0])],\n",
       " [tensor([  0, 168, 335,  44]), tensor([24])],\n",
       " [tensor([  0, 370,  24,   3]), tensor([335])],\n",
       " [tensor([  3,   0, 335,  24]), tensor([40])],\n",
       " [tensor([370,  24,   3,   0]), tensor([168])],\n",
       " [tensor([40, 44, 24,  0]), tensor([3])],\n",
       " [tensor([ 24,  40, 168,   0]), tensor([370])],\n",
       " [tensor([  1,  58, 222, 149]), tensor([7])],\n",
       " [tensor([149,  58,  24,   7]), tensor([222])],\n",
       " [tensor([  7,  24, 222,  58]), tensor([149])],\n",
       " [tensor([ 24,   7, 222, 149]), tensor([58])],\n",
       " [tensor([ 58,   7,  24, 149]), tensor([1])],\n",
       " [tensor([149,   7,   1, 222]), tensor([24])],\n",
       " [tensor([   6,  255,   36, 2001]), tensor([198])],\n",
       " [tensor([618, 142,  15, 366]), tensor([6])],\n",
       " [tensor([ 198,  366,    6, 2001]), tensor([142])],\n",
       " [tensor([205,   6, 142, 656]), tensor([366])],\n",
       " [tensor([   6, 2001, 1439,  255]), tensor([15])],\n",
       " [tensor([ 656, 2001,  366,    6]), tensor([1439])],\n",
       " [tensor([1439,    6,   15,   36]), tensor([2001])],\n",
       " [tensor([ 198, 2001,  205,  656]), tensor([36])],\n",
       " [tensor([1439,   36, 2001,  205]), tensor([618])],\n",
       " [tensor([ 366,  618, 1439, 2001]), tensor([656])],\n",
       " [tensor([618, 656,   6, 366]), tensor([255])],\n",
       " [tensor([  6, 255, 656, 142]), tensor([205])],\n",
       " [tensor([220,  10, 159,   2]), tensor([531])],\n",
       " [tensor([ 85,  10, 159,   1]), tensor([220])],\n",
       " [tensor([159,   2,  82,  10]), tensor([228])],\n",
       " [tensor([ 12, 159,  81,  85]), tensor([67])],\n",
       " [tensor([220,   5,  67, 228]), tensor([159])],\n",
       " [tensor([ 2, 12, 85, 81]), tensor([1])],\n",
       " [tensor([ 82, 531,  10,   2]), tensor([12])],\n",
       " [tensor([228, 531,  67,   2]), tensor([5])],\n",
       " [tensor([159,  67,  12,  85]), tensor([13])],\n",
       " [tensor([13,  1, 67,  5]), tensor([2])],\n",
       " [tensor([81,  5,  1, 13]), tensor([10])],\n",
       " [tensor([ 82,  13, 220, 531]), tensor([85])],\n",
       " [tensor([ 12,   1, 159,   5]), tensor([82])],\n",
       " [tensor([ 12,  67, 228,  10]), tensor([81])],\n",
       " [tensor([ 50, 144,  29, 521]), tensor([606])],\n",
       " [tensor([235,  29,   4, 144]), tensor([112])],\n",
       " [tensor([ 54,  10, 112,  29]), tensor([179])],\n",
       " [tensor([606,   0,   4, 235]), tensor([245])],\n",
       " [tensor([ 10, 245, 108,   5]), tensor([2])],\n",
       " [tensor([521,   0, 108,   4]), tensor([9])],\n",
       " [tensor([606,   0,  54,  10]), tensor([235])],\n",
       " [tensor([ 10,   0,  92, 112]), tensor([4])],\n",
       " [tensor([ 10, 521,   0, 108]), tensor([1])],\n",
       " [tensor([  5, 606,  29,   1]), tensor([50])],\n",
       " [tensor([521,   4,  15,   5]), tensor([144])],\n",
       " [tensor([  1,   2, 606,  54]), tensor([108])],\n",
       " [tensor([108, 521,   9, 144]), tensor([92])],\n",
       " [tensor([  5,  54, 112,   1]), tensor([15])],\n",
       " [tensor([144,   2,  92,   4]), tensor([29])],\n",
       " [tensor([  0,  54,   5, 112]), tensor([10])],\n",
       " [tensor([235, 108, 112,  15]), tensor([5])],\n",
       " [tensor([179, 521,  10,   0]), tensor([54])],\n",
       " [tensor([179, 245,  10, 521]), tensor([0])],\n",
       " [tensor([ 29,   2, 108,   5]), tensor([521])],\n",
       " [tensor([   3,   17, 2145,  793]), tensor([10])],\n",
       " [tensor([   3,   11, 2145,  793]), tensor([1])],\n",
       " [tensor([793,  10,  11,  17]), tensor([140])],\n",
       " [tensor([  1,  17,   2, 793]), tensor([11])],\n",
       " [tensor([  11,    1,  793, 2145]), tensor([2])],\n",
       " [tensor([   2,  793,   11, 2145]), tensor([3])],\n",
       " [tensor([2145,  140,    1,    2]), tensor([793])],\n",
       " [tensor([2145,  793,    2,    1]), tensor([17])],\n",
       " [tensor([  3, 140,  17,   2]), tensor([2145])],\n",
       " [tensor([ 49,  13,  97, 145]), tensor([452])],\n",
       " [tensor([ 30,  13, 506,  49]), tensor([97])],\n",
       " [tensor([145,  30, 452,   2]), tensor([49])],\n",
       " [tensor([ 97, 452,  49,  13]), tensor([506])],\n",
       " [tensor([  2, 506,  97,  13]), tensor([145])],\n",
       " [tensor([145,   2,  49,  13]), tensor([30])],\n",
       " [tensor([ 30,   2, 452,  49]), tensor([13])],\n",
       " [tensor([ 49,  97, 506, 145]), tensor([2])],\n",
       " [tensor([  36,  231,   31, 4763]), tensor([436])],\n",
       " [tensor([ 231,  114,   36, 4763]), tensor([1])],\n",
       " [tensor([ 462,    1,   61, 1349]), tensor([31])],\n",
       " [tensor([  16,   36,   31, 4763]), tensor([7])],\n",
       " [tensor([ 158, 4763,   36,   20]), tensor([231])],\n",
       " [tensor([  16, 4763,  436,    7]), tensor([462])],\n",
       " [tensor([1349,   20,    1,  462]), tensor([158])],\n",
       " [tensor([158,   7,  20,  31]), tensor([1349])],\n",
       " [tensor([ 462,  231, 4763,   36]), tensor([114])],\n",
       " [tensor([1349,   31,    7,   20]), tensor([61])],\n",
       " [tensor([ 158,    7, 4763,    1]), tensor([20])],\n",
       " [tensor([114, 436, 462,  61]), tensor([16])],\n",
       " [tensor([231,  61, 114, 158]), tensor([36])],\n",
       " [tensor([  20, 1349,  462,   16]), tensor([4763])],\n",
       " [tensor([34,  7,  7,  7]), tensor([233])],\n",
       " [tensor([69,  7, 34,  7]), tensor([26])],\n",
       " [tensor([233,   7,  26,  26]), tensor([69])],\n",
       " [tensor([69, 34, 69, 34]), tensor([7])],\n",
       " [tensor([26, 69,  7,  7]), tensor([34])],\n",
       " [tensor([1916, 1303, 2973, 1602]), tensor([1740])],\n",
       " [tensor([ 126, 1740,  128,   60]), tensor([1602])],\n",
       " [tensor([2973, 1303,  128, 1602]), tensor([1916])],\n",
       " [tensor([2973,  128,   60, 1602]), tensor([1303])],\n",
       " [tensor([1303,  126, 1602,  128]), tensor([562])],\n",
       " [tensor([1740,  126,  128, 1916]), tensor([60])],\n",
       " [tensor([ 128, 1303, 1602,   60]), tensor([2973])],\n",
       " [tensor([2973,  128, 1916, 1303]), tensor([126])],\n",
       " [tensor([ 562, 1916,  126, 2973]), tensor([128])],\n",
       " [tensor([ 70, 137,   0, 120]), tensor([24])],\n",
       " [tensor([137, 217, 120,   0]), tensor([70])],\n",
       " [tensor([ 24,  45,  70, 120]), tensor([137])],\n",
       " [tensor([ 45,  70, 217, 120]), tensor([0])],\n",
       " [tensor([ 24,  45, 217,  70]), tensor([120])],\n",
       " [tensor([ 24,   0, 120,  70]), tensor([45])],\n",
       " [tensor([ 24, 120,   0,  45]), tensor([217])],\n",
       " [tensor([2105,  826, 1295,    4]), tensor([8])],\n",
       " [tensor([   8,  826,  143, 5401]), tensor([1295])],\n",
       " [tensor([ 143, 5401,    8, 2105]), tensor([826])],\n",
       " [tensor([ 826,    8, 2105, 5401]), tensor([4])],\n",
       " [tensor([   8, 1295,    4, 2105]), tensor([143])],\n",
       " [tensor([1295, 5401,    4,    8]), tensor([2105])],\n",
       " [tensor([   8, 1295,  143,  826]), tensor([5401])],\n",
       " [tensor([240, 130, 182,  18]), tensor([2720])],\n",
       " [tensor([ 18, 240,   3,  31]), tensor([12])],\n",
       " [tensor([16, 12,  0,  2]), tensor([28])],\n",
       " [tensor([ 22, 130,   2,  28]), tensor([6])],\n",
       " [tensor([ 18, 797,  16,  28]), tensor([487])],\n",
       " [tensor([   2,   20,   12, 2720]), tensor([924])],\n",
       " [tensor([601, 924,   0,  16]), tensor([14])],\n",
       " [tensor([  20,    3,   28, 3472]), tensor([240])],\n",
       " [tensor([ 12, 130,  28, 797]), tensor([182])],\n",
       " [tensor([  0, 797,  16, 240]), tensor([20])],\n",
       " [tensor([112,   6, 130, 182]), tensor([3472])],\n",
       " [tensor([ 112,  797, 2720,  182]), tensor([16])],\n",
       " [tensor([2720,    6,   20,   31]), tensor([3])],\n",
       " [tensor([ 3,  2,  6, 18]), tensor([0])],\n",
       " [tensor([3472,    3,   16,   22]), tensor([18])],\n",
       " [tensor([ 18,  16,  20, 240]), tensor([601])],\n",
       " [tensor([487,   2,  18, 112]), tensor([130])],\n",
       " [tensor([ 601,  182, 3472,  924]), tensor([112])],\n",
       " [tensor([ 12, 112, 130,  14]), tensor([797])],\n",
       " [tensor([  28,   16,  797, 2720]), tensor([31])],\n",
       " [tensor([ 18,  31, 924,  20]), tensor([2])],\n",
       " [tensor([112, 182,  28,   3]), tensor([22])],\n",
       " [tensor([117, 290,  60, 102]), tensor([196])],\n",
       " [tensor([102, 196, 117,  22]), tensor([1856])],\n",
       " [tensor([  11, 1856,  102,   22]), tensor([117])],\n",
       " [tensor([  22,  290, 1856,  196]), tensor([102])],\n",
       " [tensor([290, 196, 102,  22]), tensor([60])],\n",
       " [tensor([117,  11,  60,  22]), tensor([290])],\n",
       " [tensor([ 60, 117, 102, 196]), tensor([11])],\n",
       " [tensor([ 290,   11,  196, 1856]), tensor([22])],\n",
       " [tensor([  2,  49,   1, 135]), tensor([10])],\n",
       " [tensor([  2, 135, 523,  10]), tensor([56])],\n",
       " [tensor([  2, 135,  10,  49]), tensor([3])],\n",
       " [tensor([  2,  10,   3, 135]), tensor([523])],\n",
       " [tensor([56, 10,  2,  3]), tensor([135])],\n",
       " [tensor([  3,   1,  10, 523]), tensor([2])],\n",
       " [tensor([135,   2, 523,   1]), tensor([49])],\n",
       " [tensor([  2, 135,  56,   3]), tensor([1])],\n",
       " [tensor([297,   1,  65, 156]), tensor([565])],\n",
       " [tensor([65, 21, 17,  3]), tensor([11])],\n",
       " [tensor([297,   3,  21, 565]), tensor([1])],\n",
       " [tensor([392,  21,  17,  65]), tensor([52])],\n",
       " [tensor([ 65,   3,   1, 156]), tensor([21])],\n",
       " [tensor([297, 392,   3,  11]), tensor([156])],\n",
       " [tensor([565,   1,  52, 297]), tensor([17])],\n",
       " [tensor([  1, 565,  52,  21]), tensor([297])],\n",
       " [tensor([ 65, 565, 156,  11]), tensor([392])],\n",
       " [tensor([565,  65,  17,  21]), tensor([3])],\n",
       " [tensor([392,  21, 156,   1]), tensor([65])],\n",
       " [tensor([182,   9,  75,  57]), tensor([108])],\n",
       " [tensor([ 75, 182, 127, 347]), tensor([3])],\n",
       " [tensor([ 75, 127,  14,   9]), tensor([57])],\n",
       " [tensor([182, 127, 108,  46]), tensor([347])],\n",
       " [tensor([347,  27,  12,  15]), tensor([67])],\n",
       " [tensor([ 67,  12,   9, 182]), tensor([14])],\n",
       " [tensor([ 12, 182,  27, 127]), tensor([9])],\n",
       " [tensor([ 15,  27,  67, 347]), tensor([182])],\n",
       " [tensor([127,  46,   3,  10]), tensor([75])],\n",
       " [tensor([ 75,   9,   0, 108]), tensor([127])],\n",
       " [tensor([ 12,  67, 182, 347]), tensor([15])],\n",
       " [tensor([ 75,  46, 347,  15]), tensor([0])],\n",
       " [tensor([27, 12, 14, 46]), tensor([10])],\n",
       " [tensor([  0,  75,  46, 347]), tensor([12])],\n",
       " [tensor([67, 15, 12, 57]), tensor([46])],\n",
       " [tensor([ 12,  10, 182,   0]), tensor([27])],\n",
       " [tensor([804,  81, 356, 359]), tensor([51])],\n",
       " [tensor([356, 707,  12,  81]), tensor([54])],\n",
       " [tensor([ 54, 623, 707, 359]), tensor([290])],\n",
       " [tensor([  2, 290, 707,  54]), tensor([12])],\n",
       " [tensor([359,  51, 356,  10]), tensor([804])],\n",
       " [tensor([804,  10,   1, 150]), tensor([1015])],\n",
       " [tensor([ 51,  54, 356, 150]), tensor([0])],\n",
       " [tensor([  1,  12,  67, 290]), tensor([356])],\n",
       " [tensor([ 12, 290,  51, 623]), tensor([67])],\n",
       " [tensor([  2,  67, 623, 356]), tensor([150])],\n",
       " [tensor([356,   0,  10, 804]), tensor([1])],\n",
       " [tensor([ 150,  804,   10, 1015]), tensor([707])],\n",
       " [tensor([  0, 150,  10,  67]), tensor([81])],\n",
       " [tensor([1015,    2,    1,   10]), tensor([359])],\n",
       " [tensor([  2,   1,  12, 290]), tensor([10])],\n",
       " [tensor([359,   0,  10, 356]), tensor([623])],\n",
       " [tensor([804,   1, 290,   0]), tensor([2])],\n",
       " [tensor([ 409, 1778, 1778, 1778]), tensor([275])],\n",
       " [tensor([1778, 1778,  275, 1778]), tensor([409])],\n",
       " [tensor([409, 275, 275, 275]), tensor([1778])],\n",
       " [tensor([273, 231,  13,  16]), tensor([1349])],\n",
       " [tensor([  19,  231, 1349,  273]), tensor([13])],\n",
       " [tensor([ 273,   16, 1349,   13]), tensor([231])],\n",
       " [tensor([  19,   13, 1349,  273]), tensor([16])],\n",
       " [tensor([  19, 1349,   16,   13]), tensor([273])],\n",
       " [tensor([1349,  273,   16,  231]), tensor([19])],\n",
       " [tensor([87, 12, 99, 33]), tensor([62])],\n",
       " [tensor([87, 62, 37, 99]), tensor([12])],\n",
       " [tensor([62, 12, 33, 87]), tensor([37])],\n",
       " [tensor([12, 99, 62, 87]), tensor([33])],\n",
       " [tensor([12, 87, 37, 62]), tensor([99])],\n",
       " [tensor([62, 33, 99, 37]), tensor([87])],\n",
       " [tensor([   0,  743, 1116,  283]), tensor([172])],\n",
       " [tensor([1399,  263,   18,  571]), tensor([656])],\n",
       " [tensor([   6,  263, 1399,  743]), tensor([1116])],\n",
       " [tensor([   2,    3,    0, 1399]), tensor([6])],\n",
       " [tensor([571, 172,  11, 871]), tensor([743])],\n",
       " [tensor([  2, 283, 656, 172]), tensor([871])],\n",
       " [tensor([   0,    2, 1116,  871]), tensor([234])],\n",
       " [tensor([  0,   3,   2, 172]), tensor([263])],\n",
       " [tensor([   6, 1116,  234,    3]), tensor([2])],\n",
       " [tensor([ 263,  234, 1116,  571]), tensor([103])],\n",
       " [tensor([743, 172,   0, 283]), tensor([11])],\n",
       " [tensor([172,   6, 571, 871]), tensor([0])],\n",
       " [tensor([  2,  18, 871,   3]), tensor([1399])],\n",
       " [tensor([ 172, 1116,  656,   11]), tensor([283])],\n",
       " [tensor([  0,  18, 103, 871]), tensor([3])],\n",
       " [tensor([656, 172, 263,  18]), tensor([571])],\n",
       " [tensor([172, 234,   3,   2]), tensor([18])],\n",
       " [tensor([233,   8,  26, 363]), tensor([7])],\n",
       " [tensor([  8, 363, 959, 181]), tensor([233])],\n",
       " [tensor([1138,    7,    4,   26]), tensor([8])],\n",
       " [tensor([ 181,    4, 1138,    8]), tensor([363])],\n",
       " [tensor([959,   7, 363, 233]), tensor([181])],\n",
       " [tensor([  7, 363,   8, 233]), tensor([26])],\n",
       " [tensor([ 26, 363,   4, 181]), tensor([959])],\n",
       " [tensor([  7,   4, 181, 959]), tensor([1138])],\n",
       " [tensor([959, 363,   8,  26]), tensor([4])],\n",
       " [tensor([ 571, 3444,  121, 3431]), tensor([1236])],\n",
       " [tensor([ 571, 1236,  500, 3431]), tensor([2118])],\n",
       " [tensor([ 422,  500,  121, 1236]), tensor([3444])],\n",
       " [tensor([1236, 3431,  500,    0]), tensor([422])],\n",
       " [tensor([1236, 3107, 2118,  571]), tensor([121])],\n",
       " [tensor([ 422, 3431,  571, 1236]), tensor([500])],\n",
       " [tensor([ 500,    0, 3444, 1236]), tensor([3107])],\n",
       " [tensor([2118, 1236, 3444,  121]), tensor([0])],\n",
       " [tensor([ 422, 2118,  500,  571]), tensor([3431])],\n",
       " [tensor([ 422, 1236,    0, 2118]), tensor([571])],\n",
       " [tensor([1527,  108,   87,   31]), tensor([837])],\n",
       " [tensor([ 110, 1945,  554,  640]), tensor([3])],\n",
       " [tensor([ 87,   7, 640, 159]), tensor([1])],\n",
       " [tensor([1945,  159,  110,   12]), tensor([554])],\n",
       " [tensor([  87,    0,  159, 1945]), tensor([22])],\n",
       " [tensor([   0,    3,    7, 1527]), tensor([640])],\n",
       " [tensor([ 554, 1945,    7,  108]), tensor([148])],\n",
       " [tensor([ 110,    1, 1945,  108]), tensor([31])],\n",
       " [tensor([ 22, 108,   1,  87]), tensor([110])],\n",
       " [tensor([ 31, 110,   3,   0]), tensor([7])],\n",
       " [tensor([  0, 554, 837, 110]), tensor([1527])],\n",
       " [tensor([110,   0, 837, 554]), tensor([87])],\n",
       " [tensor([ 31, 640,  22,   3]), tensor([12])],\n",
       " [tensor([554,   7,  12, 148]), tensor([108])],\n",
       " [tensor([  31, 1527,    0,   87]), tensor([159])],\n",
       " [tensor([  1, 110,  12,  22]), tensor([0])],\n",
       " [tensor([148, 640, 108,  31]), tensor([1945])],\n",
       " [tensor([183, 453,  82, 453]), tensor([161])],\n",
       " [tensor([ 82, 183, 161, 161]), tensor([119])],\n",
       " [tensor([183, 161, 119, 119]), tensor([453])],\n",
       " [tensor([161, 453, 183, 183]), tensor([82])],\n",
       " [tensor([453, 453, 453,  82]), tensor([183])],\n",
       " [tensor([1848,    3,  218,  319]), tensor([130])],\n",
       " [tensor([218, 319,   1,  93]), tensor([657])],\n",
       " [tensor([657,   1,  93, 130]), tensor([1848])],\n",
       " [tensor([218, 319,   1, 657]), tensor([3])],\n",
       " [tensor([ 93,   3, 319, 657]), tensor([218])],\n",
       " [tensor([218,  93,   3, 657]), tensor([1])],\n",
       " [tensor([  1,   3,  93, 218]), tensor([319])],\n",
       " [tensor([   1,  657,  218, 1848]), tensor([93])],\n",
       " [tensor([  7,   8, 671,  96]), tensor([1230])],\n",
       " [tensor([4789,  405,  865,  460]), tensor([96])],\n",
       " [tensor([4789,    8,   96, 1230]), tensor([865])],\n",
       " [tensor([4789,    8,    7,  405]), tensor([2282])],\n",
       " [tensor([   7,  671, 2282, 4789]), tensor([8])],\n",
       " [tensor([   7,    8,  405, 4789]), tensor([460])],\n",
       " [tensor([   8, 2282,  865,  460]), tensor([671])],\n",
       " [tensor([ 405,  865,   96, 1230]), tensor([7])],\n",
       " [tensor([4789,  460,   96,    7]), tensor([405])],\n",
       " [tensor([865,  96, 460, 671]), tensor([4789])],\n",
       " [tensor([1085,    4,   10,    5]), tensor([593])],\n",
       " [tensor([ 10,   1,   5, 593]), tensor([1085])],\n",
       " [tensor([593,   1,  35,  10]), tensor([4])],\n",
       " [tensor([  10, 1085,  593,    5]), tensor([1])],\n",
       " [tensor([   0,    4, 1085,    1]), tensor([5])],\n",
       " [tensor([   4,  593, 1085,   10]), tensor([35])],\n",
       " [tensor([   0, 1085,    4,  593]), tensor([10])],\n",
       " [tensor([ 10, 593,  35,   4]), tensor([0])],\n",
       " [tensor([120,   2,   3,  76]), tensor([629])],\n",
       " [tensor([470, 120, 629,   8]), tensor([30])],\n",
       " [tensor([120,  30, 629,  76]), tensor([470])],\n",
       " [tensor([ 8,  3,  2, 30]), tensor([76])],\n",
       " [tensor([ 30, 470,   2,  76]), tensor([3])],\n",
       " [tensor([  8, 120,  30, 629]), tensor([2])],\n",
       " [tensor([  2,   3, 120, 629]), tensor([8])],\n",
       " [tensor([ 2, 30,  3,  8]), tensor([120])],\n",
       " [tensor([ 152,  152, 3038,    9]), tensor([0])],\n",
       " [tensor([  9, 152, 152,   0]), tensor([3038])],\n",
       " [tensor([0, 0, 0, 9]), tensor([152])],\n",
       " [tensor([3038,    0,    0,    0]), tensor([9])],\n",
       " [tensor([ 0, 42, 80, 26]), tensor([11])],\n",
       " [tensor([11, 80, 26, 13]), tensor([9])],\n",
       " [tensor([   9, 3037,   42,  778]), tensor([26])],\n",
       " [tensor([  0, 778,  26,  42]), tensor([80])],\n",
       " [tensor([13,  0, 26, 42]), tensor([3037])],\n",
       " [tensor([11, 42,  9, 13]), tensor([0])],\n",
       " [tensor([ 9, 42, 13, 11]), tensor([778])],\n",
       " [tensor([ 0, 42, 26,  9]), tensor([13])],\n",
       " [tensor([3037,   26,   13,   11]), tensor([42])],\n",
       " [tensor([491,  26,   0,  38]), tensor([96])],\n",
       " [tensor([491,   0,  96,  26]), tensor([9])],\n",
       " [tensor([38, 26, 96,  0]), tensor([491])],\n",
       " [tensor([ 96,   0,   9, 491]), tensor([38])],\n",
       " [tensor([ 96,   0,  38, 491]), tensor([26])],\n",
       " [tensor([26, 96,  9, 38]), tensor([0])],\n",
       " [tensor([ 225,   27, 3891,    2]), tensor([157])],\n",
       " [tensor([ 225, 3891,    0,    2]), tensor([27])],\n",
       " [tensor([ 225,   27,  157, 3891]), tensor([2])],\n",
       " [tensor([ 225,    2,  157, 3891]), tensor([0])],\n",
       " [tensor([3891,    2,   27,    0]), tensor([225])],\n",
       " [tensor([ 27,   2, 157, 225]), tensor([3891])],\n",
       " [tensor([ 656, 1633,  333,  104]), tensor([60])],\n",
       " [tensor([ 656,  333,   34, 1633]), tensor([104])],\n",
       " [tensor([104, 790,  60,  34]), tensor([656])],\n",
       " [tensor([ 104, 1633,   60,  656]), tensor([333])],\n",
       " [tensor([333, 104, 790, 656]), tensor([1633])],\n",
       " [tensor([790, 333, 104, 656]), tensor([34])],\n",
       " [tensor([ 34, 656,  60, 333]), tensor([790])],\n",
       " [tensor([229,  12, 100,   1]), tensor([200])],\n",
       " [tensor([  25, 1034,  200,   59]), tensor([43])],\n",
       " [tensor([95,  1, 62, 12]), tensor([1034])],\n",
       " [tensor([  25,   59,  251, 1034]), tensor([95])],\n",
       " [tensor([  12,  111,  229, 1390]), tensor([25])],\n",
       " [tensor([   0,   25,  699, 1390]), tensor([251])],\n",
       " [tensor([1390,   12,  100,  229]), tensor([0])],\n",
       " [tensor([  95,    1,   25, 1390]), tensor([100])],\n",
       " [tensor([200,  59, 251,  25]), tensor([111])],\n",
       " [tensor([12,  1, 43, 62]), tensor([699])],\n",
       " [tensor([95, 12, 62, 59]), tensor([1])],\n",
       " [tensor([100, 111,  62, 200]), tensor([12])],\n",
       " [tensor([251,  12,  43, 100]), tensor([62])],\n",
       " [tensor([251,  62,  95,   0]), tensor([12])],\n",
       " [tensor([ 95,  25,  62, 100]), tensor([229])],\n",
       " [tensor([43, 59, 62, 95]), tensor([1390])],\n",
       " [tensor([ 699,  100, 1034,   95]), tensor([59])],\n",
       " [tensor([ 48,   0,  11, 124]), tensor([569])],\n",
       " [tensor([211,   8,   9, 569]), tensor([11])],\n",
       " [tensor([ 11, 569,  48,   8]), tensor([0])],\n",
       " [tensor([ 11, 569,   8,  80]), tensor([92])],\n",
       " [tensor([ 92,  80,  48, 569]), tensor([134])],\n",
       " [tensor([11,  9, 48,  0]), tensor([124])],\n",
       " [tensor([  0, 134,  80,   8]), tensor([211])],\n",
       " [tensor([569, 134,   9,  80]), tensor([8])],\n",
       " [tensor([124, 569,  48,   8]), tensor([9])],\n",
       " [tensor([ 80,  92, 124,   0]), tensor([48])],\n",
       " [tensor([ 11, 211,  48, 569]), tensor([80])],\n",
       " [tensor([85, 97, 55,  5]), tensor([3])],\n",
       " [tensor([ 87,  31,   2, 300]), tensor([55])],\n",
       " [tensor([102,  83, 230,   5]), tensor([97])],\n",
       " [tensor([ 12,  87,  15, 230]), tensor([5])],\n",
       " [tensor([ 12, 102,  83, 230]), tensor([85])],\n",
       " [tensor([85, 97, 55, 76]), tensor([102])],\n",
       " [tensor([ 31, 300,   3,   2]), tensor([230])],\n",
       " [tensor([230,  83,  15,   5]), tensor([300])],\n",
       " [tensor([230,  76,   5,  85]), tensor([15])],\n",
       " [tensor([300,  83,  76,  85]), tensor([87])],\n",
       " [tensor([ 87, 300,  12, 102]), tensor([2])],\n",
       " [tensor([87,  2, 85, 15]), tensor([12])],\n",
       " [tensor([  3, 102,   2, 230]), tensor([76])],\n",
       " [tensor([87, 76,  2, 15]), tensor([31])],\n",
       " [tensor([102,  12, 230,  87]), tensor([83])],\n",
       " [tensor([ 345, 2012,  340,   51]), tensor([3])],\n",
       " [tensor([2012,    0,   51,  340]), tensor([345])],\n",
       " [tensor([ 345,   51, 2012,  340]), tensor([0])],\n",
       " [tensor([  0,   3,  51, 340]), tensor([2012])],\n",
       " [tensor([ 340,  345,    0, 2012]), tensor([51])],\n",
       " [tensor([  51, 2012,  345,    0]), tensor([340])],\n",
       " [tensor([188, 204, 132,  34]), tensor([135])],\n",
       " [tensor([ 34, 135,   6, 143]), tensor([188])],\n",
       " [tensor([132, 204, 135,   6]), tensor([143])],\n",
       " [tensor([188, 135, 204,   6]), tensor([34])],\n",
       " [tensor([  6, 135, 188, 143]), tensor([132])],\n",
       " [tensor([143, 132, 204, 135]), tensor([6])],\n",
       " [tensor([143,   6, 135, 188]), tensor([204])],\n",
       " [tensor([  33, 3712,  325,  186]), tensor([94])],\n",
       " [tensor([277, 325,  33,  47]), tensor([434])],\n",
       " [tensor([434, 277, 325, 186]), tensor([130])],\n",
       " [tensor([130, 434,  47, 277]), tensor([325])],\n",
       " [tensor([  47, 3712,  130,   33]), tensor([186])],\n",
       " [tensor([ 94, 277, 130,  47]), tensor([3712])],\n",
       " [tensor([325, 130, 434, 186]), tensor([33])],\n",
       " [tensor([ 186,  325, 3712,  130]), tensor([277])],\n",
       " [tensor([186, 130, 434, 277]), tensor([47])],\n",
       " [tensor([  0, 957,  48, 473]), tensor([624])],\n",
       " [tensor([ 42,  13, 624,  48]), tensor([473])],\n",
       " [tensor([473,  48, 957, 624]), tensor([475])],\n",
       " [tensor([473,  13,  42, 475]), tensor([0])],\n",
       " [tensor([473,  42, 957, 624]), tensor([13])],\n",
       " [tensor([624, 473,  13,  42]), tensor([957])],\n",
       " [tensor([473, 475,   0,  42]), tensor([48])],\n",
       " [tensor([624, 473,  48,  13]), tensor([42])],\n",
       " ...]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Load images with gradient accumulation capabilities\n",
    "        images = images.view(-1, 28*28).requires_grad_()\n",
    "        \n",
    "        # Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass to get output/logits\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "        \n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        iter += 1\n",
    "        \n",
    "        if iter % 500 == 0:\n",
    "            # Calculate Accuracy         \n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Iterate through test dataset\n",
    "            for images, labels in test_loader:\n",
    "                # Load images with gradient accumulation capabilities\n",
    "                images = images.view(-1, 28*28).requires_grad_()\n",
    "                \n",
    "                # Forward pass only to get logits/output\n",
    "                outputs = model(images)\n",
    "                \n",
    "                # Get predictions from the maximum value\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                \n",
    "                # Total number of labels\n",
    "                total += labels.size(0)\n",
    "                \n",
    "                # Total correct predictions\n",
    "                correct += (predicted == labels).sum()\n",
    "            \n",
    "            accuracy = 100 * correct / total\n",
    "            \n",
    "            # Print Loss\n",
    "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "only one element tensors can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-157-6c7b7204d5aa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msamples_idx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "dataset = torch.tensor(samples_idx,dtype = torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample1 = sample(ingredients[1],\"flour\",CONTEXT_SIZE)\n",
    "sample1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'salt': 0,\n",
       " 'garlic': 1,\n",
       " 'onions': 2,\n",
       " 'olive oil': 3,\n",
       " 'butter': 4,\n",
       " 'water': 5,\n",
       " 'garlic cloves': 6,\n",
       " 'sugar': 7,\n",
       " 'eggs': 8,\n",
       " 'flour': 9,\n",
       " 'tomatoes': 10,\n",
       " 'ground black pepper': 11,\n",
       " 'cilantro': 12,\n",
       " 'vegetable oil': 13,\n",
       " 'pepper': 14,\n",
       " 'ginger': 15,\n",
       " 'soy sauce': 16,\n",
       " 'kosher salt': 17,\n",
       " 'lemon juice': 18,\n",
       " 'green onions': 19,\n",
       " 'carrots': 20,\n",
       " 'parmesan cheese': 21,\n",
       " 'ground cumin': 22,\n",
       " 'extra-virgin olive oil': 23,\n",
       " 'black pepper': 24,\n",
       " 'lime juice': 25,\n",
       " 'milk': 26,\n",
       " 'parsley': 27,\n",
       " 'chili powder': 28,\n",
       " 'oil': 29,\n",
       " 'red bell pepper': 30,\n",
       " 'scallions': 31,\n",
       " 'purple onion': 32,\n",
       " 'onion': 33,\n",
       " 'corn starch': 34,\n",
       " 'shrimp': 35,\n",
       " 'sesame oil': 36,\n",
       " 'jalapeno chilies': 37,\n",
       " 'baking powder': 38,\n",
       " 'dried oregano': 39,\n",
       " 'sour cream': 40,\n",
       " 'chicken broth': 41,\n",
       " 'cayenne pepper': 42,\n",
       " 'lime': 43,\n",
       " 'cooking spray': 44,\n",
       " 'brown sugar': 45,\n",
       " 'shallots': 46,\n",
       " 'green bell pepper': 47,\n",
       " 'garlic powder': 48,\n",
       " 'basil': 49,\n",
       " 'celery': 50,\n",
       " 'ground pepper': 51,\n",
       " 'honey': 52,\n",
       " 'vanilla extract': 53,\n",
       " 'paprika': 54,\n",
       " 'sea salt': 55,\n",
       " 'red pepper': 56,\n",
       " 'lemon': 57,\n",
       " 'fish sauce': 58,\n",
       " 'canola oil': 59,\n",
       " 'ground cinnamon': 60,\n",
       " 'rice vinegar': 61,\n",
       " 'avocado': 62,\n",
       " 'yellow onion': 63,\n",
       " 'dry white wine': 64,\n",
       " 'red pepper flakes': 65,\n",
       " 'heavy cream': 66,\n",
       " 'tomato paste': 67,\n",
       " 'cilantro leaves': 68,\n",
       " 'egg yolks': 69,\n",
       " 'white sugar': 70,\n",
       " 'boneless skinless chicken breasts': 71,\n",
       " 'bay leaves': 72,\n",
       " 'flat leaf parsley': 73,\n",
       " 'thyme': 74,\n",
       " 'chicken stock': 75,\n",
       " 'potatoes': 76,\n",
       " 'chicken': 77,\n",
       " 'salsa': 78,\n",
       " 'corn tortillas': 79,\n",
       " 'buttermilk': 80,\n",
       " 'ground turmeric': 81,\n",
       " 'cumin seed': 82,\n",
       " 'cumin': 83,\n",
       " 'egg whites': 84,\n",
       " 'garam masala': 85,\n",
       " 'baking soda': 86,\n",
       " 'green chilies': 87,\n",
       " 'mint': 88,\n",
       " 'black beans': 89,\n",
       " 'zucchini': 90,\n",
       " 'flour tortillas': 91,\n",
       " 'dried thyme': 92,\n",
       " 'mushrooms': 93,\n",
       " 'tomato sauce': 94,\n",
       " 'bay leaf': 95,\n",
       " 'granulated sugar': 96,\n",
       " 'coconut milk': 97,\n",
       " 'ground beef': 98,\n",
       " 'plum tomatoes': 99,\n",
       " 'oregano': 100,\n",
       " 'chicken breasts': 101,\n",
       " 'ground coriander': 102,\n",
       " 'mayonaise': 103,\n",
       " 'whole milk': 104,\n",
       " 'cucumber': 105,\n",
       " 'shredded cheddar cheese': 106,\n",
       " 'basil leaves': 107,\n",
       " 'tumeric': 108,\n",
       " 'red wine vinegar': 109,\n",
       " 'coriander': 110,\n",
       " 'white onion': 111,\n",
       " 'curry powder': 112,\n",
       " 'cinnamon': 113,\n",
       " 'sesame seeds': 114,\n",
       " 'worcestershire sauce': 115,\n",
       " 'hot sauce': 116,\n",
       " 'ground ginger': 117,\n",
       " 'red chili peppers': 118,\n",
       " 'cinnamon sticks': 119,\n",
       " 'bacon': 120,\n",
       " 'whipping cream': 121,\n",
       " 'clove': 122,\n",
       " 'boneless skinless chicken breast halves': 123,\n",
       " 'peanut oil': 124,\n",
       " 'lemon zest': 125,\n",
       " 'ground nutmeg': 126,\n",
       " 'warm water': 127,\n",
       " 'cream cheese': 128,\n",
       " 'peeled ginger': 129,\n",
       " 'spinach': 130,\n",
       " 'celery ribs': 131,\n",
       " 'dried basil': 132,\n",
       " 'yogurt': 133,\n",
       " 'ground red pepper': 134,\n",
       " 'balsamic vinegar': 135,\n",
       " 'orange juice': 136,\n",
       " 'white vinegar': 137,\n",
       " 'chives': 138,\n",
       " 'capers': 139,\n",
       " 'rosemary': 140,\n",
       " 'onion powder': 141,\n",
       " 'rice': 142,\n",
       " 'dijon mustard': 143,\n",
       " 'cold water': 144,\n",
       " 'eggplant': 145,\n",
       " 'mirin': 146,\n",
       " 'orange': 147,\n",
       " 'lime wedges': 148,\n",
       " 'cooking oil': 149,\n",
       " 'cayenne': 150,\n",
       " 'sliced green onions': 151,\n",
       " 'nutmeg': 152,\n",
       " 'raisins': 153,\n",
       " 'mozzarella cheese': 154,\n",
       " 'fat free less sodium chicken broth': 155,\n",
       " 'shredded mozzarella cheese': 156,\n",
       " 'ground pork': 157,\n",
       " 'beansprouts': 158,\n",
       " 'ginger root': 159,\n",
       " 'cheese': 160,\n",
       " 'black peppercorns': 161,\n",
       " 'sauce': 162,\n",
       " 'sweet potatoes': 163,\n",
       " 'cheddar cheese': 164,\n",
       " 'cherry tomatoes': 165,\n",
       " 'ketchup': 166,\n",
       " 'oyster sauce': 167,\n",
       " 'leeks': 168,\n",
       " 'white pepper': 169,\n",
       " 'bell pepper': 170,\n",
       " 'sodium soy sauce': 171,\n",
       " 'pecans': 172,\n",
       " 'hoisin sauce': 173,\n",
       " 'light brown sugar': 174,\n",
       " 'spring onions': 175,\n",
       " 'cabbage': 176,\n",
       " 'peanuts': 177,\n",
       " 'vegetable broth': 178,\n",
       " 'cracked black pepper': 179,\n",
       " 'cooked rice': 180,\n",
       " 'confectioners sugar': 181,\n",
       " 'chickpeas': 182,\n",
       " 'coriander seeds': 183,\n",
       " 'ricotta cheese': 184,\n",
       " 'active dry yeast': 185,\n",
       " 'italian seasoning': 186,\n",
       " 'lean ground beef': 187,\n",
       " 'salt chicken broth': 188,\n",
       " 'green pepper': 189,\n",
       " 'frozen peas': 190,\n",
       " 'feta cheese crumbles': 191,\n",
       " 'corn': 192,\n",
       " 'white wine': 193,\n",
       " 'cajun seasoning': 194,\n",
       " 'cornmeal': 195,\n",
       " 'ground cloves': 196,\n",
       " 'beef broth': 197,\n",
       " 'lemongrass': 198,\n",
       " 'long-grain rice': 199,\n",
       " 'boneless chicken skinless thigh': 200,\n",
       " 'ground allspice': 201,\n",
       " 'hot water': 202,\n",
       " 'toasted sesame oil': 203,\n",
       " 'white wine vinegar': 204,\n",
       " 'boiling water': 205,\n",
       " 'fennel seeds': 206,\n",
       " 'parmigiano reggiano cheese': 207,\n",
       " 'pinenuts': 208,\n",
       " 'light soy sauce': 209,\n",
       " 'smoked paprika': 210,\n",
       " 'yellow corn meal': 211,\n",
       " 'spaghetti': 212,\n",
       " 'ground white pepper': 213,\n",
       " 'sake': 214,\n",
       " 'dill': 215,\n",
       " 'sodium chicken broth': 216,\n",
       " 'green beans': 217,\n",
       " 'sweet onion': 218,\n",
       " 'half & half': 219,\n",
       " 'chile pepper': 220,\n",
       " 'cider vinegar': 221,\n",
       " 'chicken thighs': 222,\n",
       " 'pork': 223,\n",
       " 'mustard seeds': 224,\n",
       " 'vinegar': 225,\n",
       " 'pasta': 226,\n",
       " 'chiles': 227,\n",
       " 'ghee': 228,\n",
       " 'tomatillos': 229,\n",
       " 'basmati rice': 230,\n",
       " 'shiitake': 231,\n",
       " 'dark brown sugar': 232,\n",
       " 'vanilla': 233,\n",
       " 'hot pepper sauce': 234,\n",
       " 'bread crumbs': 235,\n",
       " 'green chile': 236,\n",
       " 'shredded Monterey Jack cheese': 237,\n",
       " 'enchilada sauce': 238,\n",
       " 'tortilla chips': 239,\n",
       " 'Sriracha': 240,\n",
       " 'serrano chile': 241,\n",
       " 'dark soy sauce': 242,\n",
       " 'cooked chicken': 243,\n",
       " 'dry red wine': 244,\n",
       " 'beef': 245,\n",
       " 'dry bread crumbs': 246,\n",
       " 'toasted sesame seeds': 247,\n",
       " 'greek yogurt': 248,\n",
       " 'arborio rice': 249,\n",
       " 'garlic paste': 250,\n",
       " 'radishes': 251,\n",
       " 'curry leaves': 252,\n",
       " 'saffron threads': 253,\n",
       " 'prosciutto': 254,\n",
       " 'dry sherry': 255,\n",
       " 'romaine lettuce': 256,\n",
       " 'corn kernels': 257,\n",
       " 'juice': 258,\n",
       " 'yellow bell pepper': 259,\n",
       " 'apple cider vinegar': 260,\n",
       " 'long grain white rice': 261,\n",
       " 'black olives': 262,\n",
       " 'creole seasoning': 263,\n",
       " 'melted butter': 264,\n",
       " 'baguette': 265,\n",
       " 'mango': 266,\n",
       " 'feta cheese': 267,\n",
       " 'peaches': 268,\n",
       " 'taco seasoning': 269,\n",
       " 'okra': 270,\n",
       " 'mint leaves': 271,\n",
       " 'pasta sauce': 272,\n",
       " 'asparagus': 273,\n",
       " 'rice noodles': 274,\n",
       " 'sweetened condensed milk': 275,\n",
       " 'baby spinach': 276,\n",
       " 'vegetable oil cooking spray': 277,\n",
       " 'frozen spinach': 278,\n",
       " 'russet potatoes': 279,\n",
       " 'star anise': 280,\n",
       " 'cream': 281,\n",
       " 'refried beans': 282,\n",
       " 'andouille sausage': 283,\n",
       " 'ham': 284,\n",
       " 'unsweetened cocoa powder': 285,\n",
       " 'flank steak': 286,\n",
       " 'walnuts': 287,\n",
       " 'Shaoxing wine': 288,\n",
       " 'coconut': 289,\n",
       " 'ground cardamom': 290,\n",
       " 'white rice': 291,\n",
       " 'yoghurt': 292,\n",
       " 'linguine': 293,\n",
       " 'garlic salt': 294,\n",
       " 'coriander powder': 295,\n",
       " 'chinese five-spice powder': 296,\n",
       " 'roasted red peppers': 297,\n",
       " 'almonds': 298,\n",
       " 'monterey jack': 299,\n",
       " 'cauliflower': 300,\n",
       " 'sliced mushrooms': 301,\n",
       " 'peas': 302,\n",
       " 'red wine': 303,\n",
       " 'margarine': 304,\n",
       " 'golden raisins': 305,\n",
       " 'cashew nuts': 306,\n",
       " 'tortillas': 307,\n",
       " 'green peas': 308,\n",
       " 'thyme leaves': 309,\n",
       " 'bananas': 310,\n",
       " 'evaporated milk': 311,\n",
       " 'marinara sauce': 312,\n",
       " 'rice wine': 313,\n",
       " 'lemon wedge': 314,\n",
       " 'bacon slices': 315,\n",
       " 'cilantro sprigs': 316,\n",
       " 'pork tenderloin': 317,\n",
       " 'red chili powder': 318,\n",
       " 'grape tomatoes': 319,\n",
       " 'thai chile': 320,\n",
       " 'shortening': 321,\n",
       " 'roma tomatoes': 322,\n",
       " 'noodles': 323,\n",
       " 'pineapple': 324,\n",
       " 'part-skim mozzarella cheese': 325,\n",
       " 'dark sesame oil': 326,\n",
       " 'firm tofu': 327,\n",
       " 'napa cabbage': 328,\n",
       " 'green cabbage': 329,\n",
       " 'sage': 330,\n",
       " 'salt and ground black pepper': 331,\n",
       " 'green olives': 332,\n",
       " 'heavy whipping cream': 333,\n",
       " 'lemon peel': 334,\n",
       " 'yukon gold potatoes': 335,\n",
       " 'broccoli': 336,\n",
       " 'kalamata': 337,\n",
       " 'coconut oil': 338,\n",
       " 'bourbon whiskey': 339,\n",
       " 'fennel bulb': 340,\n",
       " 'strawberries': 341,\n",
       " 'brown rice': 342,\n",
       " 'tomato purÃƒÂ©e': 343,\n",
       " 'beer': 344,\n",
       " 'olives': 345,\n",
       " 'bread flour': 346,\n",
       " 'saffron': 347,\n",
       " 'cannellini beans': 348,\n",
       " 'lasagna noodles': 349,\n",
       " 'baking potatoes': 350,\n",
       " 'thyme sprigs': 351,\n",
       " 'red potato': 352,\n",
       " 'vegetables': 353,\n",
       " 'parsley leaves': 354,\n",
       " 'english cucumber': 355,\n",
       " 'couscous': 356,\n",
       " 'broccoli florets': 357,\n",
       " 'black-eyed peas': 358,\n",
       " 'kalamata olives': 359,\n",
       " 'chipotles in adobo': 360,\n",
       " 'anchovy fillets': 361,\n",
       " 'sausages': 362,\n",
       " 'self rising flour': 363,\n",
       " 'chili': 364,\n",
       " 'sliced almonds': 365,\n",
       " 'unsweetened coconut milk': 366,\n",
       " 'vegetable stock': 367,\n",
       " 'lettuce leaves': 368,\n",
       " 'grits': 369,\n",
       " '1% milk': 370,\n",
       " 'Tabasco Pepper Sauce': 371,\n",
       " 'guacamole': 372,\n",
       " 'water chestnuts': 373,\n",
       " 'mussels': 374,\n",
       " 'brandy': 375,\n",
       " 'french bread': 376,\n",
       " 'arugula': 377,\n",
       " 'pinto beans': 378,\n",
       " 'thai basil': 379,\n",
       " 'taco seasoning mix': 380,\n",
       " 'roasted peanuts': 381,\n",
       " 'reduced sodium chicken broth': 382,\n",
       " 'butternut squash': 383,\n",
       " 'lettuce': 384,\n",
       " 'Mexican cheese blend': 385,\n",
       " 'bread': 386,\n",
       " 'stewed tomatoes': 387,\n",
       " 'jack cheese': 388,\n",
       " 'tarragon': 389,\n",
       " 'all purpose unbleached flour': 390,\n",
       " 'snow peas': 391,\n",
       " 'penne pasta': 392,\n",
       " 'shredded lettuce': 393,\n",
       " 'almond extract': 394,\n",
       " 'whole wheat flour': 395,\n",
       " 'vanilla beans': 396,\n",
       " 'apples': 397,\n",
       " 'slivered almonds': 398,\n",
       " 'iceberg lettuce': 399,\n",
       " 'pancetta': 400,\n",
       " 'goat cheese': 401,\n",
       " 'spices': 402,\n",
       " 'beef stock': 403,\n",
       " 'raspberries': 404,\n",
       " 'semisweet chocolate': 405,\n",
       " 'frozen corn': 406,\n",
       " 'dried rosemary': 407,\n",
       " 'pure vanilla extract': 408,\n",
       " 'ice': 409,\n",
       " 'asian fish sauce': 410,\n",
       " 'cake flour': 411,\n",
       " 'dried parsley': 412,\n",
       " 'sun-dried tomatoes': 413,\n",
       " 'hot red pepper flakes': 414,\n",
       " 'bread crumb': 415,\n",
       " 'shredded sharp cheddar cheese': 416,\n",
       " 'chipotle chile': 417,\n",
       " 'cream of tartar': 418,\n",
       " 'ground turkey': 419,\n",
       " 'chili flakes': 420,\n",
       " 'ice water': 421,\n",
       " 'sherry vinegar': 422,\n",
       " 'sharp cheddar cheese': 423,\n",
       " 'lard': 424,\n",
       " 'pecorino romano cheese': 425,\n",
       " 'collard greens': 426,\n",
       " 'dry mustard': 427,\n",
       " 'shredded cheese': 428,\n",
       " 'provolone cheese': 429,\n",
       " 'orange zest': 430,\n",
       " 'light corn syrup': 431,\n",
       " 'salmon fillets': 432,\n",
       " 'marjoram': 433,\n",
       " 'shredded carrots': 434,\n",
       " 'light coconut milk': 435,\n",
       " 'jasmine rice': 436,\n",
       " 'button mushrooms': 437,\n",
       " 'wonton wrappers': 438,\n",
       " 'reduced sodium soy sauce': 439,\n",
       " 'herbs': 440,\n",
       " 'daikon': 441,\n",
       " 'fettucine': 442,\n",
       " 'crÃƒÂ¨me fraÃƒÂ®che': 443,\n",
       " 'kidney beans': 444,\n",
       " 'boneless chicken breast': 445,\n",
       " 'chicken breast halves': 446,\n",
       " 'sliced black olives': 447,\n",
       " 'kale': 448,\n",
       " 'smoked sausage': 449,\n",
       " 'cream cheese, soften': 450,\n",
       " 'poblano chiles': 451,\n",
       " 'extra firm tofu': 452,\n",
       " 'cardamom pods': 453,\n",
       " 'polenta': 454,\n",
       " 'queso fresco': 455,\n",
       " 'garbanzo beans': 456,\n",
       " 'peppercorns': 457,\n",
       " 'pimentos': 458,\n",
       " 'meat': 459,\n",
       " 'mascarpone': 460,\n",
       " 'ancho chile pepper': 461,\n",
       " 'Gochujang base': 462,\n",
       " 'szechwan peppercorns': 463,\n",
       " 'tofu': 464,\n",
       " 'caster sugar': 465,\n",
       " 'chili sauce': 466,\n",
       " 'sweet paprika': 467,\n",
       " 'peanut butter': 468,\n",
       " 'bittersweet chocolate': 469,\n",
       " 'seasoning': 470,\n",
       " 'cooked white rice': 471,\n",
       " 'parsley sprigs': 472,\n",
       " 'rice flour': 473,\n",
       " 'pork shoulder': 474,\n",
       " 'seasoning salt': 475,\n",
       " 'bamboo shoots': 476,\n",
       " 'ground cayenne pepper': 477,\n",
       " 'artichoke hearts': 478,\n",
       " 'red cabbage': 479,\n",
       " 'cottage cheese': 480,\n",
       " 'cotija': 481,\n",
       " 'fat free milk': 482,\n",
       " 'chinese rice wine': 483,\n",
       " 'kaffir lime leaves': 484,\n",
       " 'lime zest': 485,\n",
       " 'tequila': 486,\n",
       " 'panko breadcrumbs': 487,\n",
       " 'cremini mushrooms': 488,\n",
       " 'ground lamb': 489,\n",
       " 'chicken wings': 490,\n",
       " 'vegetable shortening': 491,\n",
       " 'non-fat sour cream': 492,\n",
       " 'salsa verde': 493,\n",
       " 'seeds': 494,\n",
       " 'orange peel': 495,\n",
       " 'italian sausage': 496,\n",
       " 'palm sugar': 497,\n",
       " 'ice cubes': 498,\n",
       " 'romano cheese': 499,\n",
       " 'white bread': 500,\n",
       " 'chili paste': 501,\n",
       " 'chili pepper': 502,\n",
       " 'pork belly': 503,\n",
       " 'Italian parsley leaves': 504,\n",
       " 'garlic chili sauce': 505,\n",
       " 'red curry paste': 506,\n",
       " 'dry yeast': 507,\n",
       " 'allspice': 508,\n",
       " 'dried apricot': 509,\n",
       " 'whole peeled tomatoes': 510,\n",
       " 'dashi': 511,\n",
       " 'mint sprigs': 512,\n",
       " 'ginger paste': 513,\n",
       " 'dried shiitake mushrooms': 514,\n",
       " 'part-skim ricotta cheese': 515,\n",
       " 'Thai red curry paste': 516,\n",
       " 'pesto': 517,\n",
       " 'corn oil': 518,\n",
       " 'frozen corn kernels': 519,\n",
       " 'marsala wine': 520,\n",
       " 'chillies': 521,\n",
       " 'poblano peppers': 522,\n",
       " 'Italian bread': 523,\n",
       " 'granny smith apples': 524,\n",
       " 'quinoa': 525,\n",
       " 'yellow squash': 526,\n",
       " 'paneer': 527,\n",
       " 'pork sausages': 528,\n",
       " 'greek style yogurt': 529,\n",
       " 'yeast': 530,\n",
       " 'chicken legs': 531,\n",
       " 'kimchi': 532,\n",
       " 'crawfish': 533,\n",
       " 'pizza doughs': 534,\n",
       " 'bok choy': 535,\n",
       " 'gingerroot': 536,\n",
       " 'sea scallops': 537,\n",
       " 'chile powder': 538,\n",
       " 'cooked chicken breasts': 539,\n",
       " 'sweet chili sauce': 540,\n",
       " 'barbecue sauce': 541,\n",
       " 'lump crab meat': 542,\n",
       " 'chili oil': 543,\n",
       " 'cardamom': 544,\n",
       " 'egg noodles': 545,\n",
       " 'turnips': 546,\n",
       " 'creamy peanut butter': 547,\n",
       " 'ricotta': 548,\n",
       " 'mustard': 549,\n",
       " 'fontina cheese': 550,\n",
       " 'penne': 551,\n",
       " 'shredded cabbage': 552,\n",
       " 'Thai fish sauce': 553,\n",
       " 'black mustard seeds': 554,\n",
       " 'gruyere cheese': 555,\n",
       " 'parmesan': 556,\n",
       " 'swiss chard': 557,\n",
       " 'dried red chile peppers': 558,\n",
       " 'serrano peppers': 559,\n",
       " 'lemon grass': 560,\n",
       " 'red lentils': 561,\n",
       " 'maple syrup': 562,\n",
       " 'bow-tie pasta': 563,\n",
       " 'whole kernel corn, drain': 564,\n",
       " 'sausage casings': 565,\n",
       " 'dark rum': 566,\n",
       " 'beets': 567,\n",
       " 'vidalia onion': 568,\n",
       " 'catfish fillets': 569,\n",
       " 'hazelnuts': 570,\n",
       " 'watercress': 571,\n",
       " 'hard-boiled egg': 572,\n",
       " 'whipped cream': 573,\n",
       " 'curry paste': 574,\n",
       " 'sugar pea': 575,\n",
       " 'sliced carrots': 576,\n",
       " 'reduced-fat sour cream': 577,\n",
       " 'frozen pastry puff sheets': 578,\n",
       " 'unflavored gelatin': 579,\n",
       " 'cornflour': 580,\n",
       " 'sage leaves': 581,\n",
       " 'old bay seasoning': 582,\n",
       " 'rosemary sprigs': 583,\n",
       " 'chicken drumsticks': 584,\n",
       " 'Mexican oregano': 585,\n",
       " 'pecan halves': 586,\n",
       " 'clam juice': 587,\n",
       " 'dried porcini mushrooms': 588,\n",
       " 'chorizo sausage': 589,\n",
       " 'beans': 590,\n",
       " 'mozzarella': 591,\n",
       " 'green tomatoes': 592,\n",
       " 'baby spinach leaves': 593,\n",
       " 'molasses': 594,\n",
       " 'boiling potatoes': 595,\n",
       " 'sunflower oil': 596,\n",
       " 'dough': 597,\n",
       " 'pears': 598,\n",
       " 'seasoned bread crumbs': 599,\n",
       " 'ground chicken': 600,\n",
       " 'greens': 601,\n",
       " 'masa harina': 602,\n",
       " 'canned sodium chicken broth': 603,\n",
       " 'nori': 604,\n",
       " 'red kidney beans': 605,\n",
       " 'stock': 606,\n",
       " 'ripe olives': 607,\n",
       " 'clams': 608,\n",
       " 'green bean': 609,\n",
       " 'cooked shrimp': 610,\n",
       " 'rice vermicelli': 611,\n",
       " 'blanched almonds': 612,\n",
       " 'leaves': 613,\n",
       " 'cognac': 614,\n",
       " 'adobo sauce': 615,\n",
       " 'angel hair': 616,\n",
       " 'squid': 617,\n",
       " 'tamari soy sauce': 618,\n",
       " 'leg of lamb': 619,\n",
       " 'whole wheat tortillas': 620,\n",
       " 'broth': 621,\n",
       " 'firmly packed brown sugar': 622,\n",
       " 'fat': 623,\n",
       " 'prawns': 624,\n",
       " 'blackberries': 625,\n",
       " 'shiitake mushrooms': 626,\n",
       " 'corn husks': 627,\n",
       " 'quickcooking grits': 628,\n",
       " 'cooked ham': 629,\n",
       " 'jicama': 630,\n",
       " 'frozen whole kernel corn': 631,\n",
       " 'fish': 632,\n",
       " 'tomatoes with juice': 633,\n",
       " 'orange bell pepper': 634,\n",
       " 'lentils': 635,\n",
       " 'boneless pork shoulder': 636,\n",
       " 'cream of chicken soup': 637,\n",
       " 'elbow macaroni': 638,\n",
       " 'orzo': 639,\n",
       " 'amchur': 640,\n",
       " 'vanilla ice cream': 641,\n",
       " 'pico de gallo': 642,\n",
       " 'green cardamom': 643,\n",
       " 'pineapple juice': 644,\n",
       " 'mung bean sprouts': 645,\n",
       " 'rolls': 646,\n",
       " 'caraway seeds': 647,\n",
       " 'dried currants': 648,\n",
       " 'mace': 649,\n",
       " 'chicken pieces': 650,\n",
       " 'skim milk': 651,\n",
       " 'canned tomatoes': 652,\n",
       " 'galangal': 653,\n",
       " 'pepper jack': 654,\n",
       " 'pistachios': 655,\n",
       " 'golden brown sugar': 656,\n",
       " 'asiago': 657,\n",
       " 'tamarind paste': 658,\n",
       " 'crabmeat': 659,\n",
       " 'prunes': 660,\n",
       " 'tomato juice': 661,\n",
       " 'lamb shoulder': 662,\n",
       " 'baby bok choy': 663,\n",
       " 'fenugreek seeds': 664,\n",
       " 'cream style corn': 665,\n",
       " 'agave nectar': 666,\n",
       " 'ancho powder': 667,\n",
       " 'vegetable oil spray': 668,\n",
       " 'pearl onions': 669,\n",
       " 'white cornmeal': 670,\n",
       " 'rum': 671,\n",
       " 'sherry': 672,\n",
       " 'reduced fat milk': 673,\n",
       " 'serrano chilies': 674,\n",
       " 'pizza sauce': 675,\n",
       " 'panko': 676,\n",
       " 'sushi rice': 677,\n",
       " 'sweetened coconut flakes': 678,\n",
       " 'chees mozzarella': 679,\n",
       " 'salted butter': 680,\n",
       " 'marinade': 681,\n",
       " 'small red potato': 682,\n",
       " 'lemon slices': 683,\n",
       " 'canned black beans': 684,\n",
       " 'artichokes': 685,\n",
       " 'whole cloves': 686,\n",
       " 'cachaca': 687,\n",
       " 'beef tenderloin': 688,\n",
       " 'coconut cream': 689,\n",
       " 'lime rind': 690,\n",
       " 'semi-sweet chocolate morsels': 691,\n",
       " 'grapeseed oil': 692,\n",
       " 'blueberries': 693,\n",
       " 'white miso': 694,\n",
       " 'fillets': 695,\n",
       " 'rice paper': 696,\n",
       " 'superfine sugar': 697,\n",
       " 'ranch dressing': 698,\n",
       " 'hominy': 699,\n",
       " 'green cardamom pods': 700,\n",
       " 'table salt': 701,\n",
       " 'miso paste': 702,\n",
       " 'beef rib short': 703,\n",
       " 'corn flour': 704,\n",
       " 'condensed milk': 705,\n",
       " 'prepared horseradish': 706,\n",
       " 'fat skimmed chicken broth': 707,\n",
       " 'curry': 708,\n",
       " 'chorizo': 709,\n",
       " 'cilantro stems': 710,\n",
       " 'fennel': 711,\n",
       " 'sweet italian sausage': 712,\n",
       " 'red beans': 713,\n",
       " 'baby carrots': 714,\n",
       " 'salad dressing': 715,\n",
       " 'vodka': 716,\n",
       " 'white beans': 717,\n",
       " 'dried shrimp': 718,\n",
       " 'instant yeast': 719,\n",
       " 'asafoetida': 720,\n",
       " 'halibut fillets': 721,\n",
       " 'chicken bouillon': 722,\n",
       " 'hot pepper': 723,\n",
       " 'cauliflower florets': 724,\n",
       " 'peeled tomatoes': 725,\n",
       " 'pork loin': 726,\n",
       " 'fish fillets': 727,\n",
       " 'spinach leaves': 728,\n",
       " 'skirt steak': 729,\n",
       " 'parsnips': 730,\n",
       " 'grating cheese': 731,\n",
       " 'refrigerated piecrusts': 732,\n",
       " 'pork butt': 733,\n",
       " 'tilapia fillets': 734,\n",
       " 'egg substitute': 735,\n",
       " 'poppy seeds': 736,\n",
       " 'white sesame seeds': 737,\n",
       " 'creole mustard': 738,\n",
       " 'chipotle peppers': 739,\n",
       " 'bird chile': 740,\n",
       " 'rotisserie chicken': 741,\n",
       " 'rigatoni': 742,\n",
       " 'chuck': 743,\n",
       " 'portabello mushroom': 744,\n",
       " 'masala': 745,\n",
       " 'lemon rind': 746,\n",
       " 'semolina': 747,\n",
       " 'lower sodium chicken broth': 748,\n",
       " '2% reduced-fat milk': 749,\n",
       " 'phyllo dough': 750,\n",
       " 'Mexican cheese': 751,\n",
       " 'celery seed': 752,\n",
       " 'konbu': 753,\n",
       " 'red food coloring': 754,\n",
       " 'chunky salsa': 755,\n",
       " 'anchovy paste': 756,\n",
       " 'red enchilada sauce': 757,\n",
       " 'tomato ketchup': 758,\n",
       " 'soba noodles': 759,\n",
       " 'sirloin steak': 760,\n",
       " 'lamb': 761,\n",
       " 'fat free yogurt': 762,\n",
       " 'coffee': 763,\n",
       " 'curds': 764,\n",
       " 'crimini mushrooms': 765,\n",
       " 'udon': 766,\n",
       " 'oregano leaves': 767,\n",
       " 'corn syrup': 768,\n",
       " 'urad dal': 769,\n",
       " 'dry roasted peanuts': 770,\n",
       " 'short-grain rice': 771,\n",
       " 'salad': 772,\n",
       " 'jumbo shrimp': 773,\n",
       " 'taco sauce': 774,\n",
       " 'herbes de provence': 775,\n",
       " 'chuck roast': 776,\n",
       " 'ground sirloin': 777,\n",
       " 'steak': 778,\n",
       " 'pumpkin': 779,\n",
       " 'mixed greens': 780,\n",
       " 'oysters': 781,\n",
       " 'italian salad dressing': 782,\n",
       " 'light sour cream': 783,\n",
       " 'dipping sauces': 784,\n",
       " 'chicken livers': 785,\n",
       " 'dried sage': 786,\n",
       " 'mashed potatoes': 787,\n",
       " 'brown mustard seeds': 788,\n",
       " 'roasting chickens': 789,\n",
       " 'instant espresso powder': 790,\n",
       " 'smoked ham hocks': 791,\n",
       " 'shiitake mushroom caps': 792,\n",
       " 'boneless pork loin': 793,\n",
       " 'parmigiano-reggiano cheese': 794,\n",
       " 'sweet corn': 795,\n",
       " 'GruyÃƒÂ¨re cheese': 796,\n",
       " 'tahini': 797,\n",
       " 'potato starch': 798,\n",
       " 'white hominy': 799,\n",
       " 'littleneck clams': 800,\n",
       " 'turkey': 801,\n",
       " 'cooking wine': 802,\n",
       " 'escarole': 803,\n",
       " 'preserved lemon': 804,\n",
       " 'radicchio': 805,\n",
       " 'ramen noodles': 806,\n",
       " 'dressing': 807,\n",
       " 'beef brisket': 808,\n",
       " 'chicken meat': 809,\n",
       " 'bacon drippings': 810,\n",
       " 'sweet pepper': 811,\n",
       " 'poultry seasoning': 812,\n",
       " 'green bell pepper, slice': 813,\n",
       " 'sausage links': 814,\n",
       " 'anise': 815,\n",
       " 'asparagus spears': 816,\n",
       " 'condensed cream of chicken soup': 817,\n",
       " 'tea bags': 818,\n",
       " 'spring roll wrappers': 819,\n",
       " 'skinless chicken breasts': 820,\n",
       " 'cocoa powder': 821,\n",
       " 'light mayonnaise': 822,\n",
       " 'plums': 823,\n",
       " 'salad oil': 824,\n",
       " 'lower sodium soy sauce': 825,\n",
       " 'manchego cheese': 826,\n",
       " 'chinese cabbage': 827,\n",
       " 'taco shells': 828,\n",
       " 'pie crust': 829,\n",
       " 'mango chutney': 830,\n",
       " 'dates': 831,\n",
       " 'orange liqueur': 832,\n",
       " 'rotini': 833,\n",
       " 'pepper flakes': 834,\n",
       " 'seaweed': 835,\n",
       " 'tomato salsa': 836,\n",
       " 'fenugreek leaves': 837,\n",
       " 'white mushrooms': 838,\n",
       " 'steamed rice': 839,\n",
       " 'boneless chicken thighs': 840,\n",
       " 'scallion greens': 841,\n",
       " 'roasted tomatoes': 842,\n",
       " 'wine': 843,\n",
       " 'rib': 844,\n",
       " 'beaten eggs': 845,\n",
       " 'seasoned rice wine vinegar': 846,\n",
       " 'fish stock': 847,\n",
       " 'dry roast peanuts': 848,\n",
       " 'biscuits': 849,\n",
       " 'swiss cheese': 850,\n",
       " 'salmon': 851,\n",
       " 'liquid': 852,\n",
       " 'navel oranges': 853,\n",
       " 'thick-cut bacon': 854,\n",
       " 'clarified butter': 855,\n",
       " 'pecorino cheese': 856,\n",
       " 'apple juice': 857,\n",
       " 'firmly packed light brown sugar': 858,\n",
       " 'smoked salmon': 859,\n",
       " 'guajillo chiles': 860,\n",
       " 'semolina flour': 861,\n",
       " 'teriyaki sauce': 862,\n",
       " 'anchovies': 863,\n",
       " 'file powder': 864,\n",
       " 'whole milk ricotta cheese': 865,\n",
       " 'shrimp paste': 866,\n",
       " 'dried chile': 867,\n",
       " 'picante sauce': 868,\n",
       " 'condensed cream of mushroom soup': 869,\n",
       " 'kasuri methi': 870,\n",
       " 'hamburger buns': 871,\n",
       " 'pork chops': 872,\n",
       " 'savoy cabbage': 873,\n",
       " 'lime slices': 874,\n",
       " 'egg roll wrappers': 875,\n",
       " 'Italian turkey sausage': 876,\n",
       " 'salami': 877,\n",
       " 'brewed coffee': 878,\n",
       " 'cooked turkey': 879,\n",
       " 'pepperoni': 880,\n",
       " 'deveined shrimp': 881,\n",
       " 'japanese eggplants': 882,\n",
       " 'Anaheim chile': 883,\n",
       " 'broccoli rabe': 884,\n",
       " 'artichok heart marin': 885,\n",
       " 'chutney': 886,\n",
       " 'leav spinach': 887,\n",
       " 'anise seed': 888,\n",
       " 'hot Italian sausages': 889,\n",
       " 'cod fillets': 890,\n",
       " 'granulated garlic': 891,\n",
       " 'reduced fat sharp cheddar cheese': 892,\n",
       " 'edamame': 893,\n",
       " 'cheese tortellini': 894,\n",
       " 'turbinado': 895,\n",
       " 'country ham': 896,\n",
       " 'California bay leaves': 897,\n",
       " 'fusilli': 898,\n",
       " 'lobster': 899,\n",
       " 'extra sharp cheddar cheese': 900,\n",
       " 'currant': 901,\n",
       " 'shredded parmesan cheese': 902,\n",
       " 'chips': 903,\n",
       " 'mustard powder': 904,\n",
       " 'sambal ulek': 905,\n",
       " 'capsicum': 906,\n",
       " 'dried cranberries': 907,\n",
       " 'ground round': 908,\n",
       " 'mustard greens': 909,\n",
       " 'fruit': 910,\n",
       " 'beef stew meat': 911,\n",
       " 'red onions': 912,\n",
       " 'roast red peppers, drain': 913,\n",
       " 'chicken bouillon granules': 914,\n",
       " 'puff pastry': 915,\n",
       " 'sourdough bread': 916,\n",
       " 'seedless cucumber': 917,\n",
       " 'bread slices': 918,\n",
       " 'black sesame seeds': 919,\n",
       " 'whole-milk yogurt': 920,\n",
       " 'vinaigrette': 921,\n",
       " 'ground almonds': 922,\n",
       " 'ground chuck': 923,\n",
       " 'chickpea flour': 924,\n",
       " 'thai green curry paste': 925,\n",
       " 'duck': 926,\n",
       " 'spanish onion': 927,\n",
       " 'yellow peppers': 928,\n",
       " 'pumpkin seeds': 929,\n",
       " 'date': 930,\n",
       " 'great northern beans': 931,\n",
       " 'tapioca flour': 932,\n",
       " 'celery salt': 933,\n",
       " 'dried tarragon leaves': 934,\n",
       " 'nonstick spray': 935,\n",
       " 'shells': 936,\n",
       " 'rubbed sage': 937,\n",
       " 'miso': 938,\n",
       " 'pork loin chops': 939,\n",
       " 'new potatoes': 940,\n",
       " 'tuna steaks': 941,\n",
       " 'wild mushrooms': 942,\n",
       " 'tuna': 943,\n",
       " 'pastry': 944,\n",
       " 'green curry paste': 945,\n",
       " 'rice stick noodles': 946,\n",
       " 'pitas': 947,\n",
       " 'haricots verts': 948,\n",
       " 'white sandwich bread': 949,\n",
       " 'bottled clam juice': 950,\n",
       " 'flaked coconut': 951,\n",
       " 'chile paste': 952,\n",
       " 'yellow mustard': 953,\n",
       " 'pineapple chunks': 954,\n",
       " 'nuts': 955,\n",
       " 'enokitake': 956,\n",
       " 'club soda': 957,\n",
       " 'colby jack cheese': 958,\n",
       " 'softened butter': 959,\n",
       " 'rocket leaves': 960,\n",
       " 'organic vegetable broth': 961,\n",
       " 'mayonnaise': 962,\n",
       " 'orecchiette': 963,\n",
       " 'baby arugula': 964,\n",
       " 'gari': 965,\n",
       " 'cherries': 966,\n",
       " 'double cream': 967,\n",
       " 'chipotle chile powder': 968,\n",
       " 'salad greens': 969,\n",
       " 'cocoa': 970,\n",
       " 'country bread': 971,\n",
       " 'cranberries': 972,\n",
       " 'lime leaves': 973,\n",
       " 'shredded coconut': 974,\n",
       " 'greek seasoning': 975,\n",
       " 'light cream': 976,\n",
       " 'malt vinegar': 977,\n",
       " 'olive oil flavored cooking spray': 978,\n",
       " 'chicken stock cubes': 979,\n",
       " 'silken tofu': 980,\n",
       " 'chinese black vinegar': 981,\n",
       " 'Chinese egg noodles': 982,\n",
       " 'pita bread': 983,\n",
       " 'pizza crust': 984,\n",
       " 'ham hock': 985,\n",
       " 'slaw mix': 986,\n",
       " 'orange marmalade': 987,\n",
       " 'vietnamese fish sauce': 988,\n",
       " 'shredded swiss cheese': 989,\n",
       " 'brown cardamom': 990,\n",
       " 'spanish chorizo': 991,\n",
       " 'horseradish': 992,\n",
       " 'rib eye steaks': 993,\n",
       " 'croutons': 994,\n",
       " 'dill pickles': 995,\n",
       " 'dry vermouth': 996,\n",
       " 'chocolate': 997,\n",
       " 'parsley flakes': 998,\n",
       " 'hass avocado': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ingredients_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'salt': 0,\n",
       " 'garlic': 1,\n",
       " 'onions': 2,\n",
       " 'olive oil': 3,\n",
       " 'butter': 4,\n",
       " 'water': 5,\n",
       " 'garlic cloves': 6,\n",
       " 'sugar': 7,\n",
       " 'eggs': 8,\n",
       " 'flour': 9,\n",
       " 'tomatoes': 10,\n",
       " 'ground black pepper': 11,\n",
       " 'cilantro': 12,\n",
       " 'vegetable oil': 13,\n",
       " 'pepper': 14,\n",
       " 'ginger': 15,\n",
       " 'soy sauce': 16,\n",
       " 'kosher salt': 17,\n",
       " 'lemon juice': 18,\n",
       " 'green onions': 19,\n",
       " 'carrots': 20,\n",
       " 'parmesan cheese': 21,\n",
       " 'ground cumin': 22,\n",
       " 'extra-virgin olive oil': 23,\n",
       " 'black pepper': 24,\n",
       " 'lime juice': 25,\n",
       " 'milk': 26,\n",
       " 'parsley': 27,\n",
       " 'chili powder': 28,\n",
       " 'oil': 29,\n",
       " 'red bell pepper': 30,\n",
       " 'scallions': 31,\n",
       " 'purple onion': 32,\n",
       " 'onion': 33,\n",
       " 'corn starch': 34,\n",
       " 'shrimp': 35,\n",
       " 'sesame oil': 36,\n",
       " 'jalapeno chilies': 37,\n",
       " 'baking powder': 38,\n",
       " 'dried oregano': 39,\n",
       " 'sour cream': 40,\n",
       " 'chicken broth': 41,\n",
       " 'cayenne pepper': 42,\n",
       " 'lime': 43,\n",
       " 'cooking spray': 44,\n",
       " 'brown sugar': 45,\n",
       " 'shallots': 46,\n",
       " 'green bell pepper': 47,\n",
       " 'garlic powder': 48,\n",
       " 'basil': 49,\n",
       " 'celery': 50,\n",
       " 'ground pepper': 51,\n",
       " 'honey': 52,\n",
       " 'vanilla extract': 53,\n",
       " 'paprika': 54,\n",
       " 'sea salt': 55,\n",
       " 'red pepper': 56,\n",
       " 'lemon': 57,\n",
       " 'fish sauce': 58,\n",
       " 'canola oil': 59,\n",
       " 'ground cinnamon': 60,\n",
       " 'rice vinegar': 61,\n",
       " 'avocado': 62,\n",
       " 'yellow onion': 63,\n",
       " 'dry white wine': 64,\n",
       " 'red pepper flakes': 65,\n",
       " 'heavy cream': 66,\n",
       " 'tomato paste': 67,\n",
       " 'cilantro leaves': 68,\n",
       " 'egg yolks': 69,\n",
       " 'white sugar': 70,\n",
       " 'boneless skinless chicken breasts': 71,\n",
       " 'bay leaves': 72,\n",
       " 'flat leaf parsley': 73,\n",
       " 'thyme': 74,\n",
       " 'chicken stock': 75,\n",
       " 'potatoes': 76,\n",
       " 'chicken': 77,\n",
       " 'salsa': 78,\n",
       " 'corn tortillas': 79,\n",
       " 'buttermilk': 80,\n",
       " 'ground turmeric': 81,\n",
       " 'cumin seed': 82,\n",
       " 'cumin': 83,\n",
       " 'egg whites': 84,\n",
       " 'garam masala': 85,\n",
       " 'baking soda': 86,\n",
       " 'green chilies': 87,\n",
       " 'mint': 88,\n",
       " 'black beans': 89,\n",
       " 'zucchini': 90,\n",
       " 'flour tortillas': 91,\n",
       " 'dried thyme': 92,\n",
       " 'mushrooms': 93,\n",
       " 'tomato sauce': 94,\n",
       " 'bay leaf': 95,\n",
       " 'granulated sugar': 96,\n",
       " 'coconut milk': 97,\n",
       " 'ground beef': 98,\n",
       " 'plum tomatoes': 99,\n",
       " 'oregano': 100,\n",
       " 'chicken breasts': 101,\n",
       " 'ground coriander': 102,\n",
       " 'mayonaise': 103,\n",
       " 'whole milk': 104,\n",
       " 'cucumber': 105,\n",
       " 'shredded cheddar cheese': 106,\n",
       " 'basil leaves': 107,\n",
       " 'tumeric': 108,\n",
       " 'red wine vinegar': 109,\n",
       " 'coriander': 110,\n",
       " 'white onion': 111,\n",
       " 'curry powder': 112,\n",
       " 'cinnamon': 113,\n",
       " 'sesame seeds': 114,\n",
       " 'worcestershire sauce': 115,\n",
       " 'hot sauce': 116,\n",
       " 'ground ginger': 117,\n",
       " 'red chili peppers': 118,\n",
       " 'cinnamon sticks': 119,\n",
       " 'bacon': 120,\n",
       " 'whipping cream': 121,\n",
       " 'clove': 122,\n",
       " 'boneless skinless chicken breast halves': 123,\n",
       " 'peanut oil': 124,\n",
       " 'lemon zest': 125,\n",
       " 'ground nutmeg': 126,\n",
       " 'warm water': 127,\n",
       " 'cream cheese': 128,\n",
       " 'peeled ginger': 129,\n",
       " 'spinach': 130,\n",
       " 'celery ribs': 131,\n",
       " 'dried basil': 132,\n",
       " 'yogurt': 133,\n",
       " 'ground red pepper': 134,\n",
       " 'balsamic vinegar': 135,\n",
       " 'orange juice': 136,\n",
       " 'white vinegar': 137,\n",
       " 'chives': 138,\n",
       " 'capers': 139,\n",
       " 'rosemary': 140,\n",
       " 'onion powder': 141,\n",
       " 'rice': 142,\n",
       " 'dijon mustard': 143,\n",
       " 'cold water': 144,\n",
       " 'eggplant': 145,\n",
       " 'mirin': 146,\n",
       " 'orange': 147,\n",
       " 'lime wedges': 148,\n",
       " 'cooking oil': 149,\n",
       " 'cayenne': 150,\n",
       " 'sliced green onions': 151,\n",
       " 'nutmeg': 152,\n",
       " 'raisins': 153,\n",
       " 'mozzarella cheese': 154,\n",
       " 'fat free less sodium chicken broth': 155,\n",
       " 'shredded mozzarella cheese': 156,\n",
       " 'ground pork': 157,\n",
       " 'beansprouts': 158,\n",
       " 'ginger root': 159,\n",
       " 'cheese': 160,\n",
       " 'black peppercorns': 161,\n",
       " 'sauce': 162,\n",
       " 'sweet potatoes': 163,\n",
       " 'cheddar cheese': 164,\n",
       " 'cherry tomatoes': 165,\n",
       " 'ketchup': 166,\n",
       " 'oyster sauce': 167,\n",
       " 'leeks': 168,\n",
       " 'white pepper': 169,\n",
       " 'bell pepper': 170,\n",
       " 'sodium soy sauce': 171,\n",
       " 'pecans': 172,\n",
       " 'hoisin sauce': 173,\n",
       " 'light brown sugar': 174,\n",
       " 'spring onions': 175,\n",
       " 'cabbage': 176,\n",
       " 'peanuts': 177,\n",
       " 'vegetable broth': 178,\n",
       " 'cracked black pepper': 179,\n",
       " 'cooked rice': 180,\n",
       " 'confectioners sugar': 181,\n",
       " 'chickpeas': 182,\n",
       " 'coriander seeds': 183,\n",
       " 'ricotta cheese': 184,\n",
       " 'active dry yeast': 185,\n",
       " 'italian seasoning': 186,\n",
       " 'lean ground beef': 187,\n",
       " 'salt chicken broth': 188,\n",
       " 'green pepper': 189,\n",
       " 'frozen peas': 190,\n",
       " 'feta cheese crumbles': 191,\n",
       " 'corn': 192,\n",
       " 'white wine': 193,\n",
       " 'cajun seasoning': 194,\n",
       " 'cornmeal': 195,\n",
       " 'ground cloves': 196,\n",
       " 'beef broth': 197,\n",
       " 'lemongrass': 198,\n",
       " 'long-grain rice': 199,\n",
       " 'boneless chicken skinless thigh': 200,\n",
       " 'ground allspice': 201,\n",
       " 'hot water': 202,\n",
       " 'toasted sesame oil': 203,\n",
       " 'white wine vinegar': 204,\n",
       " 'boiling water': 205,\n",
       " 'fennel seeds': 206,\n",
       " 'parmigiano reggiano cheese': 207,\n",
       " 'pinenuts': 208,\n",
       " 'light soy sauce': 209,\n",
       " 'smoked paprika': 210,\n",
       " 'yellow corn meal': 211,\n",
       " 'spaghetti': 212,\n",
       " 'ground white pepper': 213,\n",
       " 'sake': 214,\n",
       " 'dill': 215,\n",
       " 'sodium chicken broth': 216,\n",
       " 'green beans': 217,\n",
       " 'sweet onion': 218,\n",
       " 'half & half': 219,\n",
       " 'chile pepper': 220,\n",
       " 'cider vinegar': 221,\n",
       " 'chicken thighs': 222,\n",
       " 'pork': 223,\n",
       " 'mustard seeds': 224,\n",
       " 'vinegar': 225,\n",
       " 'pasta': 226,\n",
       " 'chiles': 227,\n",
       " 'ghee': 228,\n",
       " 'tomatillos': 229,\n",
       " 'basmati rice': 230,\n",
       " 'shiitake': 231,\n",
       " 'dark brown sugar': 232,\n",
       " 'vanilla': 233,\n",
       " 'hot pepper sauce': 234,\n",
       " 'bread crumbs': 235,\n",
       " 'green chile': 236,\n",
       " 'shredded Monterey Jack cheese': 237,\n",
       " 'enchilada sauce': 238,\n",
       " 'tortilla chips': 239,\n",
       " 'Sriracha': 240,\n",
       " 'serrano chile': 241,\n",
       " 'dark soy sauce': 242,\n",
       " 'cooked chicken': 243,\n",
       " 'dry red wine': 244,\n",
       " 'beef': 245,\n",
       " 'dry bread crumbs': 246,\n",
       " 'toasted sesame seeds': 247,\n",
       " 'greek yogurt': 248,\n",
       " 'arborio rice': 249,\n",
       " 'garlic paste': 250,\n",
       " 'radishes': 251,\n",
       " 'curry leaves': 252,\n",
       " 'saffron threads': 253,\n",
       " 'prosciutto': 254,\n",
       " 'dry sherry': 255,\n",
       " 'romaine lettuce': 256,\n",
       " 'corn kernels': 257,\n",
       " 'juice': 258,\n",
       " 'yellow bell pepper': 259,\n",
       " 'apple cider vinegar': 260,\n",
       " 'long grain white rice': 261,\n",
       " 'black olives': 262,\n",
       " 'creole seasoning': 263,\n",
       " 'melted butter': 264,\n",
       " 'baguette': 265,\n",
       " 'mango': 266,\n",
       " 'feta cheese': 267,\n",
       " 'peaches': 268,\n",
       " 'taco seasoning': 269,\n",
       " 'okra': 270,\n",
       " 'mint leaves': 271,\n",
       " 'pasta sauce': 272,\n",
       " 'asparagus': 273,\n",
       " 'rice noodles': 274,\n",
       " 'sweetened condensed milk': 275,\n",
       " 'baby spinach': 276,\n",
       " 'vegetable oil cooking spray': 277,\n",
       " 'frozen spinach': 278,\n",
       " 'russet potatoes': 279,\n",
       " 'star anise': 280,\n",
       " 'cream': 281,\n",
       " 'refried beans': 282,\n",
       " 'andouille sausage': 283,\n",
       " 'ham': 284,\n",
       " 'unsweetened cocoa powder': 285,\n",
       " 'flank steak': 286,\n",
       " 'walnuts': 287,\n",
       " 'Shaoxing wine': 288,\n",
       " 'coconut': 289,\n",
       " 'ground cardamom': 290,\n",
       " 'white rice': 291,\n",
       " 'yoghurt': 292,\n",
       " 'linguine': 293,\n",
       " 'garlic salt': 294,\n",
       " 'coriander powder': 295,\n",
       " 'chinese five-spice powder': 296,\n",
       " 'roasted red peppers': 297,\n",
       " 'almonds': 298,\n",
       " 'monterey jack': 299,\n",
       " 'cauliflower': 300,\n",
       " 'sliced mushrooms': 301,\n",
       " 'peas': 302,\n",
       " 'red wine': 303,\n",
       " 'margarine': 304,\n",
       " 'golden raisins': 305,\n",
       " 'cashew nuts': 306,\n",
       " 'tortillas': 307,\n",
       " 'green peas': 308,\n",
       " 'thyme leaves': 309,\n",
       " 'bananas': 310,\n",
       " 'evaporated milk': 311,\n",
       " 'marinara sauce': 312,\n",
       " 'rice wine': 313,\n",
       " 'lemon wedge': 314,\n",
       " 'bacon slices': 315,\n",
       " 'cilantro sprigs': 316,\n",
       " 'pork tenderloin': 317,\n",
       " 'red chili powder': 318,\n",
       " 'grape tomatoes': 319,\n",
       " 'thai chile': 320,\n",
       " 'shortening': 321,\n",
       " 'roma tomatoes': 322,\n",
       " 'noodles': 323,\n",
       " 'pineapple': 324,\n",
       " 'part-skim mozzarella cheese': 325,\n",
       " 'dark sesame oil': 326,\n",
       " 'firm tofu': 327,\n",
       " 'napa cabbage': 328,\n",
       " 'green cabbage': 329,\n",
       " 'sage': 330,\n",
       " 'salt and ground black pepper': 331,\n",
       " 'green olives': 332,\n",
       " 'heavy whipping cream': 333,\n",
       " 'lemon peel': 334,\n",
       " 'yukon gold potatoes': 335,\n",
       " 'broccoli': 336,\n",
       " 'kalamata': 337,\n",
       " 'coconut oil': 338,\n",
       " 'bourbon whiskey': 339,\n",
       " 'fennel bulb': 340,\n",
       " 'strawberries': 341,\n",
       " 'brown rice': 342,\n",
       " 'tomato purÃƒÂ©e': 343,\n",
       " 'beer': 344,\n",
       " 'olives': 345,\n",
       " 'bread flour': 346,\n",
       " 'saffron': 347,\n",
       " 'cannellini beans': 348,\n",
       " 'lasagna noodles': 349,\n",
       " 'baking potatoes': 350,\n",
       " 'thyme sprigs': 351,\n",
       " 'red potato': 352,\n",
       " 'vegetables': 353,\n",
       " 'parsley leaves': 354,\n",
       " 'english cucumber': 355,\n",
       " 'couscous': 356,\n",
       " 'broccoli florets': 357,\n",
       " 'black-eyed peas': 358,\n",
       " 'kalamata olives': 359,\n",
       " 'chipotles in adobo': 360,\n",
       " 'anchovy fillets': 361,\n",
       " 'sausages': 362,\n",
       " 'self rising flour': 363,\n",
       " 'chili': 364,\n",
       " 'sliced almonds': 365,\n",
       " 'unsweetened coconut milk': 366,\n",
       " 'vegetable stock': 367,\n",
       " 'lettuce leaves': 368,\n",
       " 'grits': 369,\n",
       " '1% milk': 370,\n",
       " 'Tabasco Pepper Sauce': 371,\n",
       " 'guacamole': 372,\n",
       " 'water chestnuts': 373,\n",
       " 'mussels': 374,\n",
       " 'brandy': 375,\n",
       " 'french bread': 376,\n",
       " 'arugula': 377,\n",
       " 'pinto beans': 378,\n",
       " 'thai basil': 379,\n",
       " 'taco seasoning mix': 380,\n",
       " 'roasted peanuts': 381,\n",
       " 'reduced sodium chicken broth': 382,\n",
       " 'butternut squash': 383,\n",
       " 'lettuce': 384,\n",
       " 'Mexican cheese blend': 385,\n",
       " 'bread': 386,\n",
       " 'stewed tomatoes': 387,\n",
       " 'jack cheese': 388,\n",
       " 'tarragon': 389,\n",
       " 'all purpose unbleached flour': 390,\n",
       " 'snow peas': 391,\n",
       " 'penne pasta': 392,\n",
       " 'shredded lettuce': 393,\n",
       " 'almond extract': 394,\n",
       " 'whole wheat flour': 395,\n",
       " 'vanilla beans': 396,\n",
       " 'apples': 397,\n",
       " 'slivered almonds': 398,\n",
       " 'iceberg lettuce': 399,\n",
       " 'pancetta': 400,\n",
       " 'goat cheese': 401,\n",
       " 'spices': 402,\n",
       " 'beef stock': 403,\n",
       " 'raspberries': 404,\n",
       " 'semisweet chocolate': 405,\n",
       " 'frozen corn': 406,\n",
       " 'dried rosemary': 407,\n",
       " 'pure vanilla extract': 408,\n",
       " 'ice': 409,\n",
       " 'asian fish sauce': 410,\n",
       " 'cake flour': 411,\n",
       " 'dried parsley': 412,\n",
       " 'sun-dried tomatoes': 413,\n",
       " 'hot red pepper flakes': 414,\n",
       " 'bread crumb': 415,\n",
       " 'shredded sharp cheddar cheese': 416,\n",
       " 'chipotle chile': 417,\n",
       " 'cream of tartar': 418,\n",
       " 'ground turkey': 419,\n",
       " 'chili flakes': 420,\n",
       " 'ice water': 421,\n",
       " 'sherry vinegar': 422,\n",
       " 'sharp cheddar cheese': 423,\n",
       " 'lard': 424,\n",
       " 'pecorino romano cheese': 425,\n",
       " 'collard greens': 426,\n",
       " 'dry mustard': 427,\n",
       " 'shredded cheese': 428,\n",
       " 'provolone cheese': 429,\n",
       " 'orange zest': 430,\n",
       " 'light corn syrup': 431,\n",
       " 'salmon fillets': 432,\n",
       " 'marjoram': 433,\n",
       " 'shredded carrots': 434,\n",
       " 'light coconut milk': 435,\n",
       " 'jasmine rice': 436,\n",
       " 'button mushrooms': 437,\n",
       " 'wonton wrappers': 438,\n",
       " 'reduced sodium soy sauce': 439,\n",
       " 'herbs': 440,\n",
       " 'daikon': 441,\n",
       " 'fettucine': 442,\n",
       " 'crÃƒÂ¨me fraÃƒÂ®che': 443,\n",
       " 'kidney beans': 444,\n",
       " 'boneless chicken breast': 445,\n",
       " 'chicken breast halves': 446,\n",
       " 'sliced black olives': 447,\n",
       " 'kale': 448,\n",
       " 'smoked sausage': 449,\n",
       " 'cream cheese, soften': 450,\n",
       " 'poblano chiles': 451,\n",
       " 'extra firm tofu': 452,\n",
       " 'cardamom pods': 453,\n",
       " 'polenta': 454,\n",
       " 'queso fresco': 455,\n",
       " 'garbanzo beans': 456,\n",
       " 'peppercorns': 457,\n",
       " 'pimentos': 458,\n",
       " 'meat': 459,\n",
       " 'mascarpone': 460,\n",
       " 'ancho chile pepper': 461,\n",
       " 'Gochujang base': 462,\n",
       " 'szechwan peppercorns': 463,\n",
       " 'tofu': 464,\n",
       " 'caster sugar': 465,\n",
       " 'chili sauce': 466,\n",
       " 'sweet paprika': 467,\n",
       " 'peanut butter': 468,\n",
       " 'bittersweet chocolate': 469,\n",
       " 'seasoning': 470,\n",
       " 'cooked white rice': 471,\n",
       " 'parsley sprigs': 472,\n",
       " 'rice flour': 473,\n",
       " 'pork shoulder': 474,\n",
       " 'seasoning salt': 475,\n",
       " 'bamboo shoots': 476,\n",
       " 'ground cayenne pepper': 477,\n",
       " 'artichoke hearts': 478,\n",
       " 'red cabbage': 479,\n",
       " 'cottage cheese': 480,\n",
       " 'cotija': 481,\n",
       " 'fat free milk': 482,\n",
       " 'chinese rice wine': 483,\n",
       " 'kaffir lime leaves': 484,\n",
       " 'lime zest': 485,\n",
       " 'tequila': 486,\n",
       " 'panko breadcrumbs': 487,\n",
       " 'cremini mushrooms': 488,\n",
       " 'ground lamb': 489,\n",
       " 'chicken wings': 490,\n",
       " 'vegetable shortening': 491,\n",
       " 'non-fat sour cream': 492,\n",
       " 'salsa verde': 493,\n",
       " 'seeds': 494,\n",
       " 'orange peel': 495,\n",
       " 'italian sausage': 496,\n",
       " 'palm sugar': 497,\n",
       " 'ice cubes': 498,\n",
       " 'romano cheese': 499,\n",
       " 'white bread': 500,\n",
       " 'chili paste': 501,\n",
       " 'chili pepper': 502,\n",
       " 'pork belly': 503,\n",
       " 'Italian parsley leaves': 504,\n",
       " 'garlic chili sauce': 505,\n",
       " 'red curry paste': 506,\n",
       " 'dry yeast': 507,\n",
       " 'allspice': 508,\n",
       " 'dried apricot': 509,\n",
       " 'whole peeled tomatoes': 510,\n",
       " 'dashi': 511,\n",
       " 'mint sprigs': 512,\n",
       " 'ginger paste': 513,\n",
       " 'dried shiitake mushrooms': 514,\n",
       " 'part-skim ricotta cheese': 515,\n",
       " 'Thai red curry paste': 516,\n",
       " 'pesto': 517,\n",
       " 'corn oil': 518,\n",
       " 'frozen corn kernels': 519,\n",
       " 'marsala wine': 520,\n",
       " 'chillies': 521,\n",
       " 'poblano peppers': 522,\n",
       " 'Italian bread': 523,\n",
       " 'granny smith apples': 524,\n",
       " 'quinoa': 525,\n",
       " 'yellow squash': 526,\n",
       " 'paneer': 527,\n",
       " 'pork sausages': 528,\n",
       " 'greek style yogurt': 529,\n",
       " 'yeast': 530,\n",
       " 'chicken legs': 531,\n",
       " 'kimchi': 532,\n",
       " 'crawfish': 533,\n",
       " 'pizza doughs': 534,\n",
       " 'bok choy': 535,\n",
       " 'gingerroot': 536,\n",
       " 'sea scallops': 537,\n",
       " 'chile powder': 538,\n",
       " 'cooked chicken breasts': 539,\n",
       " 'sweet chili sauce': 540,\n",
       " 'barbecue sauce': 541,\n",
       " 'lump crab meat': 542,\n",
       " 'chili oil': 543,\n",
       " 'cardamom': 544,\n",
       " 'egg noodles': 545,\n",
       " 'turnips': 546,\n",
       " 'creamy peanut butter': 547,\n",
       " 'ricotta': 548,\n",
       " 'mustard': 549,\n",
       " 'fontina cheese': 550,\n",
       " 'penne': 551,\n",
       " 'shredded cabbage': 552,\n",
       " 'Thai fish sauce': 553,\n",
       " 'black mustard seeds': 554,\n",
       " 'gruyere cheese': 555,\n",
       " 'parmesan': 556,\n",
       " 'swiss chard': 557,\n",
       " 'dried red chile peppers': 558,\n",
       " 'serrano peppers': 559,\n",
       " 'lemon grass': 560,\n",
       " 'red lentils': 561,\n",
       " 'maple syrup': 562,\n",
       " 'bow-tie pasta': 563,\n",
       " 'whole kernel corn, drain': 564,\n",
       " 'sausage casings': 565,\n",
       " 'dark rum': 566,\n",
       " 'beets': 567,\n",
       " 'vidalia onion': 568,\n",
       " 'catfish fillets': 569,\n",
       " 'hazelnuts': 570,\n",
       " 'watercress': 571,\n",
       " 'hard-boiled egg': 572,\n",
       " 'whipped cream': 573,\n",
       " 'curry paste': 574,\n",
       " 'sugar pea': 575,\n",
       " 'sliced carrots': 576,\n",
       " 'reduced-fat sour cream': 577,\n",
       " 'frozen pastry puff sheets': 578,\n",
       " 'unflavored gelatin': 579,\n",
       " 'cornflour': 580,\n",
       " 'sage leaves': 581,\n",
       " 'old bay seasoning': 582,\n",
       " 'rosemary sprigs': 583,\n",
       " 'chicken drumsticks': 584,\n",
       " 'Mexican oregano': 585,\n",
       " 'pecan halves': 586,\n",
       " 'clam juice': 587,\n",
       " 'dried porcini mushrooms': 588,\n",
       " 'chorizo sausage': 589,\n",
       " 'beans': 590,\n",
       " 'mozzarella': 591,\n",
       " 'green tomatoes': 592,\n",
       " 'baby spinach leaves': 593,\n",
       " 'molasses': 594,\n",
       " 'boiling potatoes': 595,\n",
       " 'sunflower oil': 596,\n",
       " 'dough': 597,\n",
       " 'pears': 598,\n",
       " 'seasoned bread crumbs': 599,\n",
       " 'ground chicken': 600,\n",
       " 'greens': 601,\n",
       " 'masa harina': 602,\n",
       " 'canned sodium chicken broth': 603,\n",
       " 'nori': 604,\n",
       " 'red kidney beans': 605,\n",
       " 'stock': 606,\n",
       " 'ripe olives': 607,\n",
       " 'clams': 608,\n",
       " 'green bean': 609,\n",
       " 'cooked shrimp': 610,\n",
       " 'rice vermicelli': 611,\n",
       " 'blanched almonds': 612,\n",
       " 'leaves': 613,\n",
       " 'cognac': 614,\n",
       " 'adobo sauce': 615,\n",
       " 'angel hair': 616,\n",
       " 'squid': 617,\n",
       " 'tamari soy sauce': 618,\n",
       " 'leg of lamb': 619,\n",
       " 'whole wheat tortillas': 620,\n",
       " 'broth': 621,\n",
       " 'firmly packed brown sugar': 622,\n",
       " 'fat': 623,\n",
       " 'prawns': 624,\n",
       " 'blackberries': 625,\n",
       " 'shiitake mushrooms': 626,\n",
       " 'corn husks': 627,\n",
       " 'quickcooking grits': 628,\n",
       " 'cooked ham': 629,\n",
       " 'jicama': 630,\n",
       " 'frozen whole kernel corn': 631,\n",
       " 'fish': 632,\n",
       " 'tomatoes with juice': 633,\n",
       " 'orange bell pepper': 634,\n",
       " 'lentils': 635,\n",
       " 'boneless pork shoulder': 636,\n",
       " 'cream of chicken soup': 637,\n",
       " 'elbow macaroni': 638,\n",
       " 'orzo': 639,\n",
       " 'amchur': 640,\n",
       " 'vanilla ice cream': 641,\n",
       " 'pico de gallo': 642,\n",
       " 'green cardamom': 643,\n",
       " 'pineapple juice': 644,\n",
       " 'mung bean sprouts': 645,\n",
       " 'rolls': 646,\n",
       " 'caraway seeds': 647,\n",
       " 'dried currants': 648,\n",
       " 'mace': 649,\n",
       " 'chicken pieces': 650,\n",
       " 'skim milk': 651,\n",
       " 'canned tomatoes': 652,\n",
       " 'galangal': 653,\n",
       " 'pepper jack': 654,\n",
       " 'pistachios': 655,\n",
       " 'golden brown sugar': 656,\n",
       " 'asiago': 657,\n",
       " 'tamarind paste': 658,\n",
       " 'crabmeat': 659,\n",
       " 'prunes': 660,\n",
       " 'tomato juice': 661,\n",
       " 'lamb shoulder': 662,\n",
       " 'baby bok choy': 663,\n",
       " 'fenugreek seeds': 664,\n",
       " 'cream style corn': 665,\n",
       " 'agave nectar': 666,\n",
       " 'ancho powder': 667,\n",
       " 'vegetable oil spray': 668,\n",
       " 'pearl onions': 669,\n",
       " 'white cornmeal': 670,\n",
       " 'rum': 671,\n",
       " 'sherry': 672,\n",
       " 'reduced fat milk': 673,\n",
       " 'serrano chilies': 674,\n",
       " 'pizza sauce': 675,\n",
       " 'panko': 676,\n",
       " 'sushi rice': 677,\n",
       " 'sweetened coconut flakes': 678,\n",
       " 'chees mozzarella': 679,\n",
       " 'salted butter': 680,\n",
       " 'marinade': 681,\n",
       " 'small red potato': 682,\n",
       " 'lemon slices': 683,\n",
       " 'canned black beans': 684,\n",
       " 'artichokes': 685,\n",
       " 'whole cloves': 686,\n",
       " 'cachaca': 687,\n",
       " 'beef tenderloin': 688,\n",
       " 'coconut cream': 689,\n",
       " 'lime rind': 690,\n",
       " 'semi-sweet chocolate morsels': 691,\n",
       " 'grapeseed oil': 692,\n",
       " 'blueberries': 693,\n",
       " 'white miso': 694,\n",
       " 'fillets': 695,\n",
       " 'rice paper': 696,\n",
       " 'superfine sugar': 697,\n",
       " 'ranch dressing': 698,\n",
       " 'hominy': 699,\n",
       " 'green cardamom pods': 700,\n",
       " 'table salt': 701,\n",
       " 'miso paste': 702,\n",
       " 'beef rib short': 703,\n",
       " 'corn flour': 704,\n",
       " 'condensed milk': 705,\n",
       " 'prepared horseradish': 706,\n",
       " 'fat skimmed chicken broth': 707,\n",
       " 'curry': 708,\n",
       " 'chorizo': 709,\n",
       " 'cilantro stems': 710,\n",
       " 'fennel': 711,\n",
       " 'sweet italian sausage': 712,\n",
       " 'red beans': 713,\n",
       " 'baby carrots': 714,\n",
       " 'salad dressing': 715,\n",
       " 'vodka': 716,\n",
       " 'white beans': 717,\n",
       " 'dried shrimp': 718,\n",
       " 'instant yeast': 719,\n",
       " 'asafoetida': 720,\n",
       " 'halibut fillets': 721,\n",
       " 'chicken bouillon': 722,\n",
       " 'hot pepper': 723,\n",
       " 'cauliflower florets': 724,\n",
       " 'peeled tomatoes': 725,\n",
       " 'pork loin': 726,\n",
       " 'fish fillets': 727,\n",
       " 'spinach leaves': 728,\n",
       " 'skirt steak': 729,\n",
       " 'parsnips': 730,\n",
       " 'grating cheese': 731,\n",
       " 'refrigerated piecrusts': 732,\n",
       " 'pork butt': 733,\n",
       " 'tilapia fillets': 734,\n",
       " 'egg substitute': 735,\n",
       " 'poppy seeds': 736,\n",
       " 'white sesame seeds': 737,\n",
       " 'creole mustard': 738,\n",
       " 'chipotle peppers': 739,\n",
       " 'bird chile': 740,\n",
       " 'rotisserie chicken': 741,\n",
       " 'rigatoni': 742,\n",
       " 'chuck': 743,\n",
       " 'portabello mushroom': 744,\n",
       " 'masala': 745,\n",
       " 'lemon rind': 746,\n",
       " 'semolina': 747,\n",
       " 'lower sodium chicken broth': 748,\n",
       " '2% reduced-fat milk': 749,\n",
       " 'phyllo dough': 750,\n",
       " 'Mexican cheese': 751,\n",
       " 'celery seed': 752,\n",
       " 'konbu': 753,\n",
       " 'red food coloring': 754,\n",
       " 'chunky salsa': 755,\n",
       " 'anchovy paste': 756,\n",
       " 'red enchilada sauce': 757,\n",
       " 'tomato ketchup': 758,\n",
       " 'soba noodles': 759,\n",
       " 'sirloin steak': 760,\n",
       " 'lamb': 761,\n",
       " 'fat free yogurt': 762,\n",
       " 'coffee': 763,\n",
       " 'curds': 764,\n",
       " 'crimini mushrooms': 765,\n",
       " 'udon': 766,\n",
       " 'oregano leaves': 767,\n",
       " 'corn syrup': 768,\n",
       " 'urad dal': 769,\n",
       " 'dry roasted peanuts': 770,\n",
       " 'short-grain rice': 771,\n",
       " 'salad': 772,\n",
       " 'jumbo shrimp': 773,\n",
       " 'taco sauce': 774,\n",
       " 'herbes de provence': 775,\n",
       " 'chuck roast': 776,\n",
       " 'ground sirloin': 777,\n",
       " 'steak': 778,\n",
       " 'pumpkin': 779,\n",
       " 'mixed greens': 780,\n",
       " 'oysters': 781,\n",
       " 'italian salad dressing': 782,\n",
       " 'light sour cream': 783,\n",
       " 'dipping sauces': 784,\n",
       " 'chicken livers': 785,\n",
       " 'dried sage': 786,\n",
       " 'mashed potatoes': 787,\n",
       " 'brown mustard seeds': 788,\n",
       " 'roasting chickens': 789,\n",
       " 'instant espresso powder': 790,\n",
       " 'smoked ham hocks': 791,\n",
       " 'shiitake mushroom caps': 792,\n",
       " 'boneless pork loin': 793,\n",
       " 'parmigiano-reggiano cheese': 794,\n",
       " 'sweet corn': 795,\n",
       " 'GruyÃƒÂ¨re cheese': 796,\n",
       " 'tahini': 797,\n",
       " 'potato starch': 798,\n",
       " 'white hominy': 799,\n",
       " 'littleneck clams': 800,\n",
       " 'turkey': 801,\n",
       " 'cooking wine': 802,\n",
       " 'escarole': 803,\n",
       " 'preserved lemon': 804,\n",
       " 'radicchio': 805,\n",
       " 'ramen noodles': 806,\n",
       " 'dressing': 807,\n",
       " 'beef brisket': 808,\n",
       " 'chicken meat': 809,\n",
       " 'bacon drippings': 810,\n",
       " 'sweet pepper': 811,\n",
       " 'poultry seasoning': 812,\n",
       " 'green bell pepper, slice': 813,\n",
       " 'sausage links': 814,\n",
       " 'anise': 815,\n",
       " 'asparagus spears': 816,\n",
       " 'condensed cream of chicken soup': 817,\n",
       " 'tea bags': 818,\n",
       " 'spring roll wrappers': 819,\n",
       " 'skinless chicken breasts': 820,\n",
       " 'cocoa powder': 821,\n",
       " 'light mayonnaise': 822,\n",
       " 'plums': 823,\n",
       " 'salad oil': 824,\n",
       " 'lower sodium soy sauce': 825,\n",
       " 'manchego cheese': 826,\n",
       " 'chinese cabbage': 827,\n",
       " 'taco shells': 828,\n",
       " 'pie crust': 829,\n",
       " 'mango chutney': 830,\n",
       " 'dates': 831,\n",
       " 'orange liqueur': 832,\n",
       " 'rotini': 833,\n",
       " 'pepper flakes': 834,\n",
       " 'seaweed': 835,\n",
       " 'tomato salsa': 836,\n",
       " 'fenugreek leaves': 837,\n",
       " 'white mushrooms': 838,\n",
       " 'steamed rice': 839,\n",
       " 'boneless chicken thighs': 840,\n",
       " 'scallion greens': 841,\n",
       " 'roasted tomatoes': 842,\n",
       " 'wine': 843,\n",
       " 'rib': 844,\n",
       " 'beaten eggs': 845,\n",
       " 'seasoned rice wine vinegar': 846,\n",
       " 'fish stock': 847,\n",
       " 'dry roast peanuts': 848,\n",
       " 'biscuits': 849,\n",
       " 'swiss cheese': 850,\n",
       " 'salmon': 851,\n",
       " 'liquid': 852,\n",
       " 'navel oranges': 853,\n",
       " 'thick-cut bacon': 854,\n",
       " 'clarified butter': 855,\n",
       " 'pecorino cheese': 856,\n",
       " 'apple juice': 857,\n",
       " 'firmly packed light brown sugar': 858,\n",
       " 'smoked salmon': 859,\n",
       " 'guajillo chiles': 860,\n",
       " 'semolina flour': 861,\n",
       " 'teriyaki sauce': 862,\n",
       " 'anchovies': 863,\n",
       " 'file powder': 864,\n",
       " 'whole milk ricotta cheese': 865,\n",
       " 'shrimp paste': 866,\n",
       " 'dried chile': 867,\n",
       " 'picante sauce': 868,\n",
       " 'condensed cream of mushroom soup': 869,\n",
       " 'kasuri methi': 870,\n",
       " 'hamburger buns': 871,\n",
       " 'pork chops': 872,\n",
       " 'savoy cabbage': 873,\n",
       " 'lime slices': 874,\n",
       " 'egg roll wrappers': 875,\n",
       " 'Italian turkey sausage': 876,\n",
       " 'salami': 877,\n",
       " 'brewed coffee': 878,\n",
       " 'cooked turkey': 879,\n",
       " 'pepperoni': 880,\n",
       " 'deveined shrimp': 881,\n",
       " 'japanese eggplants': 882,\n",
       " 'Anaheim chile': 883,\n",
       " 'broccoli rabe': 884,\n",
       " 'artichok heart marin': 885,\n",
       " 'chutney': 886,\n",
       " 'leav spinach': 887,\n",
       " 'anise seed': 888,\n",
       " 'hot Italian sausages': 889,\n",
       " 'cod fillets': 890,\n",
       " 'granulated garlic': 891,\n",
       " 'reduced fat sharp cheddar cheese': 892,\n",
       " 'edamame': 893,\n",
       " 'cheese tortellini': 894,\n",
       " 'turbinado': 895,\n",
       " 'country ham': 896,\n",
       " 'California bay leaves': 897,\n",
       " 'fusilli': 898,\n",
       " 'lobster': 899,\n",
       " 'extra sharp cheddar cheese': 900,\n",
       " 'currant': 901,\n",
       " 'shredded parmesan cheese': 902,\n",
       " 'chips': 903,\n",
       " 'mustard powder': 904,\n",
       " 'sambal ulek': 905,\n",
       " 'capsicum': 906,\n",
       " 'dried cranberries': 907,\n",
       " 'ground round': 908,\n",
       " 'mustard greens': 909,\n",
       " 'fruit': 910,\n",
       " 'beef stew meat': 911,\n",
       " 'red onions': 912,\n",
       " 'roast red peppers, drain': 913,\n",
       " 'chicken bouillon granules': 914,\n",
       " 'puff pastry': 915,\n",
       " 'sourdough bread': 916,\n",
       " 'seedless cucumber': 917,\n",
       " 'bread slices': 918,\n",
       " 'black sesame seeds': 919,\n",
       " 'whole-milk yogurt': 920,\n",
       " 'vinaigrette': 921,\n",
       " 'ground almonds': 922,\n",
       " 'ground chuck': 923,\n",
       " 'chickpea flour': 924,\n",
       " 'thai green curry paste': 925,\n",
       " 'duck': 926,\n",
       " 'spanish onion': 927,\n",
       " 'yellow peppers': 928,\n",
       " 'pumpkin seeds': 929,\n",
       " 'date': 930,\n",
       " 'great northern beans': 931,\n",
       " 'tapioca flour': 932,\n",
       " 'celery salt': 933,\n",
       " 'dried tarragon leaves': 934,\n",
       " 'nonstick spray': 935,\n",
       " 'shells': 936,\n",
       " 'rubbed sage': 937,\n",
       " 'miso': 938,\n",
       " 'pork loin chops': 939,\n",
       " 'new potatoes': 940,\n",
       " 'tuna steaks': 941,\n",
       " 'wild mushrooms': 942,\n",
       " 'tuna': 943,\n",
       " 'pastry': 944,\n",
       " 'green curry paste': 945,\n",
       " 'rice stick noodles': 946,\n",
       " 'pitas': 947,\n",
       " 'haricots verts': 948,\n",
       " 'white sandwich bread': 949,\n",
       " 'bottled clam juice': 950,\n",
       " 'flaked coconut': 951,\n",
       " 'chile paste': 952,\n",
       " 'yellow mustard': 953,\n",
       " 'pineapple chunks': 954,\n",
       " 'nuts': 955,\n",
       " 'enokitake': 956,\n",
       " 'club soda': 957,\n",
       " 'colby jack cheese': 958,\n",
       " 'softened butter': 959,\n",
       " 'rocket leaves': 960,\n",
       " 'organic vegetable broth': 961,\n",
       " 'mayonnaise': 962,\n",
       " 'orecchiette': 963,\n",
       " 'baby arugula': 964,\n",
       " 'gari': 965,\n",
       " 'cherries': 966,\n",
       " 'double cream': 967,\n",
       " 'chipotle chile powder': 968,\n",
       " 'salad greens': 969,\n",
       " 'cocoa': 970,\n",
       " 'country bread': 971,\n",
       " 'cranberries': 972,\n",
       " 'lime leaves': 973,\n",
       " 'shredded coconut': 974,\n",
       " 'greek seasoning': 975,\n",
       " 'light cream': 976,\n",
       " 'malt vinegar': 977,\n",
       " 'olive oil flavored cooking spray': 978,\n",
       " 'chicken stock cubes': 979,\n",
       " 'silken tofu': 980,\n",
       " 'chinese black vinegar': 981,\n",
       " 'Chinese egg noodles': 982,\n",
       " 'pita bread': 983,\n",
       " 'pizza crust': 984,\n",
       " 'ham hock': 985,\n",
       " 'slaw mix': 986,\n",
       " 'orange marmalade': 987,\n",
       " 'vietnamese fish sauce': 988,\n",
       " 'shredded swiss cheese': 989,\n",
       " 'brown cardamom': 990,\n",
       " 'spanish chorizo': 991,\n",
       " 'horseradish': 992,\n",
       " 'rib eye steaks': 993,\n",
       " 'croutons': 994,\n",
       " 'dill pickles': 995,\n",
       " 'dry vermouth': 996,\n",
       " 'chocolate': 997,\n",
       " 'parsley flakes': 998,\n",
       " 'hass avocado': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ingredients_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = cbow(sample1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6837])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
